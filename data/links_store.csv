fetched_at_utc,url,url_canonical,domain,headline,L1,L2,L3,L4,L5,L6,tldr,content_text,source_title,author,publish_date
2025-09-13T18:01:51.305980Z,https://langchain-ai.github.io/langgraph/concepts/memory/,https://langchain-ai.github.io/langgraph/concepts/memory,langchain-ai.github.io,Memory Concepts in LangGraph,,,[],[],[],[],"[""LangGraph enables AI agents to retain information across interactions through short-term and long-term memory."", ""Short-term memory manages ongoing conversation history within a session, while long-term memory stores user-specific data across sessions."", ""Memory types include semantic (facts), episodic (experiences), and procedural (rules)."", ""Memory can be updated actively during interactions or in the background for deeper analysis."", ""Effective memory management enhances agent performance and user satisfaction.""]","Memory¶
Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.
This conceptual guide covers two types of memory, based on their recall scope:
-
Short-term memory, or thread-scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent's state. State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step.
-
Long-term memory stores user-specific or application-level data across sessions and is shared across conversational threads. It can be recalled at any time and in any thread. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores (reference doc) to let you save and recall long-term memories.
Short-term memory¶
Short-term memory lets your application remember previous interactions within a single thread or conversation. A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.
LangGraph manages short-term memory as part of the agent's state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph's state, the bot can access the full context for a given conversation while maintaining separation between different threads.
Manage short-term memory¶
Conversation history is the most common form of short-term memory, and long conversations pose a challenge to today's LLMs. A full history may not fit inside an LLM's context window, resulting in an irrecoverable error. Even if your LLM supports the full context length, most LLMs still perform poorly over long contexts. They get ""distracted"" by stale or off-topic content, all while suffering from slower response times and higher costs.
Chat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information.
For more information on common techniques for managing messages, see the Add and manage memory guide.
Long-term memory¶
Long-term memory in LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is thread-scoped, long-term memory is saved within custom ""namespaces.""
Long-term memory is a complex challenge without a one-size-fits-all solution. However, the following questions provide a framework to help you navigate the different techniques:
-
What is the type of memory? Humans use memories to remember facts (semantic memory), experiences (episodic memory), and rules (procedural memory). AI agents can use memory in the same ways. For example, AI agents can use memory to remember specific facts about a user to accomplish a task.
-
When do you want to update memories? Memory can be updated as part of an agent's application logic (e.g., ""on the hot path""). In this case, the agent typically decides to remember facts before responding to a user. Alternatively, memory can be updated as a background task (logic that runs in the background / asynchronously and generates memories). We explain the tradeoffs between these approaches in the section below.
Memory types¶
Different applications require various types of memory. Although the analogy isn't perfect, examining human memory types can be insightful. Some research (e.g., the CoALA paper) have even mapped these human memory types to those used in AI agents.
Semantic memory¶
Semantic memory, both in humans and AI agents, involves the retention of specific facts and concepts. In humans, it can include information learned in school and the understanding of concepts and their relationships. For AI agents, semantic memory is often used to personalize applications by remembering facts or concepts from past interactions.
Note
Semantic memory is different from ""semantic search,"" which is a technique for finding similar content using ""meaning"" (usually as embeddings). Semantic memory is a term from psychology, referring to storing facts and knowledge, while semantic search is a method for retrieving information based on meaning rather than exact matches.
Profile¶
Semantic memories can be managed in different ways. For example, memories can be a single, continuously updated ""profile"" of well-scoped and specific information about a user, organization, or other entity (including the agent itself). A profile is generally just a JSON document with various key-value pairs you've selected to represent your domain.
When remembering a profile, you will want to make sure that you are updating the profile each time. As a result, you will want to pass in the previous profile and ask the model to generate a new profile (or some JSON patch to apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or strict decoding when generating documents to ensure the memory schemas remains valid.
Collection¶
Alternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you're less likely to lose information over time. It's easier for an LLM to generate new objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to lead to higher recall downstream.
However, this shifts some complexity memory updating. The model must now delete or update existing items in the list, which can be tricky. In addition, some models may default to over-inserting and others may default to over-updating. See the Trustcall package for one way to manage this and consider evaluation (e.g., with a tool like LangSmith) to help you tune the behavior.
Working with document collections also shifts complexity to memory search over the list. The Store
currently supports both semantic search and filtering by content.
Finally, using a collection of memories can make it challenging to provide comprehensive context to the model. While individual memories may follow a specific schema, this structure might not capture the full context or relationships between memories. As a result, when using these memories to generate responses, the model may lack important contextual information that would be more readily available in a unified profile approach.
Regardless of memory management approach, the central point is that the agent will use the semantic memories to ground its responses, which often leads to more personalized and relevant interactions.
Episodic memory¶
Episodic memory, in both humans and AI agents, involves recalling past events or actions. The CoALA paper frames this well: facts can be written to semantic memory, whereas experiences can be written to episodic memory. For AI agents, episodic memory is often used to help an agent remember how to accomplish a task.
In practice, episodic memories are often implemented through few-shot example prompting, where agents learn from past sequences to perform tasks correctly. Sometimes it's easier to ""show"" than ""tell"" and LLMs learn well from examples. Few-shot learning lets you ""program"" your LLM by updating the prompt with input-output examples to illustrate the intended behavior. While various best-practices can be used to generate few-shot examples, often the challenge lies in selecting the most relevant examples based on user input.
Note that the memory store is just one way to store data as few-shot examples. If you want to have more developer involvement, or tie few-shots more closely to your evaluation harness, you can also use a LangSmith Dataset to store your data. Then dynamic few-shot example selectors can be used out-of-the box to achieve this same goal. LangSmith will index the dataset for you and enable retrieval of few shot examples that are most relevant to the user input based upon keyword similarity (using a BM25-like algorithm for keyword based similarity).
See this how-to video for example usage of dynamic few-shot example selection in LangSmith. Also, see this blog post showcasing few-shot prompting to improve tool calling performance and this blog post using few-shot example to align an LLMs to human preferences.
Procedural memory¶
Procedural memory, in both humans and AI agents, involves remembering the rules used to perform tasks. In humans, procedural memory is like the internalized knowledge of how to perform tasks, such as riding a bike via basic motor skills and balance. Episodic memory, on the other hand, involves recalling specific experiences, such as the first time you successfully rode a bike without training wheels or a memorable bike ride through a scenic route. For AI agents, procedural memory is a combination of model weights, agent code, and agent's prompt that collectively determine the agent's functionality.
In practice, it is fairly uncommon for agents to modify their model weights or rewrite their code. However, it is more common for agents to modify their own prompts.
One effective approach to refining an agent's instructions is through ""Reflection"" or meta-prompting. This involves prompting the agent with its current instructions (e.g., the system prompt) along with recent conversations or explicit user feedback. The agent then refines its own instructions based on this input. This method is particularly useful for tasks where instructions are challenging to specify upfront, as it allows the agent to learn and adapt from its interactions.
For example, we built a Tweet generator using external feedback and prompt re-writing to produce high-quality paper summaries for Twitter. In this case, the specific summarization prompt was difficult to specify a priori, but it was fairly easy for a user to critique the generated Tweets and provide feedback on how to improve the summarization process.
The below pseudo-code shows how you might implement this with the LangGraph memory store, using the store to save a prompt, the update_instructions
node to get the current prompt (as well as feedback from the conversation with the user captured in state[""messages""]
), update the prompt, and save the new prompt back to the store. Then, the call_model
get the updated prompt from the store and uses it to generate a response.
# Node that *uses* the instructions
def call_model(state: State, store: BaseStore):
namespace = (""agent_instructions"", )
instructions = store.get(namespace, key=""agent_a"")[0]
# Application logic
prompt = prompt_template.format(instructions=instructions.value[""instructions""])
...
# Node that updates instructions
def update_instructions(state: State, store: BaseStore):
namespace = (""instructions"",)
current_instructions = store.search(namespace)[0]
# Memory logic
prompt = prompt_template.format(instructions=current_instructions.value[""instructions""], conversation=state[""messages""])
output = llm.invoke(prompt)
new_instructions = output['new_instructions']
store.put((""agent_instructions"",), ""agent_a"", {""instructions"": new_instructions})
...
Writing memories¶
There are two primary methods for agents to write memories: ""in the hot path"" and ""in the background"".
In the hot path¶
Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.
However, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.
As an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.
In the background¶
Creating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.
However, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.
See our memory-service template as an reference implementation.
Memory storage¶
LangGraph stores long-term memories as JSON documents in a store. Each memory is organized under a custom namespace
(similar to a folder) and a distinct key
(like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.
from langgraph.store.memory import InMemoryStore
def embed(texts: list[str]) -> list[list[float]]:
# Replace with an actual embedding function or LangChain embeddings object
return [[1.0, 2.0] * len(texts)]
# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.
store = InMemoryStore(index={""embed"": embed, ""dims"": 2})
user_id = ""my-user""
application_context = ""chitchat""
namespace = (user_id, application_context)
store.put(
namespace,
""a-memory"",
{
""rules"": [
""User likes short, direct language"",
""User only speaks English & python"",
],
""my-key"": ""my-value"",
},
)
# get the ""memory"" by ID
item = store.get(namespace, ""a-memory"")
# search for ""memories"" within this namespace, filtering on content equivalence, sorted by vector similarity
items = store.search(
namespace, filter={""my-key"": ""my-value""}, query=""language preferences""
)
For more information about the memory store, see the Persistence guide.",Overview,LangChain AI,2025-09-13
2025-09-13T18:11:45.026341Z,https://medium.com/data-science/from-data-scientist-to-ml-ai-product-manager-39359bd44512,https://medium.com/data-science/from-data-scientist-to-ml-ai-product-manager-39359bd44512,medium.com,From Data Scientist to ML / AI Product Manager,,,[],[],[],[],"[""Transitioning from Data Scientist to ML/AI Product Manager involves mastering four key skill areas: product strategy, product delivery, stakeholder influencing, and technical fluency."", ""Product strategy focuses on identifying user pain points and prioritizing problems based on impact rather than technology novelty."", ""Product delivery emphasizes efficient value creation through phases like discovery, planning, prototyping, launch, and iteration, relying heavily on agile and MVP principles."", ""Influencing skills are crucial for communication, alignment, stakeholder management, and leading teams without direct coding work."", ""Technical fluency requires leveraging data science knowledge, understanding MLOps and backend engineering, and awareness of responsible AI and ethical considerations."", ""Building a supportive network, planning career transition carefully, and preparing for PM interviews with a business-user-solution mindset can ease the move into the ML/AI PM role.""]","From Data Scientist to ML / AI Product Manager
As Artificial Intelligence is becoming more and more popular, more companies and teams want to start or increase leveraging it. Because of that, many job positions are appearing or gaining importance in the market. A good example is the figure of Machine Learning / Artificial Intelligence Product Manager.
In my case, I transitioned from a Data Scientist role into a Machine Learning Product Manager role over two years ago. During this time, I have been able to see a constant increase in job offers related to this position, blog posts and talks discussing it, and many people considering a transition or gaining interest in it. I have also been able to confirm my passion for this role and how much I enjoy my day-to-day work, responsibilities, and value I can bring to the team and company.
The role of AI / ML PM is still quite vague and evolves almost as fast as state-of-the-art AI. Although many product teams are becoming relatively autonomous using AI thanks to plug-in solutions and GenAI APIs, I will focus on the role of AI / ML PMs working in core ML teams. These teams are usually formed by Data Scientists, Machine Learning Engineers, and Research Scientists, and together with other roles are involved in solutions where GenAI through an API might not be enough (traditional ML use cases, need of LLMs fine tuning, specific in-house use cases, ML as a service products…). For an illustrative example of such a team, you can check one of my previous posts “Working in a multidisciplinary Machine Learning team to bring value to our users”.
In this blog post, we will cover the main skills and knowledge that are needed for this position, how to get there, and learnings and tips based on what worked for me in this transition.
The most important skills for an ML PM
There are many necessary skills and knowledge needed to succeed as an ML / AI PM, but the most important ones can be divided into 4 groups: product strategy, product delivery, influencing, and tech fluency. Let’s deep dive into each group to further understand what each skill set means and how to get them.
Product Strategy
Product strategy is about understanding users and their pains, identifying the right problems and opportunities, and prioritizing them based on quantitative and qualitative evidence.
As a former Data Scientist, for me this meant falling in love with the problem and user pain to solve and not so much with the specific solution, and thinking about where we can bring more value to our users instead of where to apply this cool new AI model. I have found it key to have a clear understanding of OKRs (Objective Key Results) and to care about the final impact of the initiatives (delivering outcomes instead of outputs).
Product Managers need to prioritize tasks and initiatives, so I’ve learned the importance of balancing effort vs. reward for each initiative and ensuring this influences decisions on what and how to build solutions (e.g. considering the project management triangle - scope, quality, time). Initiatives succeed if they are able to tackle the four big product risks: value, usability, feasibility, and business viability.
The most important resources I used to learn about Product Strategy are:
- Good vs bad product manager, by Ben Horowitz.
- The reference book that everyone recommended to me and that I now recommend to any aspiring PM is “Inspired: How to create tech products customers love”, by Marty Cagan.
- Another book and author that helped me get closer to user space and user problems is “Continuous Discovery Habits: Discover Products that Create Customer Value and Business Value”, by Teresa Torres.
Product Delivery
Product Delivery is about being able to manage a team’s initiative to deliver value to the users efficiently.
I started by understanding the product feature phases (discovery, plan, design, implementation, test, launch, and iterations) and what each of them meant for me as a Data Scientist. Then followed with how value can be brought “efficiently”: starting small (through Minimum Viable Products and prototypes), delivering value fast by small steps, and iterations. To ensure initiatives move in the right direction, I have found it also key to continuously measure impact (e.g. through dashboards) and learn from quantitative and qualitative data, adapting next steps with insights and new learnings.
To learn about Product Delivery, I would recommend:
- Some of the previously shared resources (e.g. Inspired book) also cover the importance of MVP, prototyping and agile applied to Product Management. I also wrote a blog post on how to think about MVPs and prototypes in the context of ML initiatives: When ML meets Product — Less is often more.
- Learning about agile and project management (for example through this crash course), and about Jira or the project management tool used by your current company (with videos such as this crash course).
Influencing
Influencing is the ability to gain trust, align with stakeholders and guide the team.
Compared to the Data Scientist’s role, the day-to-day work as a PM changes completely: it is no longer about coding, but about communicating, aligning, and (a lot!) of meetings. Great communication and storytelling become key for this role, especially the ability to explain complex ML topics to non technical people. It becomes also important to keep stakeholders informed, give visibility to the team’s hard work, and ensure alignment and buying on the future direction of the team (proving how it will help tackle the biggest challenges and opportunities, gaining trust). Finally, it is also important to learn how to challenge, say no, act as an umbrella for the team, and sometimes deliver bad results or bad news.
The resources I would recommend for this topic:
- The complete stakeholder mapping guide, Miro
- A must read book for any Data Scientist and also for any ML Product Manager is “Storytelling with data — A Data Visualization Guide for Business Professionals”, by Cole Nussbaumer Knaflic.
- To learn further about how as a Product Manager you can influence and empower the team, “EMPOWERED: Ordinary People, Extraordinary Products”, by Marty Cagan and Chris Jones.
Tech fluency
Tech fluency for an ML / AI PM, means knowledge and sensibility in Machine Learning, Responsible AI, Data in general, MLOPs, and Back End Engineering.
Your Data Science / Machine Learning / Artificial Intelligence background is probably your strongest asset, make sure you leverage it! This knowledge will allow you to talk in the same language as Data Scientists, understand deeply and challenge the projects, have sensibility on what is possible or easy and what isn’t, potential risks, dependencies, edge cases, and limitations.
As you are going to lead products with an impact on users, including responsible AI awareness becomes paramount. Risks related to not taking this into account include ethical dilemmas, company reputation, and legal issues (e.g. specific EU laws like GDPR or AI Act). In my case, I started with the course Practical Data Ethics, from Fast.ai.
General data fluency is also necessary (probably you have it covered too): analytical thinking, being curious about data, understanding where data is stored, how to access it, importance of historical data… On top of that it is also important to kow how to measure impact, the relationship with business metrics and OKRs, and experimentation (a/b testing).
As your ML models will probably need to be deployed in order to reach a final impact on users, you might work with Machine Learning Engineers within the team (or skilled DS with model deployment knowledge). You’ll need to gain sensibility about MLOPs: what it means to put a model in production, monitor it, and maintain it. In deeplearning.ai, you can find a great course on MLOPs (Machine Learning Engineering for Production Specialization).
Finally, it can happen that your team also has Back End Engineers (usually dealing with the integration of the deployed model with the rest of the platform). In my case, this was the technical field that was further away from my expertise, so I had to invest some time learning and gaining sensibility about BE. In many companies, the technical interview for PM includes some BE related questions. Make sure to get an overview of several engineering topics such as: CICD, staging vs production environments, Monolith vs MicroServices architectures (and PROs and CONTs of each setup), Pull Requests, APIs, event driven architectures….
Wrapping up and final tips
We have covered the 4 most important knowledge areas for an ML / AI PM (product strategy, product delivery, influencing and tech fluency), why they are important, and some ideas on resources that can help you achieve them.
Just like in any career progress, I found it key to define a plan, and share my short and mid term desires and expectations with managers and colleagues. Through this, I was able to transition into a PM role in the same company where I was working as a Data Scientist. This made the transition much easier: I already knew the business, product, tech, ways of working, colleagues… I also looked for mentors and colleagues within the company to whom I could ask questions, learn specific topics from and even practice for the PM interviews.
To prepare for the interviews, I focused on changing my mindset: developing vs thinking whether to build something or not, whether to launch something or not. I found out BUS (Business, User, Solution) is a great way to structure responses during interviews and enforce this new mindset there.
What I shared in this blog post can look like a lot, but it really is much easier than learning python or understanding how back-propagation works. If you are still unsure whether this role is for you or not, know that you can always give it a try, experiment, and decide to go back to your previous role. Or maybe, who knows, you end up loving being an ML / AI PM just like I do!",From Data Scientist to ML / AI Product Manager,Anna Via,2024-04-03
2025-09-13T18:22:25.960863Z,https://blog.langchain.com/launching-long-term-memory-support-in-langgraph/,https://blog.langchain.com/launching-long-term-memory-support-in-langgraph,blog.langchain.com,Launching Long-Term Memory Support in LangGraph,,,[],[],[],[],"[""Introducing long-term memory support in LangGraph for Python and JavaScript."", ""Enables agents to store and recall information between conversations."", ""Supports cross-thread memory with custom namespacing and JSON document storage.""]","Today, we are excited to announce the first steps towards long-term memory support in LangGraph, available both in Python and JavaScript. Long-term memory lets you store and recall information between conversations so your agent can learn from feedback and adapt to user preferences. This feature is part of the OSS library, and it is enabled by default for all LangGraph Cloud & Studio users.
On Memory
Most AI applications today are goldfish; they forget everything between conversations. This isn't just inefficient— it fundamentally limits what AI can do.
Over the past year at LangChain, we've been working with customers to build memory into their agents. Through this experience, we've realized something important: there's no universally perfect solution for AI memory. The best memory for each application still contains very application specific logic. By extension, most ""agent memory"" products today are too high-level. They try to create a one-size-fits-all product that doesn't satisfy many production users' needs.
This insight is why we have built our initial memory support into LangGraph as a simple document store. High level abstractions can be easily built on top (as we will show below), but beneath it all is a simple, reliable, persistent memory layer that comes built in to all LangGraph applications.
Cross-Thread Memory
LangGraph has always excelled at managing state within a single conversation thread using checkpointers. This ""short-term memory"" lets you maintain context within a single conversation.
Today, we're extending that capability across multiple threads, enabling your agents to easily remember information across multiple conversations, all integrated in the LangGraph framework.
At its core, cross-thread memory is ""just"" a persistent document store that lets you put, get, and search for memories you've saved. These basic primitives enable:
- Cross-Thread Persistence: Store and recall information across different conversation sessions.
- Flexible Namespacing: Organize memories using custom namespaces, making it easy to manage data for different users, organizations, or contexts.
- JSON Document Storage: Save memories as JSON documents for easy manipulation and retrieval.
- Content-Based Filtering: Search for memories across namespaces based on content.
For a deeper understanding of these concepts, we've prepared a set of documents to provide framing and guidance on how to get started:
- A conceptual video walking through memory concepts
- Conceptual guides on memory in LangGraph Python & JS
- How-to guide for sharing memories across threads in Python & JS
Practical Implementation
To help you get started with implementing long-term memory in your applications, we've prepared a new LangGraph template:
This LangGraph Template shows a chatbot agent that manages its own memory. Key resources for this are
- An end-to-end tutorial video walking through the implementation
- A LangGraph Memory Agent in Python
- A LangGraph.js Memory Agent in JavaScript
These resources demonstrate one way to leverage long-term memory in LangGraph, bridging the gap between concept and implementation.
We encourage you to explore these materials and experiment with incorporating long-term memory into your LangGraph projects. As always, we welcome your feedback and look forward to seeing how you apply these new capabilities in your applications.",Launching Long-Term Memory Support in LangGraph,,2024-10-08
2025-09-13T18:33:39.563799Z,https://langchain-ai.github.io/langgraph/concepts/multi_agent/,https://langchain-ai.github.io/langgraph/concepts/multi_agent,langchain-ai.github.io,Multi-agent Systems in LangGraph,,,[],[],[],[],"[""LangGraph enables the creation of multi-agent systems to manage complex applications."", ""Benefits include modularity, specialization, and explicit control over agent communication."", ""Architectures include network, supervisor, hierarchical, and custom workflows."", ""Handoffs allow agents to pass control and state updates to each other."", ""Prebuilt implementations like 'langgraph-supervisor' and 'langgraph-swarm' facilitate development.""]","Multi-agent systems¶
An agent is a system that uses an LLM to decide the control flow of an application. As you develop these systems, they might grow more complex over time, making them harder to manage and scale. For example, you might run into the following problems:
- agent has too many tools at its disposal and makes poor decisions about which tool to call next
- context grows too complex for a single agent to keep track of
- there is a need for multiple specialization areas in the system (e.g. planner, researcher, math expert, etc.)
To tackle these, you might consider breaking your application into multiple smaller, independent agents and composing them into a multi-agent system. These independent agents can be as simple as a prompt and an LLM call, or as complex as a ReAct agent (and more!).
The primary benefits of using multi-agent systems are:
- Modularity: Separate agents make it easier to develop, test, and maintain agentic systems.
- Specialization: You can create expert agents focused on specific domains, which helps with the overall system performance.
- Control: You can explicitly control how agents communicate (as opposed to relying on function calling).
Multi-agent architectures¶
There are several ways to connect agents in a multi-agent system:
- Network: each agent can communicate with every other agent. Any agent can decide which other agent to call next.
- Supervisor: each agent communicates with a single supervisor agent. Supervisor agent makes decisions on which agent should be called next.
- Supervisor (tool-calling): this is a special case of supervisor architecture. Individual agents can be represented as tools. In this case, a supervisor agent uses a tool-calling LLM to decide which of the agent tools to call, as well as the arguments to pass to those agents.
- Hierarchical: you can define a multi-agent system with a supervisor of supervisors. This is a generalization of the supervisor architecture and allows for more complex control flows.
- Custom multi-agent workflow: each agent communicates with only a subset of agents. Parts of the flow are deterministic, and only some agents can decide which other agents to call next.
Handoffs¶
In multi-agent architectures, agents can be represented as graph nodes. Each agent node executes its step(s) and decides whether to finish execution or route to another agent, including potentially routing to itself (e.g., running in a loop). A common pattern in multi-agent interactions is handoffs, where one agent hands off control to another. Handoffs allow you to specify:
- destination: target agent to navigate to (e.g., name of the node to go to)
- payload: information to pass to that agent (e.g., state update)
To implement handoffs in LangGraph, agent nodes can return Command
object that allows you to combine both control flow and state updates:
def agent(state) -> Command[Literal[""agent"", ""another_agent""]]:
# the condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.
goto = get_next_agent(...) # 'agent' / 'another_agent'
return Command(
# Specify which agent to call next
goto=goto,
# Update the graph state
update={""my_state_key"": ""my_state_value""}
)
In a more complex scenario where each agent node is itself a graph (i.e., a subgraph), a node in one of the agent subgraphs might want to navigate to a different agent. For example, if you have two agents, alice
and bob
(subgraph nodes in a parent graph), and alice
needs to navigate to bob
, you can set graph=Command.PARENT
in the Command
object:
Note
If you need to support visualization for subgraphs communicating using Command(graph=Command.PARENT)
you would need to wrap them in a node function with Command
annotation:
Instead of this:
you would need to do this:
Handoffs as tools¶
One of the most common agent types is a tool-calling agent. For those types of agents, a common pattern is wrapping a handoff in a tool call:
API Reference: tool
from langchain_core.tools import tool
@tool
def transfer_to_bob():
""""""Transfer to bob.""""""
return Command(
# name of the agent (node) to go to
goto=""bob"",
# data to send to the agent
update={""my_state_key"": ""my_state_value""},
# indicate to LangGraph that we need to navigate to
# agent node in a parent graph
graph=Command.PARENT,
)
This is a special case of updating the graph state from tools where, in addition to the state update, the control flow is included as well.
Important
If you want to use tools that return Command
, you can use the prebuilt create_react_agent
/ ToolNode
components, or else implement your own logic:
Let's now take a closer look at the different multi-agent architectures.
Network¶
In this architecture, agents are defined as graph nodes. Each agent can communicate with every other agent (many-to-many connections) and can decide which agent to call next. This architecture is good for problems that do not have a clear hierarchy of agents or a specific sequence in which agents should be called.
API Reference: ChatOpenAI | Command | StateGraph | START | END
from typing import Literal
from langchain_openai import ChatOpenAI
from langgraph.types import Command
from langgraph.graph import StateGraph, MessagesState, START, END
model = ChatOpenAI()
def agent_1(state: MessagesState) -> Command[Literal[""agent_2"", ""agent_3"", END]]:
# you can pass relevant parts of the state to the LLM (e.g., state[""messages""])
# to determine which agent to call next. a common pattern is to call the model
# with a structured output (e.g. force it to return an output with a ""next_agent"" field)
response = model.invoke(...)
# route to one of the agents or exit based on the LLM's decision
# if the LLM returns ""__end__"", the graph will finish execution
return Command(
goto=response[""next_agent""],
update={""messages"": [response[""content""]]},
)
def agent_2(state: MessagesState) -> Command[Literal[""agent_1"", ""agent_3"", END]]:
response = model.invoke(...)
return Command(
goto=response[""next_agent""],
update={""messages"": [response[""content""]]},
)
def agent_3(state: MessagesState) -> Command[Literal[""agent_1"", ""agent_2"", END]]:
...
return Command(
goto=response[""next_agent""],
update={""messages"": [response[""content""]]},
)
builder = StateGraph(MessagesState)
builder.add_node(agent_1)
builder.add_node(agent_2)
builder.add_node(agent_3)
builder.add_edge(START, ""agent_1"")
network = builder.compile()
Supervisor¶
In this architecture, we define agents as nodes and add a supervisor node (LLM) that decides which agent nodes should be called next. We use Command
to route execution to the appropriate agent node based on supervisor's decision. This architecture also lends itself well to running multiple agents in parallel or using map-reduce pattern.
API Reference: ChatOpenAI | Command | StateGraph | START | END
from typing import Literal
from langchain_openai import ChatOpenAI
from langgraph.types import Command
from langgraph.graph import StateGraph, MessagesState, START, END
model = ChatOpenAI()
def supervisor(state: MessagesState) -> Command[Literal[""agent_1"", ""agent_2"", END]]:
# you can pass relevant parts of the state to the LLM (e.g., state[""messages""])
# to determine which agent to call next. a common pattern is to call the model
# with a structured output (e.g. force it to return an output with a ""next_agent"" field)
response = model.invoke(...)
# route to one of the agents or exit based on the supervisor's decision
# if the supervisor returns ""__end__"", the graph will finish execution
return Command(goto=response[""next_agent""])
def agent_1(state: MessagesState) -> Command[Literal[""supervisor""]]:
# you can pass relevant parts of the state to the LLM (e.g., state[""messages""])
# and add any additional logic (different models, custom prompts, structured output, etc.)
response = model.invoke(...)
return Command(
goto=""supervisor"",
update={""messages"": [response]},
)
def agent_2(state: MessagesState) -> Command[Literal[""supervisor""]]:
response = model.invoke(...)
return Command(
goto=""supervisor"",
update={""messages"": [response]},
)
builder = StateGraph(MessagesState)
builder.add_node(supervisor)
builder.add_node(agent_1)
builder.add_node(agent_2)
builder.add_edge(START, ""supervisor"")
supervisor = builder.compile()
Check out this tutorial for an example of supervisor multi-agent architecture.
Supervisor (tool-calling)¶
In this variant of the supervisor architecture, we define a supervisor agent which is responsible for calling sub-agents. The sub-agents are exposed to the supervisor as tools, and the supervisor agent decides which tool to call next. The supervisor agent follows a standard implementation as an LLM running in a while loop calling tools until it decides to stop.
API Reference: ChatOpenAI | InjectedState | create_react_agent
from typing import Annotated
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import InjectedState, create_react_agent
model = ChatOpenAI()
# this is the agent function that will be called as tool
# notice that you can pass the state to the tool via InjectedState annotation
def agent_1(state: Annotated[dict, InjectedState]):
# you can pass relevant parts of the state to the LLM (e.g., state[""messages""])
# and add any additional logic (different models, custom prompts, structured output, etc.)
response = model.invoke(...)
# return the LLM response as a string (expected tool response format)
# this will be automatically turned to ToolMessage
# by the prebuilt create_react_agent (supervisor)
return response.content
def agent_2(state: Annotated[dict, InjectedState]):
response = model.invoke(...)
return response.content
tools = [agent_1, agent_2]
# the simplest way to build a supervisor w/ tool-calling is to use prebuilt ReAct agent graph
# that consists of a tool-calling LLM node (i.e. supervisor) and a tool-executing node
supervisor = create_react_agent(model, tools)
Hierarchical¶
As you add more agents to your system, it might become too hard for the supervisor to manage all of them. The supervisor might start making poor decisions about which agent to call next, or the context might become too complex for a single supervisor to keep track of. In other words, you end up with the same problems that motivated the multi-agent architecture in the first place.
To address this, you can design your system hierarchically. For example, you can create separate, specialized teams of agents managed by individual supervisors, and a top-level supervisor to manage the teams.
API Reference: ChatOpenAI | StateGraph | START | END | Command
from typing import Literal
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.types import Command
model = ChatOpenAI()
# define team 1 (same as the single supervisor example above)
def team_1_supervisor(state: MessagesState) -> Command[Literal[""team_1_agent_1"", ""team_1_agent_2"", END]]:
response = model.invoke(...)
return Command(goto=response[""next_agent""])
def team_1_agent_1(state: MessagesState) -> Command[Literal[""team_1_supervisor""]]:
response = model.invoke(...)
return Command(goto=""team_1_supervisor"", update={""messages"": [response]})
def team_1_agent_2(state: MessagesState) -> Command[Literal[""team_1_supervisor""]]:
response = model.invoke(...)
return Command(goto=""team_1_supervisor"", update={""messages"": [response]})
team_1_builder = StateGraph(Team1State)
team_1_builder.add_node(team_1_supervisor)
team_1_builder.add_node(team_1_agent_1)
team_1_builder.add_node(team_1_agent_2)
team_1_builder.add_edge(START, ""team_1_supervisor"")
team_1_graph = team_1_builder.compile()
# define team 2 (same as the single supervisor example above)
class Team2State(MessagesState):
next: Literal[""team_2_agent_1"", ""team_2_agent_2"", ""__end__""]
def team_2_supervisor(state: Team2State):
...
def team_2_agent_1(state: Team2State):
...
def team_2_agent_2(state: Team2State):
...
team_2_builder = StateGraph(Team2State)
...
team_2_graph = team_2_builder.compile()
# define top-level supervisor
builder = StateGraph(MessagesState)
def top_level_supervisor(state: MessagesState) -> Command[Literal[""team_1_graph"", ""team_2_graph"", END]]:
# you can pass relevant parts of the state to the LLM (e.g., state[""messages""])
# to determine which team to call next. a common pattern is to call the model
# with a structured output (e.g. force it to return an output with a ""next_team"" field)
response = model.invoke(...)
# route to one of the teams or exit based on the supervisor's decision
# if the supervisor returns ""__end__"", the graph will finish execution
return Command(goto=response[""next_team""])
builder = StateGraph(MessagesState)
builder.add_node(top_level_supervisor)
builder.add_node(""team_1_graph"", team_1_graph)
builder.add_node(""team_2_graph"", team_2_graph)
builder.add_edge(START, ""top_level_supervisor"")
builder.add_edge(""team_1_graph"", ""top_level_supervisor"")
builder.add_edge(""team_2_graph"", ""top_level_supervisor"")
graph = builder.compile()
Custom multi-agent workflow¶
In this architecture we add individual agents as graph nodes and define the order in which agents are called ahead of time, in a custom workflow. In LangGraph the workflow can be defined in two ways:
-
Explicit control flow (normal edges): LangGraph allows you to explicitly define the control flow of your application (i.e. the sequence of how agents communicate) explicitly, via normal graph edges. This is the most deterministic variant of this architecture above — we always know which agent will be called next ahead of time.
-
Dynamic control flow (Command): in LangGraph you can allow LLMs to decide parts of your application control flow. This can be achieved by using
Command
. A special case of this is a supervisor tool-calling architecture. In that case, the tool-calling LLM powering the supervisor agent will make decisions about the order in which the tools (agents) are being called.
API Reference: ChatOpenAI | StateGraph | START
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START
model = ChatOpenAI()
def agent_1(state: MessagesState):
response = model.invoke(...)
return {""messages"": [response]}
def agent_2(state: MessagesState):
response = model.invoke(...)
return {""messages"": [response]}
builder = StateGraph(MessagesState)
builder.add_node(agent_1)
builder.add_node(agent_2)
# define the flow explicitly
builder.add_edge(START, ""agent_1"")
builder.add_edge(""agent_1"", ""agent_2"")
Communication and state management¶
The most important thing when building multi-agent systems is figuring out how the agents communicate.
A common, generic way for agents to communicate is via a list of messages. This opens up the following questions:
- Do agents communicate via handoffs or via tool calls?
- What messages are passed from one agent to the next?
- How are handoffs represented in the list of messages?
- How do you manage state for subagents?
Additionally, if you are dealing with more complex agents or wish to keep individual agent state separate from the multi-agent system state, you may need to use different state schemas.
Handoffs vs tool calls¶
What is the ""payload"" that is being passed around between agents? In most of the architectures discussed above, the agents communicate via handoffs and pass the graph state as part of the handoff payload. Specifically, agents pass around lists of messages as part of the graph state. In the case of the supervisor with tool-calling, the payloads are tool call arguments.
Message passing between agents¶
The most common way for agents to communicate is via a shared state channel, typically a list of messages. This assumes that there is always at least a single channel (key) in the state that is shared by the agents (e.g., messages
). When communicating via a shared message list, there is an additional consideration: should the agents share the full history of their thought process or only the final result?
Sharing full thought process¶
Agents can share the full history of their thought process (i.e., ""scratchpad"") with all other agents. This ""scratchpad"" would typically look like a list of messages. The benefit of sharing the full thought process is that it might help other agents make better decisions and improve reasoning ability for the system as a whole. The downside is that as the number of agents and their complexity grows, the ""scratchpad"" will grow quickly and might require additional strategies for memory management.
Sharing only final results¶
Agents can have their own private ""scratchpad"" and only share the final result with the rest of the agents. This approach might work better for systems with many agents or agents that are more complex. In this case, you would need to define agents with different state schemas.
For agents called as tools, the supervisor determines the inputs based on the tool schema. Additionally, LangGraph allows passing state to individual tools at runtime, so subordinate agents can access parent state, if needed.
Indicating agent name in messages¶
It can be helpful to indicate which agent a particular AI message is from, especially for long message histories. Some LLM providers (like OpenAI) support adding a name
parameter to messages — you can use that to attach the agent name to the message. If that is not supported, you can consider manually injecting the agent name into the message content, e.g., <agent>alice</agent><message>message from alice</message>
.
Representing handoffs in message history¶
Handoffs are typically done via the LLM calling a dedicated handoff tool. This is represented as an AI message with tool calls that is passed to the next agent (LLM). Most LLM providers don't support receiving AI messages with tool calls without corresponding tool messages.
You therefore have two options:
- Add an extra tool message to the message list, e.g., ""Successfully transferred to agent X""
- Remove the AI message with the tool calls
In practice, we see that most developers opt for option (1).
State management for subagents¶
A common practice is to have multiple agents communicating on a shared message list, but only adding their final messages to the list. This means that any intermediate messages (e.g., tool calls) are not saved in this list.
What if you do want to save these messages so that if this particular subagent is invoked in the future you can pass those back in?
There are two high-level approaches to achieve that:
- Store these messages in the shared message list, but filter the list before passing it to the subagent LLM. For example, you can choose to filter out all tool calls from other agents.
- Store a separate message list for each agent (e.g.,
alice_messages
) in the subagent's graph state. This would be their ""view"" of what the message history looks like.
Using different state schemas¶
An agent might need to have a different state schema from the rest of the agents. For example, a search agent might only need to keep track of queries and retrieved documents. There are two ways to achieve this in LangGraph:
- Define subgraph agents with a separate state schema. If there are no shared state keys (channels) between the subgraph and the parent graph, it's important to add input / output transformations so that the parent graph knows how to communicate with the subgraphs.
- Define agent node functions with a private input state schema that is distinct from the overall graph state schema. This allows passing information that is only needed for executing that particular agent.",Overview,LangGraph Documentation,2025-09-13
2025-09-13T19:28:55.497031Z,https://www.anthropic.com/claude-code?utm_source=google&utm_medium=paid_search_coder&utm_campaign=acq_code_us_q3&utm_term=ai%20development%20agent&gclsrc=aw.ds&gad_source=1&gad_campaignid=22795617257&gbraid=0AAAAA99jmqtS6N4o1tcqBXZUwkMLgCuVz&gclid=Cj0KCQjwrJTGBhCbARIsANFBfgvieOYFRxnEjOOTSFbTOCOJjZT2j4o_eIU1NO6JzbbXPFcBjn22oNYaAgE7EALw_wcB,https://www.anthropic.com/claude-code?gad_campaignid=22795617257&gad_source=1&gbraid=0AAAAA99jmqtS6N4o1tcqBXZUwkMLgCuVz,www.anthropic.com,Claude Code: Deep coding at terminal velocity,,,[],[],[],[],"[""Claude Code enhances coding efficiency by integrating with existing tools."", ""It maps codebases and manages complex edits seamlessly."", ""Developers report significant time savings and improved workflow.""]","Trusted by engineers at
Command-line AI, from concept to commit
Watch as Claude Code tackles an unfamiliar Next.js project, builds new functionality, creates tests, and fixes whatâs brokenâall from the command line. Join Boris and Cat as they show you what coding feels like when youâre not doing it alone.
Works with your IDEs
Claude works directly in VS Code and JetBrains, seeing your entire codebase instead of just isolated snippets. It understands your project structure and existing patterns, making suggestions that actually fit and presenting them directly in your code files. No copying and pastingâjust building.
... and your favorite command line tools
Your terminal is where real work happens. Claude Code connects with the tools that power developmentâdeployment, databases, monitoring, version control. Rather than adding another interface to juggle, it enhances your existing stack. Less context-switching.
What could you do with Claude Code?
Code onboarding
Claude Code maps and explains entire codebases in a few seconds. It uses agentic search to understand project structure and dependencies without you having to manually select context files.
Turn issues into PRs
Stop bouncing between tools. Claude Code integrates with GitHub, GitLab, and your command line tools to handle the entire workflowâreading issues, writing code, running tests, and submitting PRsâall from your terminal while you grab coffee.
Make powerful edits
Claude Codeâs understanding of your codebase and dependencies enables it to make powerful, multi-file edits that actually work.
Big fans of Claude Code
What developers say about Claude Code
Claude Code has dramatically accelerated our teamâs coding efficiency. I can now write EDA code in a notebookâpulling data, training a model, and evaluating it with basic metricsâand then ask Claude to convert that into a Metaflow pipeline. This process saves 1-2 days of routine (and often boring!) work per model.
Claude Code marks a threshold moment for AI in software development. At Intercom, it enables us to build applications we wouldnât have had bandwidth forâfrom AI labeling tools to ROI calculators for our Sales team. Its ability to handle complex, multi-step tasks sets it apart from alternatives. Claude Code is fundamentally changing whatâs possible for our engineering teams.
Get started with Claude Code
Pro
$17Per month with annual subscription discount; $200 billed up front. $20 if billed monthly.Max 5x
$100Per person billed monthlyMax 20x
$200Per person billed monthly
Team
$150Per person / month. Minimum 5 members.Enterprise
Education plan
A comprehensive universityâwide plan for institutions, including its students, faculty, and staff.
Student & faculty access
Comprehensive access for all university members at discounted rates
Academic research & learning mode
Dedicated API credits and educational features for student learning
Training & enablement
Resources for successful adoption across your institution",Claude Code: Deep coding at terminal velocity \ Anthropic,,
2025-09-13T20:07:10.803098Z,https://docs.replit.com/tutorials/vibe-coding-101,https://docs.replit.com/tutorials/vibe-coding-101,docs.replit.com,Vibe coding 101: from idea to deployed app,,,[],[],[],[],"[""Guide to 'vibe coding' process on Replit"", ""Builds interactive map of San Francisco parks"", ""Utilizes Replit Agent for development"", ""Leverages Replit Assistant for debugging"", ""Emphasizes iterative development and effective prompting""]","Matt Palmer
Head of Developer RelationsThe vibe coding philosophy
Vibe coding is less about writing every line of code and more about guiding AI tools with your vision and domain knowledge. It’s an iterative process of prompting, reviewing, and refining. Key takeaways from the video:- Conceptualize First: Start with a clear idea of what you want to build. Visualizing the end product helps, especially when prompting AI.
- Domain Knowledge is Power: Knowing relevant frameworks (like Leaflet.js for maps) or data sources (like OpenStreetMap) significantly improves AI-generated results.
- Iterative Development: Expect to debug and refine. AI tools are powerful, but they’re collaborators, not magic wands.
Project: San Francisco parks map
The goal is to build an interactive map displaying parks and public spaces in San Francisco. Problem Statement: The goal is an interactive map to discover parks and public spaces in San Francisco. Solution: An interactive tool using Leaflet.js and OpenStreetMap data. Key lessons:- Prompting AI effectively.
- Processing external data.
- Debugging and handling errors.
Building with Replit Agent
Replit Agent can scaffold entire projects, set up environments, and generate initial code.Crafting the Initial Prompt
- The goal: “Help me create a minimalist maps app to visualize San Francisco’s parks.”
- Key technologies: “You should use leaflet for map visualization and fetch data from OpenStreetMap.”
- Specific data types from OpenStreetMap (after research): Natural formations (woods, beaches, islets, cave entrances) and leisure locations (parks, gardens).
Attaching a Mockup
Agent's Process
-
Plan Creation: Agent outlines the steps it will take. Review and approve this plan.
-
Visual Preview: Agent streams a visual preview of the app’s UI.
- Environment Setup: Agent configures the development environment, installing necessary languages and packages—no manual setup required.
- Building the App: Agent writes the code for the front end and back end.
- Checkpoints: Agent creates checkpoints (Git commits) so you can roll back if something goes wrong.
Debugging with Agent
- Observe: The map loaded, but no data points appeared.
- Investigate: The console showed an error: “Failed to fetch map features error cannot read properties of undefined reading natural.”
-
Inform Agent: Paste the error message directly into the chat with Agent. Agent will attempt to debug and fix the issue.
Refining with Replit Assistant
Once Agent builds the MVP, switch to Replit Assistant for smaller, more targeted edits and refinements. Assistant is generally faster for these tasks.Improving Map Styling
Improving Map Styling
Adding Dark Mode
Adding Dark Mode
- Read files for context.
- Make changes to necessary files (e.g., theme providers, styles).
- Restart the app to apply changes.
Iterative Debugging with Assistant
Iterative Debugging with Assistant
- Initial implementation had a toggle that worked for the map but then disappeared. Feedback: “The toggle theme button works for the map, but it disappears when clicked. The theme toggle should be in the side nav and the theme should be applied to the side nav.”
- Issues with multiple toggle buttons and incorrect component references (
side nav
vs.sidebar
). Feedback & Guidance: “Now there are two toggle themes. One controls the map, the other controls the side nav. Make them into one in the side. Now and update the CSS.” When Assistant made an incorrect assumption (e.g.SideNav
component), explicitly pointing it to the correct file (@Sidebar
) helps. - Final fix to ensure the dark mode toggle in the sidebar correctly toggled the map theme to Carto Dark. Feedback: “Now the dark mode toggle in @Sidebar does not toggle the map to carto dark.”
Publishing your application
Replit makes publishing straightforward.- Select the Publish button.
- Agent suggests a publishing configuration (e.g., app name, build and run commands). Review and confirm.
- If your app uses API keys or other sensitive information, store them in Secrets. Agent will use these securely.
-
Select Publish. Replit bundles your app and makes it live on the web.
park-mapper.replit.app
). Changes made in your development environment won’t affect the published version until you click Republish.
Recap and next steps
This tutorial went from an idea to a published interactive map application without writing a single line of code manually. Replit Agent was used for the heavy lifting and Replit Assistant for refinements, leveraging domain knowledge and an iterative debugging process. Potential Next Steps for the Park Mapper App:- Add a database to store park data persistently (avoiding re-fetch on every load).
- Allow users to save or favorite parks.
- Implement advanced filtering.
- Improve styling and add custom icons for map markers.
- Enhance mobile responsiveness (e.g., ensuring filters are accessible on mobile).",Replit Docs,Matt Palmer,2023-10-01
2025-09-13T20:11:55.897787Z,https://medium.com/@yaelg/product-manager-pm-step-by-step-tutorial-building-machine-learning-products-ffa7817aa8ab,https://medium.com/@yaelg/product-manager-pm-step-by-step-tutorial-building-machine-learning-products-ffa7817aa8ab,medium.com,The Step-By-Step PM Guide to Building Machine Learning Based Products,,,[],[],[],[],"[""Introduction to machine learning for product managers."", ""Step-by-step guide to developing ML-based products."", ""Emphasis on collaboration between disciplines.""]","The Step-By-Step PM Guide to Building Machine Learning Based Products
What Product Managers Need to Know About Machine Learning Is Science, but Not Rocket Science
It’s time for every product manager, entrepreneur or business leader to get up to speed on machine learning. Even if you’re not building the next chatbot or self driving car, you’ll probably need to use machine learning in your product sooner rather than later to stay competitive. The good news is you don’t need to invent the technology (though kudos if you do), just leverage what already exists. Tech companies have open sourced tools and platforms (Amazon AI, TensorFlow, originally developed by Google, and many others) that make machine learning accessible to virtually any company today.
When I started in machine learning I knew next to nothing about it, yet in a relatively short time I was leading the development of products with machine learning at their very core (such as this). My goal is to give you a good enough understanding of both the technology and the process of developing ML products to get you started quickly. This is a step-by-step guide to becoming an effective PM in an organization that leverages machine learning to achieve business goals.
While ML is an incredibly technical space, many of the fundamentals you need to understand to maximize business impact have little to do with developing complex algorithms. They’re about ensuring you ask the right questions, understand the process of developing ML models, and structure an organization that fosters constant collaboration between disciplines rather than treating data science (the organization creating those models) as a “black box” that will magically generate insights.
This tutorial has 6 parts:
My goal is to illustrate core concepts that are broadly applicable and form a basis from which you can grow your knowledge in the areas that are most relevant to your business; therefore there may be cases where I’m oversimplifying or not addressing all possible applications or aspects of the science for the sake of clarity.
Let’s get started with part 1: What Machine Learning Can Do for Your Business and How to Figure It Out.
Many thanks to Gil Arditi, Yael Avidan, Eran Davidov and Gal Gavish for their invaluable feedback, and special thanks to Arvind Ganesan who taught me so much of what I know about machine learning. Any mistakes are entirely my own.
I write about life purpose, mindset and creativity for professionals who want more from life than they’re experiencing at https://producthumans.com/",The Step-By-Step PM Guide to Building Machine Learning Based Products,Yael Gavish,2017-07-25
2025-09-13T20:31:06.493214Z,https://www.productteacher.com/articles/breaking-into-ai-product-management,https://www.productteacher.com/articles/breaking-into-ai-product-management,www.productteacher.com,Breaking into AI Product Management,,,[],[],[],[],"[""AI product management requires a strong grasp of technical concepts and problem-solving skills."", ""Understanding data pipelines and managing data quality are crucial for AI products."", ""AI models necessitate continuous iteration and retraining to improve over time.""]","Breaking into AI Product Management
Artificial Intelligence (AI) is rapidly transforming industries, and product management roles in this field are both in high demand and uniquely challenging.
I’ve noticed that many early-career PMs are excited to pivot into AI, and I applaud this enthusiasm! But, I want to be clear: if you’re a product manager with strong skills but no direct experience in AI, the path to transitioning into AI product management requires targeted skill-building, strategic networking, and gaining experience in areas of AI that align with your career goals.
In other words, it’s going to take hard, focused work to make it happen. But, it’ll be worth it.
To help make this hard work a little bit easier, I’ve written this guide to help you navigate that transition, from understanding AI fundamentals to positioning yourself as a compelling AI PM candidate.
What Makes AI Product Management Different?
As a PM, you're likely already familiar with managing product development cycles, aligning stakeholders, and driving outcomes based on customer needs. All of these are still relevant for AI products!
However, AI product management is different in a few critical ways, and understanding these differences is essential for your success.
AI Products Are Data-Centric
Traditional products may rely on feature-based roadmaps and direct feedback loops from users. In contrast, AI products are heavily dependent on data.
Data isn’t an optional nice-to-have resource: it’s literally the foundation of the product.
As an AI PM, you’ll need to become intimately familiar with the data pipelines that fuel your product’s AI models. Understanding the lifecycle of data (how it’s collected, labeled, and processed) is crucial. This involves working with data engineers and scientists to ensure the data is clean, relevant, and representative.
Inadequate or biased data can break the performance of AI models, leading to poor user experiences.
Many new AI PMs don’t realize how much time will be spent on data preparation, management, and validation. Before focusing on features, understand the data! It’s essential for every aspect of AI development, from model accuracy to user experience.
I can tell you firsthand - a lot of AI PM work is “dirty work” where you’re going to validate the data yourself, and where you’ll manually correct bad labels or ask your data labelers to redo an entire set of labels due to some misunderstanding.
Transparently, you’ll make a lot more progress by being the first person to label the data, rather than being the final person to sign off on the data. That way, you can train your data labeling team to label exactly the way that you do!
AI Models Require Uncertain R&D Cycles
Unlike traditional software, AI models are not built and launched in a one-time sprint. Instead, they require continuous retraining and updating as new data flows in.
As an AI PM, you need to understand that AI products evolve over time and improve through repeated iteration. AI models depend on high-quality data input and constant feedback loops to enhance their predictions or classifications.
How do you manage iterations when outcomes are unpredictable? The answer lies in setting realistic expectations.
AI models will fail or make mistakes, so what matters is how you handle that failure. You’ll need to define success criteria that account for experimentation, not just perfect accuracy.
Work closely with your data science team to set up a framework for continuous model evaluation. You’ll need to align product goals with the technical realities of how models improve over time - especially when it comes to metrics like precision, recall, and accuracy.
Collaboration with Technical Teams is Mission-Critical
One of the most significant shifts you’ll experience as an AI PM is the depth of technical collaboration required.
While PMs always work cross-functionally, AI PMs need to develop a close relationship with data scientists and ML engineers. You'll need to understand their language to facilitate productive discussions about model architectures, data requirements, and performance metrics.
Get comfortable with being surrounded by PhD’s! You’re going to be the least academically-credentialed person in the room, and that’s by design. Your job is to represent the business - let your scientists represent the science.
But, as a product manager, you must guide your team of talented PhD’s to make the right business decisions, and that sometimes means telling them to stop experiments early or telling them that certain research paths are no longer viable to explore.
Intimidating? Yes. But, it’s part of the job!
How technical do you need to be? Well, you don’t need to code, but you must grasp enough technical knowledge to ask the right questions and make informed decisions. You’ll need to be comfortable discussing algorithms, model performance, and trade-offs between explainability and complexity.
Build trust with your technical team by showing a willingness to learn. Set up regular syncs where you can dive into technical discussions and show how their work ties directly to user outcomes. By understanding their challenges, you’ll be able to align on shared goals more effectively.
Ethical Considerations Matter
AI products can unintentionally reinforce biases or make decisions that have ethical implications.
As an AI PM, you have a responsibility to ensure fairness, accountability, and transparency in the models your product is built on. Bias in training data can lead to skewed predictions that negatively impact certain user groups, which can be harmful to both individuals and your brand.
How do you ensure your models are ethical? Start by questioning the data you’re using. Where is it coming from? Does it represent the diversity of your user base? Then, work with your data science team to implement fairness checks and bias audits.
Don’t wait until the end of the development process to think about ethics. Build ethical considerations into the earliest stages of product development. Ask hard questions about data, and remember that it’s better to err on the side of caution.
Key AI Technologies and Their Use Cases
Understanding the AI landscape requires familiarity with its different technologies. AI isn’t a monolith; instead, it’s a collection of specialized technologies, each with its own use cases and challenges.
As an AI PM, you’ll be expected to know which type of AI technology fits the problem you’re solving.
How should you read this section of this guide? Well, I don’t recommend that you master every aspect of every AI technology. Identify the one area that you’re eager to learn more about, and focus your attention there.
Why do I say this? Well, my wife works in LLMs, and I work in computer vision. Let me tell you - these two AI technologies barely overlap at all. (The one place they do is “transformers”, but that’s really not something that a PM needs to deeply understand.)
So, as you read the below, identify the one AI technology that you’d like to dig deeper into. Remember, product management is all about prioritization and focus - don’t try to boil the ocean!
Large Language Models (LLMs)
LLMs, like GPT-4, excel at understanding and generating human-like text. These models are particularly powerful in natural language processing (NLP) applications, such as chatbots, content generation, and virtual assistants. Here are some examples of products:
Customer support chatbots (e.g., Google's Dialogflow, Intercom)
Content generation tools (e.g., OpenAI's GPT-4, Jasper)
Engineering copilots (e.g., GitHub Copilot)
When working with LLMs, be aware of limitations in their training data. LLMs are prone to generating plausible-sounding but incorrect information, so products using these models must be designed to validate output accuracy.
Computer Vision
Computer vision (CV) enables machines to interpret visual inputs like images and videos. This technology is widely used in fields like autonomous driving, medical imaging, and security. Here are some examples of products:
Real-time performance is critical for many computer vision applications. Work closely with your engineering team to optimize latency and ensure models can process images in milliseconds, especially for mission-critical applications like self-driving cars.
Recommendation Algorithms
Recommendation algorithms analyze user behavior to suggest personalized content, products, or media. These systems are widely used in e-commerce, social media, and streaming services to increase engagement.
Recommendation algorithms rely on user feedback loops, which can amplify both positive and negative experiences. Make sure you’re continuously monitoring and fine-tuning these models to avoid undesirable feedback loops, such as promoting low-quality content.
Other AI/ML Technologies
Here are some additional AI/ML techniques used in a wide range of industries:
Speech recognition: Converts spoken language into text, widely used in virtual assistants (e.g., Google Assistant, Alexa) and transcription services (e.g., Otter.ai).
Reinforcement learning: Trains agents to make decisions by rewarding desired behaviors. It’s commonly used in robotics and gaming.
Predictive analytics: Uses historical data to predict future outcomes, useful in industries like finance and healthcare.
Understanding the specific AI technology your product will use, as well as its typical applications, will help you define product features, set metrics, and collaborate with technical teams.
Acquiring AI-Specific Knowledge and Skills
Right, so now we’ve selected the one AI technology that we’re interested in. (I can’t emphasize this enough - please do not attempt to learn LLMs, CV, recommendation algos, and other techniques in a single shot. It’s not going to help.)
Now that we’ve selected the single AI technology, let’s discuss how to flesh out the knowledge and skills that we need to serve as effective product managers for this particular AI technology.
You’ll need to acquire a foundation in machine learning (ML) and AI concepts that allows you to confidently work with technical teams, make informed decisions, and help shape the development of AI-driven products.
While you don't need to become a machine learning engineer, you must gain enough technical knowledge to bridge the gap between product and engineering teams effectively. Here’s how to get started:
Grasp AI basics
Establish a framework for additional learning
Master AI-specific metrics
Grasp AI Basics
The foundational concepts of AI may seem overwhelming at first, but focusing on core areas will help you gain the knowledge necessary to thrive as an AI PM.
Machine learning: Learn the different types of machine learning (supervised, unsupervised, and reinforcement learning) and their respective applications. Understand key concepts such as training data, model overfitting, generalization, and cross-validation. Knowing how models learn from data and make predictions is essential for any AI PM.
Deep learning: Dive into neural networks and deep learning, especially if you’re interested in areas like computer vision or natural language processing. Deep learning models, which mimic the structure of the human brain, are behind some of the most significant advances in AI, from facial recognition to speech synthesis.
Model lifecycle: AI models follow a lifecycle that includes data preparation, model training, validation, deployment, and continuous monitoring. As an AI PM, you’ll need to understand how models are developed and maintained over time. AI models, unlike traditional software products, require retraining and adjustments based on the changing nature of data inputs and user behavior.
Data pipelines: Since AI depends on high-quality data, learn about data ingestion, transformation, labeling, and storage. Get familiar with the challenges of ensuring data cleanliness and robustness.
Here are some resources to help you get started:
Coursera’s AI for Everyone: A great starting point for understanding AI at a high level.
Google's Machine Learning Crash Course: Provides a more hands-on introduction to ML concepts.
Fast.ai: Offers practical deep learning courses tailored for non-experts.
BlueDot Impact: Courses that support people to develop the knowledge, skills and connections to pursue a high-impact career in AI
How deep do you need to go with AI concepts? Start by learning enough to participate in technical discussions confidently. While you won’t be building models yourself, having a strong understanding of how AI works will allow you to ask the right questions and ensure product decisions are data-informed.
The most effective AI PMs are those who understand the ""why"" behind model performance. When a model’s predictions fall short, your role is to dive into the data, collaborate with data scientists to troubleshoot, and adjust the product roadmap based on model limitations. It’s a continual process of learning and iteration!
Establish a framework for additional learning
When you’re learning AI as a PM, it’s important to structure your education strategically so you can build on each concept incrementally. Here’s a framework that can help guide your learning:
Start with a High-Level Overview: Begin by taking foundational courses. These will give you a broad understanding of how AI works without getting lost in technical details.
Choose a Domain to Deep Dive Into: Pick one specific area of AI that interests you—such as NLP, computer vision, or recommendation systems—and immerse yourself in learning about that area. Take a hands-on course, read relevant research papers, and work through real-world use cases.
Build a Side Project: Apply what you’ve learned by building a simple AI-driven project. This could be a chatbot using an open-source NLP library or a basic image classifier using pre-trained models. Building a side project will give you practical experience in the model lifecycle, data management, and deployment challenges.
Repeat Steps 2 and 3: After your first deep dive, repeat the process for another AI domain. This iterative approach will help you gain a well-rounded understanding of various AI applications, making you more versatile as an AI PM.
A well-structured learning journey is key to transitioning into AI. Many PMs try to learn everything at once and get overwhelmed. Instead, focus on understanding one area deeply before moving on to the next.
Depth of knowledge in AI is far more valuable than breadth.
Mastering AI Product Metrics
In AI product management, traditional KPIs like user engagement, revenue, and feature adoption still matter! However, for AI products, additional metrics come into play:
Model accuracy: This measures how often your model’s predictions are correct. However, accuracy alone can be misleading, especially when dealing with imbalanced datasets.
Precision and recall: These metrics help you understand the trade-off between false positives and false negatives. In products where the cost of a false positive is high (e.g., fraud detection), precision is more important. In others, where missing a key event is costly (e.g., medical diagnosis), recall takes priority.
F1 score: This is a harmonic mean of precision and recall and is especially useful when your data is imbalanced. A high F1 score ensures that both precision and recall are optimized.
Model interpretability: Can your model's outputs be easily understood by non-technical stakeholders? Interpretability is particularly important in high-stakes domains like healthcare or finance, where users and regulators need to trust the model’s decisions.
Latency and throughput: In real-time AI applications (e.g., voice assistants or autonomous driving), these metrics measure how quickly the model can process inputs and deliver outputs. Latency refers to the time it takes for the model to return a result, while throughput measures how many operations the model can perform in a given timeframe.
Which metrics matter most? It depends on your product’s use case.
For instance, in a social media recommendation engine, precision and recall are vital for personalizing content.
In a real-time application like self-driving cars, low latency is crucial. Understand your product’s specific needs before determining which metrics to prioritize.
AI PMs often struggle to balance accuracy with interpretability. Highly accurate models may be too complex for end-users to understand. As a PM, you need to make trade-offs between accuracy, interpretability, and speed based on your product’s user experience and goals.
Build AI-Specific Experience
Once you have a solid understanding of AI principles, the next step is to gain hands-on experience. Transitioning into AI product management is not just about learning concepts; you need to demonstrate that you’ve applied those concepts in real-world scenarios.
Work on AI-Adjacent Projects
Leverage your current role to find projects that involve AI or data science. Even if you’re not an AI PM yet, there are plenty of opportunities to contribute to AI-driven initiatives in most companies. This could involve scoping out a feature that uses machine learning, collaborating with a data science team, or working on a recommendation algorithm for personalized content.
How can you find AI projects in your current role? Start by identifying any products or features that rely on data or automation. For example, you might work on improving personalization in a user interface or optimizing search results using a recommendation engine.
Focus on the business value of AI features, not just the technology. Many AI initiatives fail because they don’t directly tie into customer or business outcomes. Make sure your AI efforts are grounded in solving real user problems and driving measurable impact.
Some AI-related projects within your current organization might include:
Collaborating with a data science team on a feature that uses machine learning (e.g., recommendation engines or personalization algorithms).
Helping to scope AI features, such as chatbots or image recognition systems, for your existing product.
Running user research to understand how AI-driven insights could improve user experiences.
By working on projects that involve AI or ML, even peripherally, you’ll build valuable experience and create stories for future interviews.
Participate in Hackathons and AI Competitions
AI hackathons and competitions like those on Kaggle offer a great way to work on real-world AI problems in a team environment. These experiences provide hands-on learning and show future employers you’re serious about breaking into AI.
Participating in hackathons can give you a crash course in AI development. You’ll learn how to work under pressure, make quick decisions about data and models, and deliver an MVP (Minimum Viable Product) in a short amount of time. The experience you gain from these events is invaluable when transitioning into AI product management.
Launch Your Own AI Side Project
One of the most effective ways to demonstrate your readiness for AI product management is to build and launch your own AI-driven side project. This is where your hands-on learning comes to life. A side project allows you to explore the product lifecycle of an AI system, from idea generation to deployment, and demonstrates your initiative to potential employers.
What kind of AI side project should you build?
Start small.
Pick a domain you’re passionate about and leverage existing pre-trained models to speed up development. For example, you could create a personalized movie recommendation bot, an image classifier for identifying plant species, or even a chatbot that answers customer service queries.
By launching even a small AI product, you’ll learn the nuances of deploying machine learning models, including testing, feedback loops, and iteration.
Step 1: Identify a Problem: Look for an everyday problem that could be solved using AI. Maybe you notice inefficiencies in how certain tasks are performed in your daily life or within your organization.
Step 2: Choose a Pre-trained Model: Use platforms like Hugging Face or Google Cloud’s AutoML to access pre-trained models. These allow you to focus on building a product without needing to develop a model from scratch.
Step 3: Develop the Product: Build a simple UI and integrate the AI model into it. For example, if you’re building a recommendation system, you can use a pre-trained NLP model to analyze user preferences and suggest items.
Step 4: Deploy and Iterate: Once your MVP is live, test it, gather feedback, and improve the model based on user inputs. AI projects are iterative by nature, so use this as a learning opportunity to refine both your product and your understanding of the model.
Building an AI side project is more than just coding. Treat it like a full product development cycle. Develop user personas, define your problem statement, set KPIs, and align model outputs with business outcomes. Even if your side project isn’t commercial, the way you think about it from a product perspective will demonstrate your readiness for an AI PM role.
Once you’ve launched your AI side project, share it widely. Showcase your project on GitHub, write about your learnings on Medium, and include it in your portfolio. You’ll not only attract attention from potential employers but also gain valuable feedback from the AI community.
Share the process, not just the result. AI products are about learning from iterations, so be transparent about what worked, what didn’t, and how you improved. Sharing this thought process makes you a stronger candidate because it highlights your problem-solving skills and willingness to learn from setbacks.
Networking Strategies for AI Product Management
Building relationships in the AI space is critical for breaking into AI product management. Networking helps you stay informed about industry trends, discover job opportunities, and build credibility within the AI community.
Here’s how you can take a strategic approach to networking in AI!
Attend AI Conferences and Meetups
AI conferences such as NeurIPS or O'Reilly AI offer excellent opportunities to network with professionals in the field. You’ll hear about the latest trends and challenges in AI, which will give you insights into what AI PMs are currently working on.
Local AI meetups or virtual events can also provide valuable networking opportunities and a chance to meet AI professionals in your area.
What should you focus on at AI conferences?
Well, first off, don’t try to attend every session.
Focus on talks and workshops that align with your interests and career goals. AI product management sits at the intersection of business and technology, so attending sessions on product strategy, ethics in AI, or the latest developments in NLP or computer vision can be incredibly beneficial.
Be proactive at events. Don’t just listen to speakers; instead, make sure you’re asking the right questions.
Prepare thoughtful questions for the speakers, introduce yourself to attendees, and don’t hesitate to approach people after sessions.
Networking at conferences is one of the best ways to get noticed in the AI community. And, the AI community is tight-knit; people know people! If someone is willing to advocate for you, you’re much more likely to break into AI product management.
Leverage LinkedIn
LinkedIn is an incredibly powerful tool for connecting with AI professionals. Use it to identify AI PMs, data scientists, and engineers in companies or sectors you’re interested in, and reach out for informational interviews.
Start with Common Ground: Mention something specific about their work, such as a recent product launch or research paper they were involved in. People are more likely to respond when they see genuine interest in what they do.
Ask for Advice, Not a Job: Focus on learning about their journey into AI, the challenges they face in AI PM, and what they believe are key skills to develop. This will help you better understand the space while also building meaningful connections.
Don’t just reach out for the sake of networking. Approach these conversations with specific questions and a learning mindset. Ask about how they handle product iterations with ML teams, how they balance ethical considerations, or how they’ve managed failures in AI projects. These questions not only show your interest but also help you learn from their experiences.
Join online communities
There are several online communities dedicated to AI, such as Reddit’s r/MachineLearning, AI-focused Slack groups, and forums like Towards Data Science. Participating in these groups allows you to engage in discussions, ask questions, and learn from a wide range of AI professionals.
Consistency is key in online communities. Don’t just post once and disappear—regular engagement is how you’ll build relationships. Share your own learnings from AI side projects, comment on discussions about emerging trends, and help others where you can. Your participation will make you visible in the community, and over time, you’ll develop a network of peers and mentors who can support your journey.
Mentorship
Seek out a mentor who works in AI product management.
A mentor can help guide you through the transition process, review your projects or resume, and offer insights on navigating the field.
Many AI PMs are open to sharing their knowledge, and formal mentoring platforms like MentorCruise can facilitate these connections.
Tailor Your Resume and Portfolio for AI Roles
When applying for AI PM roles, your resume and portfolio need to highlight your AI experience and demonstrate that you have the technical know-how to collaborate with data scientists and ML engineers.
Emphasize Data-Driven Decision Making
AI products are built on data, so any experience you have with data analysis, predictive modeling, or personalization should be front and center on your resume. Highlight specific examples where you worked with data teams or implemented data-driven features.
Example: ""Collaborated with the data science team to design and implement a recommendation engine, resulting in a 15% increase in user engagement.""
Hiring managers are looking for PMs who can speak the language of data. Show that you understand how data fuels AI products by emphasizing your experience with metrics, data analysis, and any work you’ve done with data scientists or ML teams.
Showcase Collaboration with Technical Teams
Since AI PMs work closely with technical teams, demonstrating that you can bridge the gap between product and engineering is crucial. Highlight any cross-functional projects where you led initiatives that required close collaboration with data scientists or engineers.
Example: ""Led a cross-functional team of engineers, data scientists, and UX designers to develop an AI-powered feature for personalized content recommendations, from ideation to launch.""
AI PM roles demand strong technical collaboration skills. Don’t just mention that you worked with engineers; highlight how you facilitated those interactions and ensured that technical and business goals aligned. This will show that you’re capable of leading complex AI projects.
Highlight AI Projects
Even if your experience with AI is limited to side projects or hackathons, include those in your portfolio. Walk through your process of identifying a problem, selecting an AI model, and deploying the solution. Explain the challenges you faced, how you iterated, and what results you achieved.
Example: ""Developed an AI chatbot for customer support, which reduced response times by 20%. Iterated on the model based on user feedback to improve the accuracy of responses.""
Your portfolio should tell a story. Don’t just list the projects; instead, showcase the decision-making process behind them. Explain why you chose certain AI models, how you defined success metrics, and how the product evolved through iterations. This narrative will demonstrate your deep understanding of the AI product lifecycle.
Quantify Your Impact
Numbers speak volumes. Whenever possible, quantify the impact of your work. Whether it’s increased user engagement, reduced operational costs, or improved model accuracy, metrics make your contributions tangible.
Example: ""Implemented an AI-powered lead scoring system, increasing sales conversions by 25%.""
AI products are measured in terms of their outcomes, not just their features. Highlight the business impact of your AI initiatives by showing how they improved user experience, efficiency, or revenue. Quantifying your achievements helps to frame your AI experience in a way that hiring managers will appreciate.
Prepare for AI PM Interviews
AI product management interviews differ from traditional PM interviews in a few key ways. While product thinking and leadership skills remain critical, AI PM roles demand a strong grasp of technical concepts and the ability to solve problems that arise from the unique challenges of working with AI and machine learning models.
Understand Common AI Product Challenges
AI products come with their own set of challenges, such as limited or biased training data, model drift over time, and the complexities of deploying machine learning models in real-world environments. Interviewers will expect you to demonstrate awareness of these challenges and have a strategy for addressing them.
Example Challenge - Data Bias: AI models are only as good as the data they’re trained on. If the training data is biased, the model will be too, leading to skewed outcomes. For example, a facial recognition model trained on a dataset lacking diversity might struggle to identify people of different ethnic backgrounds.
How to Prepare: Be ready to explain how you would address bias in training data, ensure fairness in the model’s outputs, and manage user expectations when deploying the model.
During the interview, frame your approach to challenges by focusing on collaboration. Explain how you would work with data scientists to audit and improve datasets, as well as how you’d communicate potential risks to stakeholders. The ability to foresee and mitigate issues like bias, data drift, or overfitting will set you apart.
Master Technical Topics at a High Level
While you won’t be expected to code, interviewers will test your understanding of key AI and machine learning concepts. You'll need to explain these concepts in simple terms and discuss their impact on the product roadmap.
Example Concepts: Neural networks, reinforcement learning, decision trees, and unsupervised learning.
How to Prepare: Be comfortable discussing how models like these are built and trained, their trade-offs (e.g., accuracy vs. interpretability), and the impact of these trade-offs on the user experience.
It’s not just about regurgitating definitions. You’ll be judged on how well you can connect technical concepts to product decisions. When asked a technical question, always tie it back to business value. Why does this model matter for the user or the company? Your ability to navigate both technical depth and product strategy is what will distinguish you.
Practice AI Product Case Studies
AI PM interviews often include case studies where you’re asked to design an AI product or solve a specific AI-related problem. This could involve building a recommendation engine for a new app, optimizing an existing machine learning feature, or addressing model performance issues.
How to Prepare: Approach AI case studies the same way you would any product case study, but layer in AI-specific considerations. Start by framing the problem: What is the business goal? What are the user needs? Then, think about which type of AI solution could best meet those needs. Consider factors like data availability, model complexity, interpretability, and ethical implications.
Example: You might be asked, “How would you build a recommendation system for a music streaming platform?” In your answer, walk through your product vision, the types of user data you would collect, how you would ensure data privacy, and how you would measure success (e.g., precision, recall, or user engagement).
Don’t just think about how to build the AI feature. Think about how to maintain it for the next three years!
AI products require ongoing iteration, monitoring, and retraining. Show the interviewer that you understand how to ensure the model continues to improve over time, and how you would handle situations where the model’s performance degrades.
Highlight Ethical and Regulatory Considerations
AI is often subject to greater scrutiny than traditional software due to its potential for bias, ethical issues, and regulatory challenges. Be prepared to discuss how you would navigate these concerns in your product decisions.
Example: If you’re working on an AI-driven healthcare product, how would you ensure that the model’s predictions are both accurate and equitable? What steps would you take to comply with healthcare regulations like HIPAA?
How to Prepare: Familiarize yourself with ethical AI guidelines, data privacy laws, and industry-specific regulations (e.g., GDPR in Europe, or CCPA in California). Be ready to discuss how these frameworks influence your product roadmap and decision-making.
AI ethics is not an afterthought; it’s a core part of your product strategy. In your interview, demonstrate that you take ethical concerns seriously. Discuss how you would build fairness checks into the model development process, how you’d explain model decisions to users, and how you’d mitigate risks.
Avoid Trend Chasing
In a field as dynamic as AI, it’s easy to get caught up in the latest breakthroughs, hyped technologies, or the opinions of high-profile thought leaders. But while staying informed about the AI landscape is important, there’s a fine line between being informed and blindly following trends.
Make sure you focus on building AI products with scalable, generalized approaches that are positioned to leverage the long-term trajectory of AI advancements.
General Methods Scale Better Over Time
The most effective AI advancements come from general methods that scale with computation and data, rather than short-term, domain-specific optimizations.
Prof. Rich Sutton’s ""Bitter Lesson"" reveals that AI methods reliant on human-designed heuristics or domain expertise tend to plateau, while generalized techniques like deep learning and reinforcement learning continue to improve as computational power and data availability grow.
The success of these methods is largely due to their alignment with Moore’s Law, which predicts the exponential growth of computing power. Systems that are built to take advantage of this scalability will ultimately outperform those that are overly fine-tuned to specific, short-term needs.
The Dangers of Information Overload
Reading too many AI papers, or constantly staying up to date with every new breakthrough, can lead to confusion and decision fatigue for AI product managers.
While academic research often explores cutting-edge ideas, many papers focus on highly specialized use cases or experimental methods that may not be practical or scalable for real-world product development.
This overwhelming influx of information can make it difficult to discern which approaches are genuinely useful for your product and which are simply speculative.
When PMs try to integrate too many experimental ideas into a product roadmap, it can lead to scope creep and misalignment with the core product strategy.
Instead of attempting to absorb every new AI paper, focus on foundational techniques that have proven to scale effectively!
Build for Scalability, Not Buzz
As an AI PM, prioritize sustainable, scalable solutions that can evolve alongside technological advancements.
It’s tempting to experiment with the latest techniques or buzzworthy AI innovations, but the key to long-term success is creating models that perform reliably across diverse datasets and scenarios.
Scalability should be at the heart of your product strategy, ensuring your AI system can grow and improve without needing to be constantly rebuilt.
Trend chasing can lead to short-lived success, but true innovation in AI comes from focusing on core principles, such as building robust data pipelines, creating reliable models, and ensuring scalability.
By grounding your approach in scalable techniques and only integrating new technologies when they’re ready to contribute meaningfully, you’ll create products that can thrive for years to come.
Closing Thoughts
Breaking into AI product management requires intentional learning, networking, and gaining relevant experience.
By building a solid understanding of AI fundamentals, working on AI-related projects, and connecting with professionals in the field, you’ll be well-positioned to make the transition into this exciting and rapidly growing area.
Stay patient, keep building your skill set, and take advantage of opportunities to work with AI in any capacity.
All of this hard work will all add up to a compelling narrative when you're ready to land your first AI PM role!
Thank you to Pauli Bielewicz, Mary Paschentis, Goutham Budati, Markus Seebauer, Juliet Chuang, and Kendra Ritterhern for making this guide possible.",Breaking into AI Product Management — Product Teacher,Clement Kao,2024-10-20
2025-09-13T20:39:47.974014Z,https://medium.com/@allanouko17/customer-churn-prediction-using-machine-learning-ddf4cd7c9fd4,https://medium.com/@allanouko17/customer-churn-prediction-using-machine-learning-ddf4cd7c9fd4,medium.com,Customer Churn Prediction Using Machine Learning,,,[],[],[],[],"[""Predicting customer churn helps businesses retain clients and maintain revenue."", ""Utilizing machine learning models can identify patterns leading to customer attrition."", ""Data preprocessing, feature selection, and model evaluation are crucial steps in building an effective churn prediction model.""]","Customer churn prediction overview
Customer churn prediction predicts the likelihood of customers canceling a company’s products or services. In most cases, businesses with repeat clients or clients under subscriptions strive to maintain the customer base. Therefore, it is important to keep track of the customers who cancel their subscription plan and those who continue with the service. This approach requires the organization to know and understand their client’s behavior and the attributes that lead to the risk of the client leaving. I will explain the steps necessary in creating and deploying a churn prediction machine-learning model.
Why Predict Customer Churn?
It is important for any organization dealing with repeat clients to find ways to retain existing ones. The approach is crucial since customer churn is expensive, and acquiring new clients is more expensive than retaining existing ones. Consider an internet service (ISP) provider who has acquired a new user. They will need technicians and hardware to connect the latest client to their service. The client will only be required to pay the subscription fee to continue using their plan. If the user fails to renew their service, the company will most likely be at a loss, especially if the trend continues for several customers. The monthly recurring revenue (MRR) for such an institution will likely be low; hence, it will be unable to sustain the business. Thus, a reliable churn prediction model should help companies stay afloat as they scale up and attract more customers.
Case Study of Customer Churn Prediction Model
Creating churn prediction models involves using historical customer data to predict the likelihood of the current customer leaving or continuing with a particular service/product. The data used for the predictive models include product usage data and direct customer feedback. Besides, the predictive models identify the different trends and patterns in the data to forecast customer churn.
Scenario
Consider an e-commerce company with historical data on how its clients have interacted with its services. The company wants to know the likelihood of customers churning so it can launch targeted marketing campaigns.
Data source and GitHub repo
The data is available here, Kaggle.
The GitHub repo for the whole project
About the dataset
The data is in .xlsx format with the following features:
The dataset had no duplicate rows. Dropping the CustomerID column, which is each customer's unique identifier, has no effect on predicting churn.
EDA and Plots
Visualizing the categorical columns indicates:
1. About 83.2 % of the customers were retained, while 16.8% churned.
2. The company also has more male than female clients.
3. Most clients prefer logging in from their mobile phones to their phones and computers.
4. Most clients spend an average of 2 and 4 hours on the company’s app.
5. Most customers have about 3 or 4 devices registered for the retailer’s app.
6. Most customers prefer debit and credit cards to make payments.
7. City tier 2 has the lowest number of customers.
From the figure, notice that many customers placed their first and second orders, and the number reduced in subsequent orders. Additionally, the number of customers reduced a week after their last order.
Churn Distribution
Choosing the Right Machine Learning Model
You need to build a machine-learning model to predict customer churn. Therefore, you must choose the appropriate classification model since the target class (churn) consists of a discrete class of features (Yes and No). The classification model is suitable because it is a supervised model that uses historical data to find patterns in customer churn behavior. However, you should use regression models if the data has continuous values on the target class.
Data Preprocessing
To build a machine learning model with high accuracy, one needs to preprocess the data to reduce its complexity. Since the data had missing values, there is a need to impute these values appropriately.
from sklearn.impute import SimpleImputer # Imports SimpleImputer for handling missing data with basic strategies.
from sklearn.experimental import enable_iterative_imputer # Enables the experimental IterativeImputer in scikit-learn.
from sklearn.impute import IterativeImputer # Imports IterativeImputer for advanced imputation techniques using iterative models.
from sklearn.ensemble import RandomForestRegressor # Imports RandomForestRegressor for regression tasks using ensemble methods.
import pandas as pd # Imports the pandas library for data manipulation and analysis.
def fill_missing_values(df, random_state=None):
# Step 1: Identify numeric and categorical columns
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
categorical_columns = df.select_dtypes(include=['object']).columns.tolist() # Include both string and category data
# Step 2: Impute numeric columns
numeric_imputer = SimpleImputer(strategy='mean')
df[numeric_columns] = numeric_imputer.fit_transform(df[numeric_columns])
# Step 3: Handle categorical columns
for col in categorical_columns:
if df[col].dtype == 'object':
# Convert categorical column to one-hot encoded representation
encoded_cols = pd.get_dummies(df[col], prefix=col)
# Concatenate one-hot encoded columns
df = pd.concat([df.drop(col, axis=1), encoded_cols], axis=1)
# Step 4: Random Forest Iterative Imputer for the entire DataFrame
rf_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=random_state))
df = pd.DataFrame(rf_imputer.fit_transform(df), columns=df.columns)
return df
# Call the function to fill missing values
df = fill_missing_values(df, random_state=42)
The data had mixed data types, so the Random Forest Iterative Imputer was appropriate for filling in the missing values due to its high accuracy. The first imputation step involved identifying the dataset’s numeric and categorical columns to impute separately. After that, the missing values in the numerical columns were imputed using the mean. The categorical columns were converted to a one-hot encoded representation and then concatenated. The final step involved initiating the Random Forest Iterative Imputer for the entire data frame.
Splitting Data to Training and Testing Dataset
# Split model into training and test set
X = df.drop(columns=[""churn""])
y = df[""churn""]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
This step ensures there are different datasets for training and testing the machine learning models.
Oversampling using SMOTE
Since the target variable ‘Churn’ was highly imbalanced, balancing this target feature on the training dataset is important. The Synthetic Minority Over-sampling Technique (SMOTE) is the appropriate method to generate synthetic samples of the minority class to balance the target variable and improve model performance during model training.
from imblearn.over_sampling import SMOTE
print('Before upsampling count of label 0 {}'.format(sum(y_train==0)))
print('Before upsampling count of label 1 {}'.format(sum(y_train==1)))
# Minority Over Sampling Technique
sm = SMOTE(sampling_strategy = 1, random_state=1)
X_train_s, y_train_s = sm.fit_resample(X_train, y_train.ravel())
print('After upsampling count of label 0 {}'.format(sum(y_train_s==0)))
print('After upsampling count of label 1 {}'.format(sum(y_train_s==1)))
Models Evaluation
The different classification models included the following, ranked from the highest to the least performing.
The information shows that the XGBoost classifier was the best performer with high test accuracy and f1 score. Therefore, the final customer churn prediction model would be built based on the XGBoost classifier.
Below is a visualization of the feature weights, which indicate the importance of each feature in predicting customer churn.
Therefore, the features that will be used for the final deployment model are:
- Tenure
2. Cashback amount
3. City tier
4. Warehouse to home
5. Order amount hike from last year
6. Days since last order
7. Satisfaction score
8. Number of addresses
9. Number of devices registered
10. Complain
11. Order count
12. hourspendonapp
13. Marital status
14. Coupon used
15. Gender
Drop the unwanted columns from the training and testing dataset in this scenario.
Since the final deployment model is the XGBoost classifier, convert the Pandas data frame to a NumPy array. The conversion helps load and predict values in the Flask app without encountering an error.
# Convert the Pandas datafram to NumPy array for the XGBoost classifier.
# The conversion helps in loading and predicting values in the flask app.
X_test = X_test.values
X_train = X_train.values
# Run the model
final_model=XGBClassifier()
final_model.fit(X_train, y_train)
train_pred = final_model.predict(X_train)
test_pred = final_model.predict(X_test)
final_model.score(X_test, y_test)
Saving the Model
Now save the model as a pickle file. Also, save the data columns as a JSON file.
import pickle
pickle.dump(final_model,open('end_to_end_deployment/models/churn_prediction_model.pkl','wb'))
#save the data columns
import json
columns = {'data_columns' : [col.lower() for col in X.columns]}
with open(""end_to_end_deployment/models/columns.json"",""w"") as f:
f.write(json.dumps(columns))
Application Building
Create the customer churn prediction app using Flask. The input and output interfaces are shown below.
Given the available customer data, the company can now use customer details to determine whether they would churn. This approach would help the organization determine how to engage with the customers and launch targeted campaigns for customer retention.
GitHub Link to the project.
I recommend exploring the ‘Building Churn Prediction Model’ blog by UserMotion to learn how to develop personalized churn prediction model applications that enhance decision-making and boost customer retention with high accuracy.",Customer Churn Prediction Using Machine Learning,Allan Ouko,2024-05-21
2025-10-04T18:59:13.294033Z,https://docs.python.org/3/tutorial/introduction.html,https://docs.python.org/3/tutorial/introduction.html,docs.python.org,An Informal Introduction to Python,Tech,Development,"[""Python""]","[""Python Interpreter"", ""Data Types"", ""Variables"", ""Operators"", ""Control Flow"", ""Functions"", ""Modules"", ""Input and Output"", ""Error Handling"", ""Classes and Objects"", ""Inheritance"", ""Iterators"", ""Generators"", ""Decorators"", ""Context Managers"", ""Regular Expressions""]",[],[],"[""Introduces Python as a powerful, easy-to-learn programming language with efficient high-level data structures and an effective object-oriented programming system."", ""Highlights Python's elegant syntax, dynamic typing, and interpreted nature, making it ideal for scripting and rapid application development across various platforms."", ""Provides a tutorial that introduces basic concepts and features of Python, expecting readers to have a basic understanding of programming."", ""Mentions that the tutorial is not exhaustive but aims to introduce many of Python's noteworthy features, giving a clear idea of the language's style."", ""Suggests that after reading, readers will be able to read and write Python modules and programs, and be ready to learn more about various Python library modules."", ""Notes that the tutorial is available in multiple languages, including Spanish, Korean, French, and Japanese.""]","3. An Informal Introduction to PythonÂ¶
In the following examples, input and output are distinguished by the presence or absence of prompts (>>> and â¦): to repeat the example, you must type everything after the prompt, when the prompt appears; lines that do not begin with a prompt are output from the interpreter. Note that a secondary prompt on a line by itself in an example means you must type a blank line; this is used to end a multi-line command.
You can use the âCopyâ button (it appears in the upper-right corner when hovering over or tapping a code example), which strips prompts and omits output, to copy and paste the input lines into your interpreter.
Many of the examples in this manual, even those entered at the interactive
prompt, include comments. Comments in Python start with the hash character,
#
, and extend to the end of the physical line. A comment may appear at the
start of a line or following whitespace or code, but not within a string
literal. A hash character within a string literal is just a hash character.
Since comments are to clarify code and are not interpreted by Python, they may
be omitted when typing in examples.
Some examples:
# this is the first comment
spam = 1 # and this is the second comment
# ... and now a third!
text = ""# This is not a comment because it's inside quotes.""
3.1. Using Python as a CalculatorÂ¶
Letâs try some simple Python commands. Start the interpreter and wait for the
primary prompt, >>>
. (It shouldnât take long.)
3.1.1. NumbersÂ¶
The interpreter acts as a simple calculator: you can type an expression at it
and it will write the value. Expression syntax is straightforward: the
operators +
, -
, *
and /
can be used to perform
arithmetic; parentheses (()
) can be used for grouping.
For example:
>>> 2 + 2
4
>>> 50 - 5*6
20
>>> (50 - 5*6) / 4
5.0
>>> 8 / 5 # division always returns a floating-point number
1.6
The integer numbers (e.g. 2
, 4
, 20
) have type int
,
the ones with a fractional part (e.g. 5.0
, 1.6
) have type
float
. We will see more about numeric types later in the tutorial.
Division (/
) always returns a float. To do floor division and
get an integer result you can use the //
operator; to calculate
the remainder you can use %
:
>>> 17 / 3 # classic division returns a float
5.666666666666667
>>>
>>> 17 // 3 # floor division discards the fractional part
5
>>> 17 % 3 # the % operator returns the remainder of the division
2
>>> 5 * 3 + 2 # floored quotient * divisor + remainder
17
With Python, it is possible to use the **
operator to calculate powers [1]:
>>> 5 ** 2 # 5 squared
25
>>> 2 ** 7 # 2 to the power of 7
128
The equal sign (=
) is used to assign a value to a variable. Afterwards, no
result is displayed before the next interactive prompt:
>>> width = 20
>>> height = 5 * 9
>>> width * height
900
If a variable is not âdefinedâ (assigned a value), trying to use it will give you an error:
>>> n # try to access an undefined variable
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
NameError: name 'n' is not defined
There is full support for floating point; operators with mixed type operands convert the integer operand to floating point:
>>> 4 * 3.75 - 1
14.0
In interactive mode, the last printed expression is assigned to the variable
_
. This means that when you are using Python as a desk calculator, it is
somewhat easier to continue calculations, for example:
>>> tax = 12.5 / 100
>>> price = 100.50
>>> price * tax
12.5625
>>> price + _
113.0625
>>> round(_, 2)
113.06
This variable should be treated as read-only by the user. Donât explicitly assign a value to it â you would create an independent local variable with the same name masking the built-in variable with its magic behavior.
In addition to int
and float
, Python supports other types of
numbers, such as Decimal
and Fraction
.
Python also has built-in support for complex numbers,
and uses the j
or J
suffix to indicate the imaginary part
(e.g. 3+5j
).
3.1.2. TextÂ¶
Python can manipulate text (represented by type str
, so-called
âstringsâ) as well as numbers. This includes characters â!
â, words
ârabbit
â, names âParis
â, sentences âGot your back.
â, etc.
âYay! :)
â. They can be enclosed in single quotes ('...'
) or double
quotes (""...""
) with the same result [2].
>>> 'spam eggs' # single quotes
'spam eggs'
>>> ""Paris rabbit got your back :)! Yay!"" # double quotes
'Paris rabbit got your back :)! Yay!'
>>> '1975' # digits and numerals enclosed in quotes are also strings
'1975'
To quote a quote, we need to âescapeâ it, by preceding it with \
.
Alternatively, we can use the other type of quotation marks:
>>> 'doesn\'t' # use \' to escape the single quote...
""doesn't""
>>> ""doesn't"" # ...or use double quotes instead
""doesn't""
>>> '""Yes,"" they said.'
'""Yes,"" they said.'
>>> ""\""Yes,\"" they said.""
'""Yes,"" they said.'
>>> '""Isn\'t,"" they said.'
'""Isn\'t,"" they said.'
In the Python shell, the string definition and output string can look
different. The print()
function produces a more readable output, by
omitting the enclosing quotes and by printing escaped and special characters:
>>> s = 'First line.\nSecond line.' # \n means newline
>>> s # without print(), special characters are included in the string
'First line.\nSecond line.'
>>> print(s) # with print(), special characters are interpreted, so \n produces new line
First line.
Second line.
If you donât want characters prefaced by \
to be interpreted as
special characters, you can use raw strings by adding an r
before
the first quote:
>>> print('C:\some\name') # here \n means newline!
C:\some
ame
>>> print(r'C:\some\name') # note the r before the quote
C:\some\name
There is one subtle aspect to raw strings: a raw string may not end in
an odd number of \
characters; see
the FAQ entry for more information
and workarounds.
String literals can span multiple lines. One way is using triple-quotes:
""""""...""""""
or '''...'''
. End-of-line characters are automatically
included in the string, but itâs possible to prevent this by adding a \
at
the end of the line. In the following example, the initial newline is not
included:
>>> print(""""""\
... Usage: thingy [OPTIONS]
... -h Display this usage message
... -H hostname Hostname to connect to
... """""")
Usage: thingy [OPTIONS]
-h Display this usage message
-H hostname Hostname to connect to
>>>
Strings can be concatenated (glued together) with the +
operator, and
repeated with *
:
>>> # 3 times 'un', followed by 'ium'
>>> 3 * 'un' + 'ium'
'unununium'
Two or more string literals (i.e. the ones enclosed between quotes) next to each other are automatically concatenated.
>>> 'Py' 'thon'
'Python'
This feature is particularly useful when you want to break long strings:
>>> text = ('Put several strings within parentheses '
... 'to have them joined together.')
>>> text
'Put several strings within parentheses to have them joined together.'
This only works with two literals though, not with variables or expressions:
>>> prefix = 'Py'
>>> prefix 'thon' # can't concatenate a variable and a string literal
File ""<stdin>"", line 1
prefix 'thon'
^^^^^^
SyntaxError: invalid syntax
>>> ('un' * 3) 'ium'
File ""<stdin>"", line 1
('un' * 3) 'ium'
^^^^^
SyntaxError: invalid syntax
If you want to concatenate variables or a variable and a literal, use +
:
>>> prefix + 'thon'
'Python'
Strings can be indexed (subscripted), with the first character having index 0. There is no separate character type; a character is simply a string of size one:
>>> word = 'Python'
>>> word[0] # character in position 0
'P'
>>> word[5] # character in position 5
'n'
Indices may also be negative numbers, to start counting from the right:
>>> word[-1] # last character
'n'
>>> word[-2] # second-last character
'o'
>>> word[-6]
'P'
Note that since -0 is the same as 0, negative indices start from -1.
In addition to indexing, slicing is also supported. While indexing is used to obtain individual characters, slicing allows you to obtain a substring:
>>> word[0:2] # characters from position 0 (included) to 2 (excluded)
'Py'
>>> word[2:5] # characters from position 2 (included) to 5 (excluded)
'tho'
Slice indices have useful defaults; an omitted first index defaults to zero, an omitted second index defaults to the size of the string being sliced.
>>> word[:2] # character from the beginning to position 2 (excluded)
'Py'
>>> word[4:] # characters from position 4 (included) to the end
'on'
>>> word[-2:] # characters from the second-last (included) to the end
'on'
Note how the start is always included, and the end always excluded. This
makes sure that s[:i] + s[i:]
is always equal to s
:
>>> word[:2] + word[2:]
'Python'
>>> word[:4] + word[4:]
'Python'
One way to remember how slices work is to think of the indices as pointing between characters, with the left edge of the first character numbered 0. Then the right edge of the last character of a string of n characters has index n, for example:
+---+---+---+---+---+---+
| P | y | t | h | o | n |
+---+---+---+---+---+---+
0 1 2 3 4 5 6
-6 -5 -4 -3 -2 -1
The first row of numbers gives the position of the indices 0â¦6 in the string; the second row gives the corresponding negative indices. The slice from i to j consists of all characters between the edges labeled i and j, respectively.
For non-negative indices, the length of a slice is the difference of the
indices, if both are within bounds. For example, the length of word[1:3]
is
2.
Attempting to use an index that is too large will result in an error:
>>> word[42] # the word only has 6 characters
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
IndexError: string index out of range
However, out of range slice indexes are handled gracefully when used for slicing:
>>> word[4:42]
'on'
>>> word[42:]
''
Python strings cannot be changed â they are immutable. Therefore, assigning to an indexed position in the string results in an error:
>>> word[0] = 'J'
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
TypeError: 'str' object does not support item assignment
>>> word[2:] = 'py'
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
TypeError: 'str' object does not support item assignment
If you need a different string, you should create a new one:
>>> 'J' + word[1:]
'Jython'
>>> word[:2] + 'py'
'Pypy'
The built-in function len()
returns the length of a string:
>>> s = 'supercalifragilisticexpialidocious'
>>> len(s)
34
See also
- Text Sequence Type â str
Strings are examples of sequence types, and support the common operations supported by such types.
- String Methods
Strings support a large number of methods for basic transformations and searching.
- f-strings
String literals that have embedded expressions.
- Format String Syntax
Information about string formatting with
str.format()
.- printf-style String Formatting
The old formatting operations invoked when strings are the left operand of the
%
operator are described in more detail here.
3.1.3. ListsÂ¶
Python knows a number of compound data types, used to group together other values. The most versatile is the list, which can be written as a list of comma-separated values (items) between square brackets. Lists might contain items of different types, but usually the items all have the same type.
>>> squares = [1, 4, 9, 16, 25]
>>> squares
[1, 4, 9, 16, 25]
Like strings (and all other built-in sequence types), lists can be indexed and sliced:
>>> squares[0] # indexing returns the item
1
>>> squares[-1]
25
>>> squares[-3:] # slicing returns a new list
[9, 16, 25]
Lists also support operations like concatenation:
>>> squares + [36, 49, 64, 81, 100]
[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]
Unlike strings, which are immutable, lists are a mutable type, i.e. it is possible to change their content:
>>> cubes = [1, 8, 27, 65, 125] # something's wrong here
>>> 4 ** 3 # the cube of 4 is 64, not 65!
64
>>> cubes[3] = 64 # replace the wrong value
>>> cubes
[1, 8, 27, 64, 125]
You can also add new items at the end of the list, by using
the list.append()
method (we will see more about methods later):
>>> cubes.append(216) # add the cube of 6
>>> cubes.append(7 ** 3) # and the cube of 7
>>> cubes
[1, 8, 27, 64, 125, 216, 343]
Simple assignment in Python never copies data. When you assign a list to a variable, the variable refers to the existing list. Any changes you make to the list through one variable will be seen through all other variables that refer to it.:
>>> rgb = [""Red"", ""Green"", ""Blue""]
>>> rgba = rgb
>>> id(rgb) == id(rgba) # they reference the same object
True
>>> rgba.append(""Alph"")
>>> rgb
[""Red"", ""Green"", ""Blue"", ""Alph""]
All slice operations return a new list containing the requested elements. This means that the following slice returns a shallow copy of the list:
>>> correct_rgba = rgba[:]
>>> correct_rgba[-1] = ""Alpha""
>>> correct_rgba
[""Red"", ""Green"", ""Blue"", ""Alpha""]
>>> rgba
[""Red"", ""Green"", ""Blue"", ""Alph""]
Assignment to slices is also possible, and this can even change the size of the list or clear it entirely:
>>> letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g']
>>> letters
['a', 'b', 'c', 'd', 'e', 'f', 'g']
>>> # replace some values
>>> letters[2:5] = ['C', 'D', 'E']
>>> letters
['a', 'b', 'C', 'D', 'E', 'f', 'g']
>>> # now remove them
>>> letters[2:5] = []
>>> letters
['a', 'b', 'f', 'g']
>>> # clear the list by replacing all the elements with an empty list
>>> letters[:] = []
>>> letters
[]
The built-in function len()
also applies to lists:
>>> letters = ['a', 'b', 'c', 'd']
>>> len(letters)
4
It is possible to nest lists (create lists containing other lists), for example:
>>> a = ['a', 'b', 'c']
>>> n = [1, 2, 3]
>>> x = [a, n]
>>> x
[['a', 'b', 'c'], [1, 2, 3]]
>>> x[0]
['a', 'b', 'c']
>>> x[0][1]
'b'
3.2. First Steps Towards ProgrammingÂ¶
Of course, we can use Python for more complicated tasks than adding two and two together. For instance, we can write an initial sub-sequence of the Fibonacci series as follows:
>>> # Fibonacci series:
>>> # the sum of two elements defines the next
>>> a, b = 0, 1
>>> while a < 10:
... print(a)
... a, b = b, a+b
...
0
1
1
2
3
5
8
This example introduces several new features.
The first line contains a multiple assignment: the variables
a
andb
simultaneously get the new values 0 and 1. On the last line this is used again, demonstrating that the expressions on the right-hand side are all evaluated first before any of the assignments take place. The right-hand side expressions are evaluated from the left to the right.The
while
loop executes as long as the condition (here:a < 10
) remains true. In Python, like in C, any non-zero integer value is true; zero is false. The condition may also be a string or list value, in fact any sequence; anything with a non-zero length is true, empty sequences are false. The test used in the example is a simple comparison. The standard comparison operators are written the same as in C:<
(less than),>
(greater than),==
(equal to),<=
(less than or equal to),>=
(greater than or equal to) and!=
(not equal to).The body of the loop is indented: indentation is Pythonâs way of grouping statements. At the interactive prompt, you have to type a tab or space(s) for each indented line. In practice you will prepare more complicated input for Python with a text editor; all decent text editors have an auto-indent facility. When a compound statement is entered interactively, it must be followed by a blank line to indicate completion (since the parser cannot guess when you have typed the last line). Note that each line within a basic block must be indented by the same amount.
The
print()
function writes the value of the argument(s) it is given. It differs from just writing the expression you want to write (as we did earlier in the calculator examples) in the way it handles multiple arguments, floating-point quantities, and strings. Strings are printed without quotes, and a space is inserted between items, so you can format things nicely, like this:>>> i = 256*256 >>> print('The value of i is', i) The value of i is 65536
The keyword argument end can be used to avoid the newline after the output, or end the output with a different string:
>>> a, b = 0, 1 >>> while a < 1000: ... print(a, end=',') ... a, b = b, a+b ... 0,1,1,2,3,5,8,13,21,34,55,89,144,233,377,610,987,
Footnotes",3. An Informal Introduction to Python,Python Software Foundation,2025-04-15
2025-10-04T19:04:00.627080Z,https://www.evidentlyai.com/llm-guide/llm-as-a-judge,https://www.evidentlyai.com/llm-guide/llm-as-a-judge,www.evidentlyai.com,LLM-as-a-judge: a complete guide to using LLMs for evaluations,Tech,GenAI,"[""LLM Evaluation""]","[""LLM-as-a-judge"", ""Evaluation Methods"", ""Evaluation Prompts""]","[""Offline Evaluations"", ""Online Evaluations""]","[""Pairwise Comparisons"", ""Direct Scoring""]","[""LLM-as-a-judge is a method to assess the quality of text outputs from LLM-powered products."", ""It employs an LLM with a tailored evaluation prompt to rate generated text based on defined criteria."", ""This technique is adaptable for both offline and online evaluations, offering flexibility and cost-effectiveness."", ""The success of LLM judges depends on the prompt, model, and task complexity."", ""LLM judges can handle pairwise comparisons and direct scoring, approximating human judgment."", ""Evidently AI provides tools to create and tune LLM judges for various applications.""]","contentsâ
LLM-as-a-judge is a common technique to evaluate LLM-powered products.Â
It grew popular for a reason: itâs a practical alternative to costly human evaluation when assessing open-ended text outputs.
Judging generated texts is tricky â whether it's a âsimpleâ summary or a chatbot conversation. Metrics like accuracy donât work well because there are many ways to be ârightâ without exactly matching the example answer. And things like style or tone are subjective and hard to pin down.
Humans can handle these nuances, but manually reviewing every response doesnât scale. LLM-as-a-judge emerged as an alternative: you can use LLMs to evaluate the generated texts. Interestingly, the LLM is both the source of the problem and the solution!
In this guide, weâll cover:
While thereâs plenty of great research on this topicâweâll reference some of itâweâre going to keep things practical. This guide is for anyone working on an LLM-powered product and wondering if this technique could work for them.
ð¡ Want to build your own LLM judge?
We built Evidently, an open-source library for LLM evals with 25M+ downloads. You can use it to create and tune your own LLM judges. Give it a âï¸ on GitHub to support the project!
ð» Want a practical example? Watch this video tutorial with code to get started.
Test fast, ship faster. Evidently Cloud gives you reliable, repeatable evaluations for complex systems like RAG and agents â so you can iterate quickly and ship with confidence.Â
TL;DR. LLM-as-a-Judge uses LLMs to evaluate AI-generated texts based on custom criteria defined in an evaluation prompt.
As you build your LLM-powered product â whether it's a chatbot, code generator, or email assistant â you need to evaluate its quality.
LLM-as-a-judge is an evaluation approach that supports all these workflows. The idea is simple: ask an LLM to ""judge"" the text outputs using guidelines you define.Â
Say, you have a chatbot. You can ask an external LLM to evaluate its responses, similar to how a human evaluator would, looking at things like:
To apply the method, you take the text output from your AI system and feed it back into the LLM, this time alongside an evaluation prompt. The LLM will then return a score, label, or even a descriptive judgment â following your instructions.
The beauty of this approach is that it lets you evaluate text outputs automatically and look at custom properties specific to your use case.Â
For example, you can instruct the LLM to judge the helpfulness of the chatbotâs responses. (Side note: all prompts in this guide are just for illustration).
Simplified prompt: Given a QUESTION and RESPONSE, evaluate if the response is helpful. Helpful responses are clear, relevant, and actionable. Unhelpful responses are vague, off-topic, or lacking detail. Return a label: 'helpful' or 'unhelpful'.
If you run it over your chatbot outputs, the LLM judge will score each response.Â
You can then summarize the results over the entire conversation dataset to see the distribution: what is the share of helpful responses?
It's not just about one-off analysis: you can run these evaluations continuously on live data. This will let you track how well the chatbot works and detect issues, like a sudden surge in âunhelpfulâ responses.
Want to assess another property? Just write a new prompt!
Itâs worth noting that LLM-as-a-judge is not an evaluation metric in the same sense as, say, accuracy, precision, or NDCG. In machine learning, a metric is a well-defined, objective measure: they precisely quantify how well a modelâs predictions match the ground truth.
In contrast, LLM-as-a-judge is a general technique where you use LLM to approximate human labeling. When you ask an LLM to assess qualities like ""faithfulness to source,"" ""correctness,"" or ""helpfulness,"" you define what these terms mean in the evaluation prompt and rely on the semantic relationships the LLM learned from training data.Â
The LLM follows your instructions, just like a human would. You can then track how many responses are labeled as âgood,â but this isnât a fixed, deterministic measure. It's a use-case-specific proxy metric.
The success of using LLM judges also heavily depends on the implementation detailsâthe model you use, the prompt design, and the task complexity. You will also need to adapt the evaluation prompt to the specific evaluator LLM: both words and formats matter.Â
Before we dive into details, letâs address the elephant in the room: how come you use LLMs to evaluate âits ownâ work? Isnât this cheating?
TL;DR. Assessing text properties is easier than generating it. The LLMÂ evaluator is external to the main product and performs a simpler, more focused task.
At first, it may seem odd to use an LLM to evaluate the text outputs. If the LLM is generating the answers, why would it be better at judging them or noticing errors?
The key is that we're not asking the LLM to redo its work. Instead, we use a different prompt â or even a different LLM â to perform a separate task: evaluate specific text properties.
By making an external call to an LLM with a focused evaluation prompt, you activate different capabilities of the model. Often, using it as a simple text classifier and asking to follow a single instruction.
Think of it this way: it's easier to critique than to create. Classifying content is simpler than generating it. Detecting that something went wrong is usually easier than preventing the mistake in the first place.
When generating responses, an LLM handles many variables, integrates complex context and user inputs, and follows detailed product prompts that may contain multiple instructions at once. This complexity can lead to errors.
Evaluating responses is generally more straightforward. We're not asking the LLM to fix or correct itself; but simply to assess what has already been produced. For example, a relevance evaluator only needs to check whether the response is semantically relevant to the question, not generate a better answer.
Similarly, while your chatbot might be tricked by a malicious user into generating, say, biased content, an external LLM evaluator can still detect this. This evaluator works independently of the conversation: it examines the output and judges it on its merits. For example, ""Does this text contain bias toward any group? Yes or no."" Since LLMs are trained on vast amounts of text data, they are quite good at detecting language patterns.
This doesn't mean that LLM evaluators are ""better"" than your original model; they are simply being asked to do a simpler, more focused task.
Let's look at a few different types of judges you can implement.
TL;DR. You can ask an LLM to choose the best answer between two, assess specific qualities like conciseness or politeness, or evaluate an answer with extra context, like to detect hallucinations or determine relevance.
You can use LLM evaluators in several scenarios:
Pairwise comparison is typically done offline: you need an option to generate and contrast multiple responses. Direct scoring works both offline and online for continuous monitoring.
One of the early uses of the LLM-as-a-judge technique is in pairwise comparison, where the LLM looks at the two responses and decides which one is better. This method was described, for example, in this blog post, and got its official âjudgeâ nickname in the paper âJudging LLM-as-a-Judge with MT-Bench and Chatbot Arenaâ by Zheng et al., 2023 (MT-Bench and Chatbot arena are examples of LLM benchmarks).
In short, you generate two responses and ask the LLM to select the more appropriate one based on specific qualities or factors.
In the cited paper, the authors used GPT-4 as an evaluation model and compared its decisions to crowdsourced human preferences. The LLM evaluations achieved over 80% agreement, which is comparable to the level of agreement between human evaluators.
This is a great result, and the method has made its way into practitioners' playbooks. It is especially useful during the development stage when you want to pick the better model or prompt by running multiple pairwise comparisons.Â
Simplified prompt: You will be shown two responses to the same question. Your task is to decide which response is better based on its relevance, helpfulness, and level of detail. If both responses are equally good, declare a tie.
The ability of LLMs to match human preferences when comparing two outputs is very promising. However, the more common use of LLM judges by LLM product builders is even simpler: direct scoring of individual responses.Â Â
You can ask the LLM to score the generated texts directly by any dimension you define. Instead of focusing on general âpreferenceâ, it can handle one criterion at a time. This is particularly useful for production monitoring.
For example, you might use LLM judges to assess things like tone, clarity, adherence to format, conciseness, politeness, presence of Personally Identifiable Information (PII), etc.Â
These are generic examples. For your specific app, youâd likely develop custom evaluators that reflect what defines a quality output or what types of errors you expect to catch.Â Â
LLMs are highly effective at classification tasks that focus on language and semantics. A common way is to treat the evaluation as a binary classification problem. Alternatively, you can use a grading scale to measure how well the response meets specific criteria, such as using a Likert scale from one to five.Â Â
Simplified prompt: Evaluate the following text for conciseness. A concise response delivers the key message clearly and directly, without unnecessary words. Return one of the following labels: 'Concise,â or 'Verboseâ.
The use of LLMs for grading is examined, for example, in the following papers:
You can also evaluate entire conversations.Â In this case, youâd pass the complete multi-turn transcript: all questions a user asked inside a given session and how LLM responded.
As long as this conversation transcript fits within the LLM context window (the amount of text it can process at once, typically a few thousand tokens), the model can handle this. Itâs still a grading task, just with a longer input text.
Examples of conversation-level evaluations are:
You can use it to monitor trends and flag individual conversations for human review.
Simplified prompt: Read the conversation and assess if the user's request was resolved. 'Resolved' means the issue was addressed and the user confirmed or showed satisfaction. Return one of these labels: 'Resolved' or 'Not Resolved.'
In reference-based evaluations, you donât just evaluate the generated response aloneâyou provide additional context to review. Here are a few examples of extra inputs:Â
You still directly score the response, but instead of passing a single input, you include two or more pieces of text in the evaluation prompt and explain how they relate to each other. Letâs take a look at a few examples!
1. Evaluating correctness based on reference answer.
This is an offline evaluation where the LLM compares the response to the ""golden"" reference. Itâs great for evaluations during the experimental phase (when you iterate on your system design) and for regression testing after updates to the model or prompt.
For example, in a Q&A system, the LLM judge can check if the new response is similar to the previously approved answer to this same question.Â
This correctness judge is an alternative to deterministic metrics like ROUGE that quantify word and phrase overlap between two answers, and semantic similarity checks, which perform the comparison using a pre-trained embedding model.
Simplified prompt: Compare the generated RESPONSE to the REFERENCE answer. Evaluate if the generated response correctly conveys the same meaning, even if the wording is different. Return one of these labels: 'Correctâ or 'Incorrect.'
For example, in Segment's blog post, they described using an LLM evaluator to compare the LLM-generated queries against a pre-approved query for a given input. Similarly, Wix uses LLM judges to match the correctness of Q&AÂ responses against ground truth.
2. Evaluating answer quality considering the question.
Unlike the previous example that requires a golden reference answer, you can also run evaluations using data available at the time of response generation. For instance, in Q&A systems you can judge the answer together with the question to evaluate:
You can run these types of evaluations for ongoing quality monitoring.
Simplified prompt: Evaluate the following RESPONSE based on the given QUESTION. A complete response is one that fully addresses all parts of the question. Return one of the following labels: 'Complete' or 'Incomplete.'
3. Scoring context relevance in RAG.Â
Retrieval-Augmented Generation (RAG) is a special case of LLM product architecture.
With this implementation, the system first searches for documents that can help answer the question and then uses them to generate a response. This adds up-to-date knowledge to the LLM answer. To properly evaluate RAGâs performance, you need to assess both sides:
If you're using RAG, check out our in-depth guide on RAG evaluation.
For the first part â evaluation of search quality â you can use ranking quality metrics like NDCG or precision at K. These types of metrics quantify how well the system can find and sort the documents that help answer the query. These evaluations typically happen offline as you iterate on parameters like different search strategies.
However, ranking metrics require relevance labels, meaning each document must be marked as helpful or not for answering the question. You can do this manually, but another option is to task an LLM. In this case, it will act as a context relevance judge.
After the LLM scores each retrieved text chunk for its relevance to the query, you feed these labels onto the next step of the evaluation process to compute the ranking quality metrics.
Simplified prompt: Evaluate the relevance of the text in answering the Â QUESTION. A relevant text contains information that helps answer the question, even if partially. Return one of the following labels: 'Relevant', or 'Irrelevant.'
For example, this method is described in the paper âLarge Language Models Can Accurately Predict Searcher Preferencesâ (Thomas et al., 2023).
4. Evaluating hallucinations in RAG.
On the other side of the equation, and especially once the RAG system is in production, you want to check the final response quality. One issue that happens here is hallucination: LLMs can invent details not present in the source material.
Since you have both the answer and the retrieved context, you can run one more evaluation to see how grounded the answer is. Effectively, you create a faithfulness judge to double-check if the LLM processed the retrieved content correctly.
Simplified prompt: Evaluate the following RESPONSE for faithfulness to the CONTEXT. A faithful response should only include information present in the context, avoid inventing new details, and not contradict the context. Return one of the following labels: 'Faithful' or 'Not Faithfulâ.
For example, in DoorDashâs blog post about their RAG-based support agent, they mention using LLM judges to evaluate response coherence with context.
You can also use this approach to evaluate summaries by comparing them to the source. This helps you cross-check the quality and spot inconsistencies. For example, this paper explores methods to detect factual inaccuracies in summaries.
LLM evaluators are a powerful technique, but they require careful setup for your specific scenario. You'll need time to design and evaluate your judge, craft good prompts, and build a reliable evaluation pipeline for continuous monitoring.
Letâs explore some of these best practices.
TL;DR. Creating an LLM judge is a small ML project. Start with a labeled dataset that reflects how you want the LLM to judge your texts, then create and refine the evaluation prompt to ensure alignment with your labels.
Creating an LLM judge is much like developing any LLM-powered product â you need a prompt that tells the LLM exactly what to do. In this case, itâs an evaluation prompt that instructs the LLM to assess text inputs and return a label, score, or explanation.
Hereâs the rub: if you're using an LLM to evaluate other LLMs, and the results arenât deterministic, how do you ensure your judges align with your expectations?
Youâll need to take an iterative approach â refining the judge just like you refine your LLM product prompts. In other words, your evaluation system needs its own evaluation!
Here is the general process.
Step 1. Define the evaluation scenario.Â
First, decide what exactly you want the LLM judge to evaluate. Are you checking for correctness, tone, conciseness, or something else? You might evaluate just the generated output, compare multiple inputs, or look at the complete conversation transcript. Start with what you want to achieve, then think back to how to define the evaluator.
Tip: Keep it simple! Donât try to evaluate too many things at once. If you want to check different dimensions (like tone and accuracy), split them into separate evaluations. Use clear, binary choices whenever possible (e.g., ""correct"" vs. ""incorrect"").
Step 2. Prepare the evaluation dataset.Â
Next, create a small dataset to test your LLM judge. This can include examples from experiments or production data. If you donât have those, you can create synthetic cases that mimic the expected inputs.
Your dataset doesnât need to be huge, but it should include diverse examples, especially those that challenge your evaluation criteria.
Step 3. Label this dataset.Â
Hereâs the important part: you need to label this dataset manually, just as you want the LLM judge to do later. This labeled dataset will be your ""ground truth"" and help you measure how well the LLM judge performs.Â
Plus, labeling it yourself forces you to be intentional about what you expect the LLM to notice. This will shape how you write the evaluation instructions.
Let's say you are creating a correctness judge that will compare two LLM outputs â such as an old vs. new answer after a model update. Youâll need to decide which changes matter: are you checking if the responses are mostly the same, or do specific details count? Maybe some changes, like adding a greeting, are fine, but suggesting an extra action might be unacceptable. Often, youâd look for specific types of errors like contradictions, omissions, changes of order, or response style.
For example, in this use case we considered any extra information, such as suggesting the user contact an agent, as incorrect, even if it didnât contradict the reference answer directly.
This may be different for your use case!Â
You can, of course, skip this step: write the best prompt you can, then review and correct the LLM-generated labels to create the approved test dataset. However, starting with your labels often improves your initial prompt.
Step 4. Craft your evaluation prompt.
Once you know what youâre looking for, itâs time to create an evaluation prompt for the LLM.Â
For example, hereâs how we formulated the evaluation criteria after manually labeling the data for the correctness example. It accounts for specific errors weâve seen:
Weâll share more tips on prompt writing in the next section.Â
Want a code tutorial? Check out this guide for a step-by-step example of creating an evaluation dataset and designing an LLM judge.Â
Step 5. Evaluate and iterate.Â
Once your prompt is ready, apply it to your evaluation dataset and compare the LLM judgeâs outputs to your manually labeled ground truth.
For binary classifications, you can use metrics like precision and recall to measure how well the LLM judge is performing, especially when youâre focusing on a specific class. For example, you might prioritize the LLM's ability to detect âincorrectâ responses, even if some correct responses are misclassified: in this case, recall is a relevant quality metric.
If the results donât meet your expectations, you can adjust the prompt and rerun the evaluation. Ideally, you would also keep some part of your manually labeled dataset as a âheld-outâ dataset: and only use it to test your final prompt.Â Â Â
Note that your LLM judge doesnât need to be perfect â just âgood enoughâ for your goals. Even human evaluators make mistakes!
And finally, bring in the domain experts.
Non-technical team members â like product or subject matter experts â play a big role in setting the evaluations. They help figure out what behaviors or topics the LLM should catch, shape guidelines that go into the prompts, and test the alignment. With no-code tools, you can design this a collaborative workflow.
Creating LLM judges is also naturally an iterative process. Especially if you begin with a synthetic dataset, you might need to adjust your standards once you start grading real user data and see patterns in the LLM outputs and errors. You may then edit the judge prompt or split and add new evaluation criteria.
In the paper âWho Validates the Validators?â (Shankar et al., 2024), the authors talk about criteria drift, where manually grading outputs helps users refine their expectations based on the specific LLM outputs they see.
TL;DR. Write your own prompts. Use yes/no questions and break down complex criteria. Asking for reasoning helps improve evaluation quality and debugging.
Evaluation prompts are the core of your LLM judge. Just like with human assessors, you need to provide the LLM with precise, detailed instructions.Â Â
Ideally, you should write your own evaluation prompts. First, because this is where LLM judges really shine â you can customize them to your specific needs. Second, even minor clarifications can improve the quality of your scenario over a generic prompt.
Keep in mind that prompts behave differently across models. The exact evaluation instructions might give different results on a GPT-4.0 mini compared to GPT-4.0. Testing them against your expectations is key.
Tip: If you're using external evaluation tools with LLM-based metrics, always review their prompts and test against your labels to ensure they align with your needs.
Several prompting techniques can improve the accuracy and consistency of your LLM evaluator. These are similar to the ones you might use while developing your LLM product, such as a chain of thought prompting. Asking LLMs to grade text is no different from other tasks you might ask them to perform â the same tricks work.
Here are a few ones to consider.
1. Use binary or low-precision scoring.
Binary evaluations, like ""Polite"" vs. ""Impolite,"" tend to be more reliable and consistent for both LLMs and human evaluators. It's easier to get accurate results with two simple choices rather than trying to decide if a specific response scores 73 vs. 82 for ""politeness.""
While purpose-built probabilistic machine learning models can return such scores, LLMs generate text and aren't naturally calibrated for high-precision scoring.
You can also use a three-option approach, like ""relevant,"" ""irrelevant,"" and ""partially relevant,"" or include an ""unknown"" option when thereâs not enough information. This avoids forcing the LLM to make a decision without sufficient data.Â
2. Explain the meaning of each score.
Think of writing prompts like giving instructions to an intern who is doing the task for the first time. Donât just ask the LLM to label something as ""toxic"" or ""not toxic"". Instead, clearly define what ""toxic"" means â like language thatâs harmful or offensive in specific ways.
If you prefer to flag borderline cases rather than miss them, you can guide the LLM to be more strict with instructions like, ""If unsure, err on the side of caution and mark it as âtoxicâ"".Â
Explaining what each class means becomes even more critical with something like a 5-point scale â whatâs the difference between a 3 and a 4? If thatâs not clear, both LLMs and human reviewers will struggle to stay consistent.
If you canât easily explain these distinctions, it might be a sign you need to simplify the scale. Without guidance, LLMs may return inconsistent results, giving different scores for similar texts or leaning toward certain scores that were more common in its training data.Â
To reduce variability, you may consider using multiple evaluations. Then, you can combine the results using methods like max voting or averaging. For example, see âReplacing Judges with Juriesâ (Verga et al., 2024).Â
Want to check if your instructions are good? Try labeling some responses yourself! If you need extra clarification to make a decision, the LLM will likely do so too. If youâre adding too many details, consider breaking the task into several.
3. Simplify evaluation by splitting criteria.
If you have several aspects to evaluate, like completeness, accuracy, and relevance, it's best to split them into separate evaluators. This keeps things focused.
You can still combine the results for an overall judgment in a deterministic way. For instance, you could flag an answer if any one criterion gets a ""negative"" label (incomplete, inaccurate, or irrelevant). Alternatively, you can sum up the number of ""good"" labels for an overall score or assign weights to reflect the importance of each criterion.
This way, LLM handles one quality at a time rather than dealing with complex reasoning. This improves the accuracy of each evaluation and also makes it easier for reviewers to verify and understand the results.
4. Add examples to the prompt.Â
If your criteria is nuanced, you can also consider adding examples of inputs and judgments.
Your prompt can start with a general instruction (e.g., ""Good"" means... ""Bad"" means...) and then provide examples of both good and bad responses. These examples help the LLM better understand how to apply your criteria â especially helpful for less capable models.â
This technique is known as few-shot learning. You can read more about this approach in âLanguage Models are Few-Shot Learners.â Â (Brown et al., 2020).
However, it's important to test how adding examples impacts the judge's performance. Skewed or biased examples can influence the LLM's decisions.
For instance, if you include more negative examples than positive ones, or if all the negative examples are listed towards the end, their order or frequency may affect evaluation results. Though this tends to be less of an issue with more advanced models.Â
For more on this, check the paper âCalibrate Before Use: Improving Few-Shot Performance of Language Models.â Â (Zhao et al., 2021), where they show that few-shot learning results vary due to factors like prompt format and example order.Â
5. Encourage step-by-step reasoning.
Just like with other tasks, asking the LLM to ""think"" through its process before giving a final answer â known as the Chain of Thought (CoT) approach â can help achieve better results.
The idea of adding a reasoning step inside a prompt is explored by Wei et al. (2022) in their paper âChain-of-Thought Promptingâ Â and Kojima et al. (2022) in âLarge Language Models are Zero-Shot Reasonersâ .Â
You can do the same in your evaluation prompt: ask the model to explain its reasoning or think step by step, effectively implementing a Zero-Shot-CoT approach. This way, the model will provide both the reasoning and the result in one response. Further research shows that this significantly improves the quality of evaluations.
In addition, this also creates a reasoning trail that you can review later. This is especially helpful for troubleshooting when you go through the responses. For example, the generated justification can highlight which parts of the text led the model to flag something as incorrect or identify that it contains personally identifiable information.
Multi-turn chain of thought. Some researchers explored more sophisticated approaches to CoT. For example, one method, called G-Eval (see Liu et al., 2023), uses a process where the AI first defines the task, plans the steps, and then completes the evaluation form. However, further studies show that this auto-generated CoT doesnât always work better, and simply asking LLM to explain or analyze outperforms this method (see Chiang et al., 2023).
6. Set a low temperature.
In LLMs, temperature controls how random the output is. Higher temperatures mean more variety, while lower temperatures make outputs more âpredictableâ. For evaluations, you donât need creativity â set a low temperature so the model gives consistent answers for the same input.
7. Use a more capable model.
When evaluating, it makes sense to start with a stronger model. This generally helps ensure better alignment with human judgments. Once you have that solid baseline, you can experiment with smaller or less capable models to see if they meet your needs.
8. Get structured outputs.
Last but not least, always go for a structured output format, like JSON. It makes it much easier to parse the evaluation results for further analysis.Â
Recap.Â To sum up, here are a few tips for writing better evaluation prompts:
Starting with simpler approaches is often best. As long as you evaluate your LLM judges to confirm how well they work, you can begin with a straightforward method and stick with it if itâs effective. If you want to experiment with more complex techniques (like a multi-step chain of thought with extra LLM calls), you can build upon this initial baseline.
For a more in-depth literature review on LLM-as-a-judge research, check out excellent resources by Cameron Wolfe and Eugene Yan.
Want to get started with examples? We implemented many of these tips in the Evidently open-source library. It offers LLM evaluation templates that already include repeatable parts like formatting and reasoning. You can add your criteria in plain text or use built-in prompts for inspiration. Check out the docs to get started. To create judges with no-code, sign up for a free account on Evidently Cloud.
TL;DR. LLM observability helps monitor the performance of your system in production. Set up tracing to collect live data, schedule evaluations where portions of this data are scored by LLM judges, and use a dashboard to track trends such as user frustration and hallucinations.
Once your system goes live, real users will interact with it in ways you might not have expected. Even the most thorough pre-launch testing wonât cover all the different ways people will use it. Thatâs why itâs so important to track actual performance in real time.
In production, thereâs no perfect answer to which to compare outputs, so youâll need to monitor the quality of responses on their own. LLM evaluators make this possible. You can set up a regular process to score new generations based on selected criteria.Â
This monitoring isn't just about quality control â it can also provide insights into how users interact with your tool, such as what tasks or topics are most common.Â
Hereâs how you can set up the process:
1. Tracing.
The first step is tracing â collecting data from user interactions and storing it for analysis. Youâll need to instrument your AI application to capture all inputs, LLM and function calls and completions.
The immediate benefit is clear: once you have the logs, you can view and read them to understand whatâs happening as users interact with your application.
Initially, you might manually review responses to spot common patterns or issues. However, as the volume of data grows, manual reviews wonât scale, so youâll need some automation.
2. Schedule evaluations.Â
Once youâve got your tracing data stored and know what you want to evaluate, you can set up scheduled evaluations. These are regular checks where you run new answers or complete conversations through the LLM judge to see how the systemâs doing based on the attributes youâve set. If youâre handling a lot of data, you can sample a subset to track performance.Â
For example, if youâre running a customer service chatbot, you could evaluate 10% of the conversations for signs of frustration expressed by the user, repeated questions, or unresolved chats. Running these evaluations regularly (e.g., every hour, day, or after X conversations) will keep you updated on performance.
You can also combine LLM evaluators with other methods, like using regular expressions to check for specific terms, such as product or competitor mentions.
3. Build a dashboard.Â
To make performance tracking easier, you will need a dashboard.Â
After running an evaluation on the latest batch of data, you can add metrics like the âshare of answers labeled as hallucinatedâ or the ânumber of conversations where users expressed frustrationâ to a dashboard and visualize them over time. This helps track performance trends and spot any issues.
You can also set up alerts so that if things go off track, youâre notified right away and can step in before the issue affects too many users.
4. Look at your data!Â
Monitoring and debugging go hand in hand. Say, if you notice a rise in user frustration, you'll want to review specific problematic conversations. You can export examples to fine-tune the model or create a test set to adjust your prompts to fix the issue.
Also, once your LLM judge is live, itâs important to check in on it regularly. Even an aligned LLM evaluator â or your expectations â can shift over time, so you might need to adjust or add new judges to keep things running smoothly.
LLM Observability. You could create this workflow step by step â set up logging, storage, evaluation jobs, dashboards, and alerts. Purpose-built LLM observability tools like Evidently Cloud streamline the process. You can sign up for free and get started quickly without the need to spin up all the infrastructure yourself.
TL;DR. Pros are reasonable quality, speed, ease of updates, and the ability to run evals for custom criteria without a reference. Cons are imperfect quality, potential biases, and the costs and risks of using external LLMs.
LLM judges offer a practical, scalable solution for evaluating AI outputs, but they come with trade-offs. Here's a breakdown of its pros and cons.
High-quality evaluations. When properly set up, LLM evaluations closely match human judgment and provide consistent results. For large-scale tasks, theyâre often the only viable alternative to human review.
No reference is needed. You can use LLMs to evaluate generated outputs without a reference answer to compare to. This makes them perfect for live monitoring in production.
Itâs incredibly flexible. You can tweak prompts to evaluate anything, from how helpful a response is to how well it matches a brandâs voice, much like assigning tasks to humans.
Itâs scalable. Once configured, LLMs can handle thousands of evaluations around the clock, much faster and cheaper than human reviewers. Their fast turnaround and real-time availability are hard to beat. Plus, you can easily scale evaluations across multiple languages.
Itâs easy to update. LLM evaluators are simple to adjust as your needs and criteria change together with your product. Just update the prompt â no need to retrain a model, like in classic machine learning.
You can involve domain experts. Since LLMs use natural language, even non-technical team members can help write prompts and review results, ensuring the evaluations capture the right nuances.
Good, but not perfect. LLM evaluators arenât flawless. If the instructions are too vague or complex, the results can be subpar. LLMs might also give inconsistent judgments for the same input, though polling (asking the same question multiple times) can help. Itâs important to manage expectations and track quality.
Bias risks. LLMs are trained on vast datasets, which can carry biases into evaluations. For example, if you ask it to classify texts as ""professional"" or ""unprofessional"" without guidelines, it will rely on assumptions from its training data. For pairwise evals, there are known biases (see Zheng, L., 2023):
You can manage these by switching the order of responses or focusing on direct scoring rather than asking the LLM to pick the ""best"" response.
Data privacy. Using third-party LLM APIs for evaluations means sharing data externally. While this is often fine, considering you're already generating responses through LLMs, you need to be cautious with privacy and security â especially for sensitive data.
Itâs not instant. While LLMs are faster than humans, theyâre still slower and more expensive compared to rule-based checks or smaller ML models. This makes them less ideal for tasks like guardrails, where you need to validate an answer right as itâs being generated.
Itâs not free. Running evaluations with powerful LLMs can get expensive, especially with large datasets or multiple calls per generation. You can cut costs by mixing LLM judgments with simpler methods like regex, sampling your data, and using more efficient evaluation prompts â running a single call rather than in multiple steps.
It takes work to set up. Building an effective LLM judge requires effort. Youâll need to define criteria, design evaluation prompts, and label datasets. Itâs not a âset-it-and-forget-itâ solution either â youâll need to check in now and then to keep your judge performing well.
TL;DR. Other options include manual labeling, collecting user feedback, using task-specific ML models and rule-based checks. When a reference is available, you can use metrics like ROUGE. A mix of methods often works best.
While LLM judges are great, theyâre not the only way to evaluate AI outputs. Here are some other LLM evaluation methods to consider.
Manual labeling. Human evaluation is still the gold standard, especially for tasks that are more nuanced or subjective. It doesnât scale for large volumes, but starting with manual reviews almost always makes sense. This helps you figure out the behaviors and patterns you want to track. Once youâve nailed down your evaluation guidelines, you can automate them with LLMs. It's always a good idea to keep reviewing a portion of texts manually.
Collect user feedback. Donât forget your users! You can ask them to rate the quality of an LLMâs response directly in-app, like right after it generates a response. This gives you real-time feedback from the people actually using the system.
Purpose-built ML models. For well-defined tasks, traditional machine learning models can be very effective. There are plenty of pre-trained models available for things like sentiment analysis or detecting personal information (PII). You can also use embedding models to compare semantic similarity between texts, which is useful for things like regression testing when you need to compare old and new responses.
Fine-tuned evaluator models. As you use LLM judges, youâll naturally gather a labeled dataset that you can use to fine-tune models for specific evaluation tasks. This can be a great long-term solution if your tasks are predictable, but these models have limitations. Theyâre usually locked into their specific evaluation schemes and wonât generalize as well if the inputs or criteria change (see paper).
Generative quality metrics. For offline evaluation of structured tasks like summarization and translation, you can use metrics like ROUGE and BLEU to objectively measure response quality by comparing them to reference texts. These metrics use n-gram overlap to score how closely the output matches an ideal response.
Rule-based checks. Sometimes the simplest methods work best. Using rules, like searching for specific keywords or phrases (e.g., ""cannot"" or âapologiesâ for denial, or a list of profane words), helps catch obvious errors quickly. Itâs fast, cheap, and straightforward, plus itâs deterministic â you know exactly what youâre getting.
In practice, you combine methods. For instance, you could start with rule-based systems to filter out obvious issues, and then use LLM judges or human reviews for more complex, nuanced tasks.
LLM-as-a-Judge is a scalable alternative to human labeling. It is perfect for tasks where thereâs no single right answer and for production monitoring. If you are working on complex AI systems like RAG or AI agents, youâll need such task-specific evaluations tuned to your criteria and preferences.Â
Thatâs why we built Evidently. Our open-source library (trusted with over 25 million downloads!) makes it easy to set up and run LLM-as-a-Judge evaluations and other tests. You can use different evaluator LLMs and test judges against your own labels to better align them.
On top of it, Evidently Cloud provides a platform to collaboratively run tests and evaluations. You can generate synthetic test data, create LLM judges with no-code, and track performance over time â all in one place.
Ready to create your first LLM judge? Sign up for free or schedule a demo to see Evidently Cloud in action. We're here to help you build AI systems with confidence!",LLM-as-a-judge: a complete guide to using LLMs for evaluations,Evidently AI,2025-07-23
