fetched_at_utc,url,url_canonical,domain,headline,categories,tags,tldr,content_text,source_title,author,publish_date
2025-09-13T15:05:07.059924Z,https://medium.com/data-science/from-data-scientist-to-ml-ai-product-manager-39359bd44512,https://medium.com/data-science/from-data-scientist-to-ml-ai-product-manager-39359bd44512,medium.com,From Data Scientist to ML / AI Product Manager,"[""AI/ML"", ""Product"", ""Career""]","[""ML Product Manager"", ""AI Product Manager"", ""Data Scientist"", ""Product Strategy"", ""Product Delivery"", ""Influencing"", ""Tech Fluency""]","[""Transitioning from Data Scientist to ML/AI Product Manager requires mastering product strategy, delivery, influencing, and tech fluency."", ""Developing a clear understanding of OKRs and focusing on delivering outcomes over outputs are crucial."", ""Balancing effort vs. reward for each initiative and addressing value, usability, feasibility, and business viability are key."", ""Resources like \""Inspired: How to Create Tech Products Customers Love\"" by Marty Cagan and \""Continuous Discovery Habits\"" by Teresa Torres are recommended for learning."", ""Embracing these skills can lead to a successful and fulfilling role in ML/AI product management.""]","From Data Scientist to ML / AI Product Manager
As Artificial Intelligence is becoming more and more popular, more companies and teams want to start or increase leveraging it. Because of that, many job positions are appearing or gaining importance in the market. A good example is the figure of Machine Learning / Artificial Intelligence Product Manager.
In my case, I transitioned from a Data Scientist role into a Machine Learning Product Manager role over two years ago. During this time, I have been able to see a constant increase in job offers related to this position, blog posts and talks discussing it, and many people considering a transition or gaining interest in it. I have also been able to confirm my passion for this role and how much I enjoy my day-to-day work, responsibilities, and value I can bring to the team and company.
The role of AI / ML PM is still quite vague and evolves almost as fast as state-of-the-art AI. Although many product teams are becoming relatively autonomous using AI thanks to plug-in solutions and GenAI APIs, I will focus on the role of AI / ML PMs working in core ML teams. These teams are usually formed by Data Scientists, Machine Learning Engineers, and Research Scientists, and together with other roles are involved in solutions where GenAI through an API might not be enough (traditional ML use cases, need of LLMs fine tuning, specific in-house use cases, ML as a service products…). For an illustrative example of such a team, you can check one of my previous posts “Working in a multidisciplinary Machine Learning team to bring value to our users”.
In this blog post, we will cover the main skills and knowledge that are needed for this position, how to get there, and learnings and tips based on what worked for me in this transition.
The most important skills for an ML PM
There are many necessary skills and knowledge needed to succeed as an ML / AI PM, but the most important ones can be divided into 4 groups: product strategy, product delivery, influencing, and tech fluency. Let’s deep dive into each group to further understand what each skill set means and how to get them.
Product Strategy
Product strategy is about understanding users and their pains, identifying the right problems and opportunities, and prioritizing them based on quantitative and qualitative evidence.
As a former Data Scientist, for me this meant falling in love with the problem and user pain to solve and not so much with the specific solution, and thinking about where we can bring more value to our users instead of where to apply this cool new AI model. I have found it key to have a clear understanding of OKRs (Objective Key Results) and to care about the final impact of the initiatives (delivering outcomes instead of outputs).
Product Managers need to prioritize tasks and initiatives, so I’ve learned the importance of balancing effort vs. reward for each initiative and ensuring this influences decisions on what and how to build solutions (e.g. considering the project management triangle - scope, quality, time). Initiatives succeed if they are able to tackle the four big product risks: value, usability, feasibility, and business viability.
The most important resources I used to learn about Product Strategy are:
- Good vs bad product manager, by Ben Horowitz.
- The reference book that everyone recommended to me and that I now recommend to any aspiring PM is “Inspired: How to create tech products customers love”, by Marty Cagan.
- Another book and author that helped me get closer to user space and user problems is “Continuous Discovery Habits: Discover Products that Create Customer Value and Business Value”, by Teresa Torres.
Product Delivery
Product Delivery is about being able to manage a team’s initiative to deliver value to the users efficiently.
I started by understanding the product feature phases (discovery, plan, design, implementation, test, launch, and iterations) and what each of them meant for me as a Data Scientist. Then followed with how value can be brought “efficiently”: starting small (through Minimum Viable Products and prototypes), delivering value fast by small steps, and iterations. To ensure initiatives move in the right direction, I have found it also key to continuously measure impact (e.g. through dashboards) and learn from quantitative and qualitative data, adapting next steps with insights and new learnings.
To learn about Product Delivery, I would recommend:
- Some of the previously shared resources (e.g. Inspired book) also cover the importance of MVP, prototyping and agile applied to Product Management. I also wrote a blog post on how to think about MVPs and prototypes in the context of ML initiatives: When ML meets Product — Less is often more.
- Learning about agile and project management (for example through this crash course), and about Jira or the project management tool used by your current company (with videos such as this crash course).
Influencing
Influencing is the ability to gain trust, align with stakeholders and guide the team.
Compared to the Data Scientist’s role, the day-to-day work as a PM changes completely: it is no longer about coding, but about communicating, aligning, and (a lot!) of meetings. Great communication and storytelling become key for this role, especially the ability to explain complex ML topics to non technical people. It becomes also important to keep stakeholders informed, give visibility to the team’s hard work, and ensure alignment and buying on the future direction of the team (proving how it will help tackle the biggest challenges and opportunities, gaining trust). Finally, it is also important to learn how to challenge, say no, act as an umbrella for the team, and sometimes deliver bad results or bad news.
The resources I would recommend for this topic:
- The complete stakeholder mapping guide, Miro
- A must read book for any Data Scientist and also for any ML Product Manager is “Storytelling with data — A Data Visualization Guide for Business Professionals”, by Cole Nussbaumer Knaflic.
- To learn further about how as a Product Manager you can influence and empower the team, “EMPOWERED: Ordinary People, Extraordinary Products”, by Marty Cagan and Chris Jones.
Tech fluency
Tech fluency for an ML / AI PM, means knowledge and sensibility in Machine Learning, Responsible AI, Data in general, MLOPs, and Back End Engineering.
Your Data Science / Machine Learning / Artificial Intelligence background is probably your strongest asset, make sure you leverage it! This knowledge will allow you to talk in the same language as Data Scientists, understand deeply and challenge the projects, have sensibility on what is possible or easy and what isn’t, potential risks, dependencies, edge cases, and limitations.
As you are going to lead products with an impact on users, including responsible AI awareness becomes paramount. Risks related to not taking this into account include ethical dilemmas, company reputation, and legal issues (e.g. specific EU laws like GDPR or AI Act). In my case, I started with the course Practical Data Ethics, from Fast.ai.
General data fluency is also necessary (probably you have it covered too): analytical thinking, being curious about data, understanding where data is stored, how to access it, importance of historical data… On top of that it is also important to kow how to measure impact, the relationship with business metrics and OKRs, and experimentation (a/b testing).
As your ML models will probably need to be deployed in order to reach a final impact on users, you might work with Machine Learning Engineers within the team (or skilled DS with model deployment knowledge). You’ll need to gain sensibility about MLOPs: what it means to put a model in production, monitor it, and maintain it. In deeplearning.ai, you can find a great course on MLOPs (Machine Learning Engineering for Production Specialization).
Finally, it can happen that your team also has Back End Engineers (usually dealing with the integration of the deployed model with the rest of the platform). In my case, this was the technical field that was further away from my expertise, so I had to invest some time learning and gaining sensibility about BE. In many companies, the technical interview for PM includes some BE related questions. Make sure to get an overview of several engineering topics such as: CICD, staging vs production environments, Monolith vs MicroServices architectures (and PROs and CONTs of each setup), Pull Requests, APIs, event driven architectures….
Wrapping up and final tips
We have covered the 4 most important knowledge areas for an ML / AI PM (product strategy, product delivery, influencing and tech fluency), why they are important, and some ideas on resources that can help you achieve them.
Just like in any career progress, I found it key to define a plan, and share my short and mid term desires and expectations with managers and colleagues. Through this, I was able to transition into a PM role in the same company where I was working as a Data Scientist. This made the transition much easier: I already knew the business, product, tech, ways of working, colleagues… I also looked for mentors and colleagues within the company to whom I could ask questions, learn specific topics from and even practice for the PM interviews.
To prepare for the interviews, I focused on changing my mindset: developing vs thinking whether to build something or not, whether to launch something or not. I found out BUS (Business, User, Solution) is a great way to structure responses during interviews and enforce this new mindset there.
What I shared in this blog post can look like a lot, but it really is much easier than learning python or understanding how back-propagation works. If you are still unsure whether this role is for you or not, know that you can always give it a try, experiment, and decide to go back to your previous role. Or maybe, who knows, you end up loving being an ML / AI PM just like I do!",From Data Scientist to ML / AI Product Manager,Anna Via,2024-04-03
2025-09-13T15:07:09.608280Z,https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method,https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method,www.confident-ai.com,LLM-as-a-Judge Simply Explained: The Complete Guide to Run LLM Evals at Scale,"[""AI/ML"", ""Product"", ""Startups""]","[""LLM"", ""RAG"", ""LangGraph"", ""Agentic AI"", ""Vector DB"", ""Prompting"", ""A/B Testing"", ""Retention"", ""ML Product Manager"", ""AI Product Manager"", ""Data Scientist"", ""Product Strategy""]","[""LLM-as-a-Judge automates LLM evaluation, enhancing scalability and reliability."", ""Two types: single-output (referenceless and reference-based) and pairwise comparison."", ""Techniques to improve LLM judges include CoT prompting, in-context learning, and swapping positions."", ""DeepEval is an open-source LLM evaluation framework."", ""Confident AI offers LLM evaluation and observability tools.""]","Your LLM app is streaming out tokens faster than you can count. Thousands of words, millions of letters. Every. Single. Day. But here’s the billion-dollar question: are they any good?
Now imagine manually reviewing every single one. Staying up at 2am reading them line by line, checking for accuracy, spotting bias, and making sure they’re even relevant — it’s exhausting just thinking about it.
That’s why LLM-as-a-Judge exists — to replace that manual slog with automated scoring against custom evaluation criteria you define. Here are just a few examples:
Answer Relevancy – Did the response actually address the user’s request?
Helpfulness – Was it genuinely useful, or just pretty sentences with no substance?
Faithfulness – Did it stick to the facts instead of wandering into hallucinations?
Bias – Was it balanced, or did subtle skew creep in?
Correctness – Was it accurate from start to finish?
But, LLM judges do have their limitations, and using it without caution will cause you nothing but frustration.
In this article, I’ll let you in on everything there is to know about using LLM judges for LLM (system) evaluations so that you no longer have to stay up late to QA LLM quality.
Feeling judgemental? Let's begin.
Tl;DR
LLM-as-a-Judge is the most scalable, accurate, and reliable way to evaluate LLM apps when compared to human annotation and traditional scores like BLEU and ROUGE (backed by research)
There are two types of LLM-as-a-Judge, ""single output"" and ""pairwise"".
Single output LLM-as-a-Judge scores individual LLM outputs by outputting a score based on a custom criterion. They can be referenceless, or reference-based (requires a labelled output), and G-Eval is current one of the most common ways to create custom judges.
Pairwise LLM-as-a-judge on the other hand does not output any score, but instead choose a ""winner"" out of a list of different LLM outputs for a given input, much like the automated version of LLM arena.
Techniques to improve LLM judges include CoT prompting, in-context learning, swapping positions (for pairwise judges), and much more.
DeepEval (100% OS ⭐https://github.com/confident-ai/deepeval) allows anyone to implement both single output and pairwise LLM-as-a-Judge in under 5 lines of code, via G-Eval and Arena G-Eval respectively.
What exactly is “LLM as a Judge”?
LLM-as-a-Judge is the process of using LLMs to evaluate LLM (system) outputs, and it works by first defining an evaluation prompt based on any specific criteria of your choice, before asking an LLM judge to assign a score based on the input and outputs of your LLM system.
There are many types of LLM-as-a-judges for different use cases:
Single-output: Either reference-based or referenceless, takes in a single or multi-turn LLM interaction and outputs a quantitative verdict based on the evaluation prompt.
Pairwise: Similar to LLM arena, takes in a single or multi-turn LLM interaction and outputs which LLM app gave the better output.
We'll go through all of these in a later section, so don't worry about these too much right now.
In the real world, LLM judges are often time used as scorers for LLM-powered evaluation metrics such as G-Eval, answer relevancy, faithfulness, bias, and more. The concept is straightforward: provide an LLM with an evaluation criterion, and let it do the grading for you.
But how and where exactly would you use LLMs to judge LLM responses?
LLM-as-a-judge can be used as an automated grader/scorer for your chosen evaluation metric. To get started:
Choose the LLM you want to act as the judge.
Give it a clear, concise evaluation criterion or scoring rubric.
Provide the necessary inputs - usually the original prompt, the generated output, and any other context it needs.
The judge will then return a metric score (often between 0 and 1) based on your chosen parameters. For example, here’s a prompt you could give an LLM judge to evaluate summary coherence:
prompt = """"""
You will be given one summary (LLM output) written for a news article.
Your task is to rate the summary on how coherent it is to the original text (input).
Original Text:
{input}
Summary:
{llm_output}
Score:
""""""
By collecting these metric scores, you can build a comprehensive set of LLM evaluation results to benchmark, monitor, and regression-test your system. LLM-as-a-Judge is gaining popularity because the alternatives fall short — human evaluation is slow and costly, while traditional metrics like BERT or ROUGE miss deeper semantics in generated text.
It makes sense: How could we expect traditional, much smaller NLP models to effectively judge not just paragraphs of open-ended generated text, but also content in formats like Markdown or JSON?
LLM-as-a-Judge Use Cases
Before we go any further, it is important to understand that LLM-as-a-Judge can be used to evaluate both single and multi-turn use cases.
Single-turn refers to a single, atomic, granular interaction with your LLM app. Multi-turn on the other hand, is mainly for conversational use cases and contains not one but multiple LLM interactions. For example, RAG QA is a single-turn use case since no conversation history is involved, while a customer support chatbot is a multi-turn one.
LLM judges that score single-turn use cases expects the input and output of the LLM system you're trying to evaluate:
On the other hand, using LLM judges for multi-turn use cases involves feeding in the entire conversation into the evaluation prompt:
Multi-turn use cases are less accurate in general because there is a greater chance of context overload. However, there a various techniques to get over this problem, which you can learn more about in LLM chatbot evaluation here.
One important thing to note is, LLM judge verdicts doesn't have to be a score between 1 - 5, or 1 - 10. The range is flexible, and can also sometimes be binary, which amounts to a simple ""yes"" or ""no"" at times:
Types of LLM-as-a-Judge
As introduced in the ""Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"" paper as an alternative to human evaluation, which is often expensive and time-consuming, the three types of LLM as judges include:
Single Output Scoring (without reference): Judge LLM scores one output using a rubric, based only on the input and optional ""retrieval context"" (and tools that were called).
Single Output Scoring (with reference): Same as above, but with a gold-standard ""expected output"" to improve consistency.
Pairwise Comparison: Judge LLM sees two outputs for the same input and picks the better one based on set criteria. They let you easily compare models, prompts, and other configs in your LLM app.
⚠️ Importantly, these three judging approaches are not limited to evaluating single-turn prompts; they can also be applied to multi-turn interactions, especially for a chatbot evaluation use case.
Single-output (referenceless)
This approach uses a reference-less metric, meaning the judge LLM evaluates a single output without being shown an ideal answer. Instead, it scores the output against a predefined rubric that might include factors like accuracy, coherence, completeness, and tone.
Typical inputs include:
The original input to the LLM system
Optional supporting context, such as retrieved documents in a RAG pipeline
Optional tools called, nowadays especially for evaluating LLM agents
Example use cases:
Evaluating the helpfulness of customer support chatbot replies.
Scoring fluency and readability of creative writing generated by an LLM.
Because no gold-standard answer is provided, the judge must interpret the criteria independently. This is useful for open-ended or creative tasks where there isn’t a single correct answer.
Single-output (reference-based)
This method uses a reference-based metric, where the judge LLM is given an expected output in addition to the generated output. The reference acts as an anchor, helping the judge calibrate its evaluation and return more consistent, reproducible scores. Typical inputs include:
The original input to the LLM system.
The generated output to be scored.
A reference output, representing the expected or optimal answer.
Example use cases resolve around correctness:
Assessing the answer correctness of LLM-generated math solutions to labelled solution.
Assessing code correctness against a known correct implementation.
The presence of a reference makes this approach well-suited for tasks with clear “correct” answers. It also helps reduce judgment variability, especially for nuanced criteria like factual correctness.
Pairwise comparison
Pairwise evaluation mirrors the approach used in Chatbot Arena, but here the judge is an LLM instead of a human. The judge is given two candidate outputs for the same input and asked to choose which one better meets the evaluation criteria.
Typical inputs include:
The input to the LLM systems
Two candidate outputs, labeled in random order to prevent bias.
Evaluation criteria defining “better” (e.g., accuracy, helpfulness, tone).
This method avoids the complexity of absolute scoring, instead relying on relative quality judgments. It is particularly effective for A/B testing models, prompts, or fine-tuning approaches — and it works well even when scoring rubrics are subjective.
Example use cases:
Determining whether GPT-4 or Claude 3 produces better RAG summaries for the same query.
Choosing between two candidate marketing email drafts generated from the same brief.
If you stick till the end, I'll also show you what an LLM-Arena-as-a-Judge looks like.
Effectiveness of LLM-as-a-Judge
LLM-as-a-Judge often aligns with human judgments more closely than humans agree with each other. At first, it may seem counterintuitive - why use an LLM to evaluate text from another LLM?
The key is separation of tasks. We use a different prompt - or even a different model - dedicated to evaluation. This activates distinct capabilities and often reduces the task to a simpler classification problem: judging quality, coherence, or correctness. Detecting issues is usually easier than avoiding them in the first place.
Our testing at Confident AI shows LLM-as-a-Judge, whether single-output or pairwise, achieves surprisingly high alignment with human reviewers.
Our team aggregated annotated test cases from customers (in the form of thumbs up, thumbs down) over the past month, split the data in half, and calculated the differences in agreement between humans, and regular/arena based LLM-as-a-judge separately (implemented via G-Eval, more on this later).
What are the alternatives?
While this section arguably shouldn’t exist, two commonly preferred - but often flawed - alternatives to LLM-based evaluation are worth addressing.
Human Evaluation
- While often considered the “gold standard” for its ability to capture context and nuance, it’s slow, costly, inconsistent, and impractical at scale (e.g., ~52 full days per month to review 100k responses).
Traditional NLP Metrics (e.g., BERT, ROUGE)
- Fast and cheap but require a reference text and often miss semantic nuances in open-ended, complex outputs; both human and traditional methods also lack explainability.
Also, both human and traditional NLP evaluation methods also lack explainability, which is the ability to explain the evaluation score it has given. With that in mind, let’s go through the effectiveness of LLM judges and their pros and cons in LLM evaluation.
Confident AI: The DeepEval LLM Evaluation Platform
The leading platform to evaluate and test LLM applications on the cloud, native to DeepEval.",LLM-as-a-Judge Simply Explained: The Complete Guide to Run LLM Evals at Scale - Confident AI,Jeffrey Ip,2025-08-21
2025-09-13T16:13:01.025716Z,https://blog.langchain.com/the-rise-of-context-engineering/,https://blog.langchain.com/the-rise-of-context-engineering,blog.langchain.com,"The Rise of ""Context Engineering""","[""AI/ML"", ""Product"", ""Startups""]","[""LLM"", ""Agentic AI"", ""Prompting"", ""Product Strategy"", ""Product Delivery""]","[""Context engineering involves building dynamic systems to provide the right information and tools in the right format for LLMs to accomplish tasks."", ""Effective context engineering requires integrating various sources of context, including developer input, user interactions, tool calls, and external data."", ""The goal is to ensure LLMs receive appropriate context, instructions, and tools to perform reliably."", ""As LLM applications evolve into complex, dynamic agentic systems, context engineering becomes a crucial skill for AI engineers."", ""LangChain's LangGraph framework enables comprehensive control over context engineering, allowing developers to manage every aspect of the LLM's input and execution."", ""LangSmith, another LangChain tool, offers observability and evaluation features to test agents and track context usage, facilitating the identification and implementation of optimal context engineering strategies.""]","Header image from Dex Horthy on Twitter.
Context engineering is building dynamic systems to provide the right information and tools in the right format such that the LLM can plausibly accomplish the task.
Most of the time when an agent is not performing reliably the underlying cause is that the appropriate context, instructions and tools have not been communicated to the model.
LLM applications are evolving from single prompts to more complex, dynamic agentic systems. As such, context engineering is becoming the most important skill an AI engineer can develop.
What is context engineering?
Context engineering is building dynamic systems to provide the right information and tools in the right format such that the LLM can plausibly accomplish the task.
This is the definition that I like, which builds upon recent takes on this from Tobi Lutke, Ankur Goyal, and Walden Yan. Let’s break it down.
Context engineering is a system
Complex agents likely get context from many sources. Context can come from the developer of the application, the user, previous interactions, tool calls, or other external data. Pulling these all together involves a complex system.
This system is dynamic
Many of these pieces of context can come in dynamically. As such, the logic for constructing the final prompt needs to be dynamic as well. It is not just a static prompt.
You need the right information
A common reason agentic systems don’t perform is they just don’t have the right context. LLMs cannot read minds - you need to give them the right information. Garbage in, garbage out.
You need the right tools
It may not always be the case that the LLM will be able to solve the task just based solely on the inputs. In these situations, if you want to empower the LLM to do so, you will want to make sure that it has the right tools. These could be tools to look up more information, take actions, or anything in between. Giving the LLM the right tools is just as important as giving it the right information.
The format matters
Just like communicating with humans, how you communicate with LLMs matters. A short but descriptive error message will go a lot further a large JSON blob. This also applies to tools. What the input parameters to your tools are matters a lot when making sure that LLMs can use them.
Can it plausibly accomplish the task?
This is a great question to be asking as you think about context engineering. It reinforces that LLMs are not mind readers - you need to set them up for success. It also helps separate the failure modes. Is it failing because you haven’t given it the right information or tools? Or does it have all the right information and it just messed up? These failure modes have very different ways to fix them.
Why is context engineering important
When agentic systems mess up, it’s largely because an LLM messes. Thinking from first principles, LLMs can mess up for two reasons:
- The underlying model just messed up, it isn’t good enough
- The underlying model was not passed the appropriate context to make a good output
More often than not (especially as the models get better) model mistakes are caused more by the second reason. The context passed to the model may be bad for a few reasons:
- There is just missing context that the model would need to make the right decision. Models are not mind readers. If you do not give them the right context, they won’t know it exists.
- The context is formatted poorly. Just like humans, communication is important! How you format data when passing into a model absolutely affects how it responds
How is context engineering different from prompt engineering?
Why the shift from “prompts” to “context”? Early on, developers focused on phrasing prompts cleverly to coax better answers. But as applications grow more complex, it’s becoming clear that providing complete and structured context to the AI is far more important than any magic wording.
I would also argue that prompt engineering is a subset of context engineering. Even if you have all the context, how you assemble it in the prompt still absolutely matters. The difference is that you are not architecting your prompt to work well with a single set of input data, but rather to take a set of dynamic data and format it properly.
I would also highlight that a key part of context is often core instructions for how the LLM should behave. This is often a key part of prompt engineering. Would you say that providing clear and detailed instructions for how the agent should behave is context engineering or prompt engineering? I think it’s a bit of both.
Examples of context engineering
Some basic examples of good context engineering include:
- Tool use: Making sure that if an agent needs access to external information, it has tools that can access it. When tools return information, they are formatted in a way that is maximally digestable for LLMs
- Short term memory: If a conversation is going on for a while, creating a summary of the conversation and using that in the future.
- Long term memory: If a user has expressed preferences in a previous conversation, being able to fetch that information.
- Prompt Engineering: Instructions for how an agent should behave are clearly enumerated in the prompt.
- Retrieval: Fetching information dynamically and inserting it into the prompt before calling the LLM.
How LangGraph enables context engineering
When we built LangGraph, we built it with the goal of making it the most controllable agent framework. This also allows it to perfectly enable context engineering.
With LangGraph, you can control everything. You decide what steps are run. You decide exactly what goes into your LLM. You decide where you store the outputs. You control everything.
This allows you do all the context engineering you desire. One of the downsides of agent abstractions (which most other agent frameworks emphasize) is that they restrict context engineering. There may be places where you cannot change exactly what goes into the LLM, or exactly what steps are run beforehand.
Side note: a very good read is Dex Horthy's ""12 Factor Agents"". A lot of the points there relate to context engineering (""own your prompts"", ""own your context building"", etc). The header image for this blog is also taken from Dex. We really enjoy the way he communicates about what is important in the space.
How LangSmith helps with context engineering
LangSmith is our LLM application observability and evals solution. One of the key features in LangSmith is the ability to trace your agent calls. Although the term ""context engineering"" didn't exist when we built LangSmith, it aptly describes what this tracing helps with.
LangSmith lets you see all the steps that happen in your agent. This lets you see what steps were run to gather the data that was sent into the LLM.
LangSmith lets you see the exact inputs and outputs to the LLM. This lets you see exactly what went into the LLM - the data it had and how it was formatted. You can then debug whether that contains all the relevant information that is needed for the task. This includes what tools the LLM has access to - so you can debug whether it's been given the appropriate tools to help with the task at hand
Communication is all you need
A few months ago I wrote a blog called ""Communication is all you need"". The main point was that communicating to the LLM is hard, and not appreciated enough, and often the root cause of a lot of agent errors. Many of these points have to do with context engineering!
Context engineering isn't a new idea - agent builders have been doing it for the past year or two. It's a new term that aptly describes an increasingly important skill. We'll be writing and sharing more on this topic. We think a lot of the tools we've built (LangGraph, LangSmith) are perfectly built to enable context engineering, and so we're excited to see the emphasis on this take off.","The rise of ""context engineering""",LangChain,2025-06-23
2025-09-13T16:32:00.207193Z,https://openai.com/index/introducing-gpt-5/,https://openai.com/index/introducing-gpt-5,openai.com,Introducing GPT-5,"[""AI/ML"", ""Product"", ""Startups""]","[""LLM"", ""Agentic AI"", ""Vector DB"", ""Prompting"", ""ML Product Manager""]","[""GPT-5 is OpenAI's most advanced AI model, unifying previous breakthroughs in intelligence."", ""It offers significant improvements in accuracy, speed, reasoning, and problem-solving capabilities."", ""Businesses like Amgen and Lowe's have reported increased accuracy and reliability using GPT-5."", ""GPT-5 is available to Team users now, with access for Enterprise and Edu coming next week."", ""Developers can start using GPT-5 via the OpenAI API today.""]","We are introducing GPT‑5, our best AI system yet. GPT‑5 is a significant leap in intelligence over all our previous models, featuring state-of-the-art performance across coding, math, writing, health, visual perception, and more. It is a unified system that knows when to respond quickly and when to think longer to provide expert-level responses. GPT‑5 is available to all users, with Plus subscribers getting more usage, and Pro subscribers getting access to GPT‑5 pro, a version with extended reasoning for even more comprehensive and accurate answers.
GPT‑5 is a unified system with a smart, efficient model that answers most questions, a deeper reasoning model (GPT‑5 thinking) for harder problems, and a real‑time router that quickly decides which to use based on conversation type, complexity, tool needs, and your explicit intent (for example, if you say “think hard about this” in the prompt). The router is continuously trained on real signals, including when users switch models, preference rates for responses, and measured correctness, improving over time. Once usage limits are reached, a mini version of each model handles remaining queries. In the near future, we plan to integrate these capabilities into a single model.
GPT‑5 not only outperforms previous models on benchmarks and answers questions more quickly, but—most importantly—is more useful for real-world queries. We’ve made significant advances in reducing hallucinations, improving instruction following, and minimizing sycophancy, while leveling up GPT‑5’s performance in three of ChatGPT’s most common uses: writing, coding, and health.
GPT‑5 is our strongest coding model to date. It shows particular improvements in complex front‑end generation and debugging larger repositories. It can often create beautiful and responsive websites, apps, and games with an eye for aesthetic sensibility in just one prompt, intuitively and tastefully turning ideas into reality. Early testers also noted its design choices, with a much better understanding of things like spacing, typography, and white space. See here for full details on what GPT‑5 unlocks for developers.
Here are some examples of what GPT‑5 has created with just one prompt:
GPT‑5 is our most capable writing collaborator yet, able to help you steer and translate rough ideas into compelling, resonant writing with literary depth and rhythm. It more reliably handles writing that involves structural ambiguity, such as sustaining unrhymed iambic pentameter or free verse that flows naturally, combining respect for form with expressive clarity. These improved writing capabilities mean that ChatGPT is better at helping you with everyday tasks like drafting and editing reports, emails, memos, and more. The writing styles of GPT‑5 and GPT‑4o can be compared in the table below.
GPT‑5 is our best model yet for health-related questions, empowering users to be informed about and advocate for their health. The model scores significantly higher than any previous model on HealthBench, an evaluation we published earlier this year based on realistic scenarios and physician-defined criteria. Compared to previous models, it acts more like an active thought partner, proactively flagging potential concerns and asking questions to give more helpful answers. The model also now provides more precise and reliable responses, adapting to the user’s context, knowledge level, and geography, enabling it to provide safer and more helpful responses in a wide range of scenarios. Importantly, ChatGPT does not replace a medical professional—think of it as a partner to help you understand results, ask the right questions in the time you have with providers, and weigh options as you make decisions.
You can see some of the ways GPT‑5 is better than our previous models across domains—richer, more detailed, and useful—in these examples:
GPT-4o
GPT-5
GPT‑5 is much smarter across the board, as reflected by its performance on academic and human-evaluated benchmarks, particularly in math, coding, visual perception, and health. It sets a new state of the art across math (94.6% on AIME 2025 without tools), real-world coding (74.9% on SWE-bench Verified, 88% on Aider Polyglot), multimodal understanding (84.2% on MMMU), and health (46.2% on HealthBench Hard)—and those gains show up in everyday use. With GPT‑5 pro’s extended reasoning, the model also sets a new SOTA on GPQA, scoring 88.4% without tools.
GPT‑5 shows significant gains in benchmarks that test instruction following and agentic tool use, the kinds of capabilities that let it reliably carry out multi-step requests, coordinate across different tools, and adapt to changes in context. In practice, this means it’s better at handling complex, evolving tasks; GPT‑5 can follow your instructions more faithfully and get more of the work done end-to-end using the tools at its disposal.
The model excels across a range of multimodal benchmarks, spanning visual, video-based, spatial, and scientific reasoning. Stronger multimodal performance means ChatGPT can reason more accurately over images and other non-text inputs—whether that’s interpreting a chart, summarizing a photo of a presentation, or answering questions about a diagram.
GPT‑5 is also our best performing model on an internal benchmark measuring performance on complex, economically valuable knowledge work. When using reasoning, GPT‑5 is comparable to or better than experts in roughly half the cases, while outperforming o3 and ChatGPT Agent, across tasks spanning over 40 occupations including law, logistics, sales, and engineering.
GPT‑5 gets more value out of less thinking time. In our evaluations, GPT‑5 (with thinking) performs better than OpenAI o3 with 50-80% less output tokens across capabilities, including visual reasoning, agentic coding, and graduate-level scientific problem solving.
GPT‑5 was trained on Microsoft Azure AI supercomputers.
GPT‑5 is significantly less likely to hallucinate than our previous models. With web search enabled on anonymized prompts representative of ChatGPT production traffic, GPT‑5’s responses are ~45% less likely to contain a factual error than GPT‑4o, and when thinking, GPT‑5’s responses are ~80% less likely to contain a factual error than OpenAI o3.
We’ve particularly invested in making our models more reliable when reasoning on complex, open-ended questions. Accordingly, we’ve added new evaluations to stress‑test open-ended factuality. We measured GPT‑5’s hallucination rate when thinking on open-ended fact-seeking prompts from two public factuality benchmarks: LongFact(opens in a new window) (concepts and objects) and FActScore(opens in a new window). Across all of these benchmarks, “GPT‑5 thinking” shows a sharp drop in hallucinations—about six times fewer than o3—marking a clear leap forward in producing consistently accurate long-form content. Implementation and grading details for our evaluations on these benchmarks can be found in the system card.
Alongside improved factuality, GPT‑5 (with thinking) more honestly communicates its actions and capabilities to the user—especially for tasks which are impossible, underspecified, or missing key tools. In order to achieve a high reward during training, reasoning models may learn to lie about successfully completing a task or be overly confident about an uncertain answer. For example, to test this, we removed all the images from the prompts of the multimodal benchmark CharXiv, and found that OpenAI o3 still gave confident answers about non-existent images 86.7% of the time, compared to just 9% for GPT‑5.
When reasoning, GPT‑5 more accurately recognizes when tasks can’t be completed and communicates its limits clearly. We evaluated deception rates on settings involving impossible coding tasks and missing multimodal assets, and found that GPT‑5 (with thinking) is less deceptive than o3 across the board. On a large set of conversations representative of real production ChatGPT traffic, we’ve reduced rates of deception from 4.8% for o3 to 2.1% of GPT‑5 reasoning responses. While this represents a meaningful improvement for users, more work remains to be done, and we’re continuing research into improving the factuality and honesty of our models. Further details can be found in the system card.
Before mitigation
After mitigation
GPT‑5 advances the frontier on safety. In the past, ChatGPT relied primarily on refusal-based safety training: based on the user’s prompt, the model should either comply or refuse. While this type of training works well for explicitly malicious prompts, it can struggle to handle situations where the user’s intent is unclear, or information could be used in benign or malicious ways. Refusal training is especially inflexible for dual-use domains such as virology, where a benign request can be safely completed at a high level, but might enable a bad actor if completed in detail.
For GPT‑5, we introduced a new form of safety-training — safe completions — which teaches the model to give the most helpful answer where possible while still staying within safety boundaries. Sometimes, that may mean partially answering a user’s question or only answering at a high level. If the model needs to refuse, GPT‑5 is trained to transparently tell you why it is refusing, as well as provide safe alternatives. In both controlled experiments and our production models, we find that this approach is more nuanced, enabling better navigation of dual-use questions, stronger robustness to ambiguous intent, and fewer unnecessary overrefusals. Read more about our new approach to safety-training, as well as full details on methodology, metrics, and results, in our safe completion paper.
Overall, GPT‑5 is less effusively agreeable, uses fewer unnecessary emojis, and is more subtle and thoughtful in follow‑ups compared to GPT‑4o. It should feel less like “talking to AI” and more like chatting with a helpful friend with PhD‑level intelligence.
Earlier this year, we released an update to GPT‑4o that unintentionally made the model overly sycophantic, or excessively flattering or agreeable. We quickly rolled back the change and have since worked to understand and reduce this behavior by:
- Developing new evaluations to measure sycophancy levels
- Improving our training so the model is less sycophantic—for instance, adding examples that would normally lead to over-agreement, and then teaching it not to do that.
In targeted sycophancy evaluations using prompts specifically designed to elicit sycophantic responses, GPT‑5 meaningfully reduced sycophantic replies (from 14.5% to less than 6%). At times, reducing sycophancy can come with reductions in user satisfaction, but the improvements we made cut sycophancy by more than half while also delivering other measurable gains, so users continue to have high-quality, constructive conversations—in line with our goal to help people use ChatGPT well.
GPT‑5 is significantly better at instruction following, and we see a corresponding improvement in its ability to follow custom instructions.
We’re also launching a research preview of four new preset personalities for all ChatGPT users, made possible by the improvements on steerability. These personalities, available initially for text chat and coming later to Voice, let you set how ChatGPT interacts—whether concise and professional, thoughtful and supportive, or a bit sarcastic—without writing custom prompts. The four initial options, Cynic, Robot, Listener, and Nerd, are opt-in, adjustable anytime in settings, and designed to match your communication style.
All of these new personalities meet or exceed our bar on internal evals for reducing sycophancy.
We look forward to learning and iterating based on early feedback.
We decided to treat the “GPT‑5 thinking” model as High capability in the Biological and Chemical domain, and have implemented strong safeguards to sufficiently minimize the associated risks. We rigorously tested the model with our safety evaluations under our Preparedness Framework, completing 5,000 hours of red-teaming with partners like the CAISI and UK AISI.
Similar to our approach for ChatGPT Agent, while we do not have definitive evidence that this model could meaningfully help a novice to create severe biological harm–our defined threshold(opens in a new window) for High capability–we are taking a precautionary approach and are activating the required safeguards now in order to increase readiness for when such capabilities are available. As a result, “GPT‑5 thinking” has a robust safety stack with a multilayered defense system for biology: comprehensive threat modeling, training the model to not output harmful content through our new safe completions paradigm, always-on classifiers and reasoning monitors, and clear enforcement pipelines.
For the most challenging, complex tasks, we are also releasing GPT‑5 pro, replacing OpenAI o3‑pro, a variant of GPT‑5 that thinks for ever longer, using scaled but efficient parallel test-time compute, to provide the highest quality and most comprehensive answers. GPT‑5 pro achieves the highest performance in the GPT‑5 family on several challenging intelligence benchmarks, including state-of-the-art performance on GPQA, which contains extremely difficult science questions.
In evaluations on over 1000 economically valuable, real-world reasoning prompts, external experts preferred GPT‑5 pro over ""GPT‑5 thinking"" 67.8% of the time. GPT‑5 pro made 22% fewer major errors and excelled in health, science, mathematics, and coding. Experts rated its responses as relevant, useful, and comprehensive.
GPT‑5 is the new default in ChatGPT, replacing GPT‑4o, OpenAI o3, OpenAI o4-mini, GPT‑4.1, and GPT‑4.5 for signed-in users. Just open ChatGPT and type your question; GPT‑5 handles the rest, applying reasoning automatically when the response would benefit from it. Paid users can still select “GPT‑5 Thinking” from the model picker, or type something like ‘think hard about this’ in the prompt to ensure reasoning is used when generating a response.
GPT‑5 is starting to roll out today to all Plus, Pro, Team, and Free users, with access for Enterprise and Edu coming next week. Pro, Plus, and Team users can also start coding with GPT‑5 in the Codex CLI(opens in a new window) by signing in with ChatGPT.
As with GPT‑4o, the difference between free and paid access to GPT‑5 is usage volume. Pro subscribers get unlimited access to GPT‑5, and access to GPT‑5 Pro. Plus users can use it comfortably as their default model for everyday questions, with significantly higher usage than free users. Team, Enterprise, and Edu customers can also use GPT‑5 comfortably as their default model for everyday work, with generous limits that make it easy for entire organizations to rely on GPT‑5. For ChatGPT free-tier users, full reasoning capabilities may take a few days to fully roll out. Once free users reach their GPT‑5 usage limits, they will transition to GPT‑5 mini, a smaller, faster, and highly capable model.
Footnotes
*There is a small discrepancy with numbers reported in our previous blog post, as those were run on a former version of HLE.
**We find that the default grader in MultiChallenge (GPT-4o) frequently mis-scores model responses. We find that swapping the grader to a reasoning model, like o3-mini, improves accuracy on grading significantly on samples we’ve inspected.
***For MMMUPro, we averaged scores for standard and vision.
Contributors
Aaditya Singh, Adam Fry, Adam Perelman, Adam Tart, Adi Ganesh, Ahmed El-Kishky, Aidan McLaughlin, Aiden Low, AJ Ostrow, Akhila Ananthram, Akshay Nathan, Alan Luo, Alec Helyar, Aleksander Madry, Aleksandr Efremov, Aleksandra Spyra, Alex Baker-Whitcomb, Alex Beutel, Alex Karpenko, Alex Makelov, Alex Neitz, Alex Wei, Alexandra Barr, Alexandre Kirchmeyer, Alexey Ivanov, Alexi Christakis, Alistair Gillespie, Allison Tam, Ally Bennett, Alvin Wan, Alyssa Huang, Amy McDonald Sandjiveh, Amy Yang, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrei Gheorghe, Andres Garcia Garcia, Andrew Braunstein, Andrew Liu, Andrew Schmidt, Andrey Mereskin, Andrey Mishchenko, Andy Applebaum, Andy Rogerson, Ann Rajan, Annie Wei, Anoop Kotha, Anubha Srivastava, Anushree Agrawal, Arun Vijayvergiya, Ashley Tyra, Ashvin Nair, Avi Nayak, Ben Eggers, Bessie Ji, Beth Hoover, Bill Chen, Blair Chen, Boaz Barak, Borys Minaiev, Botao Hao, Bowen Baker, Brad Lightcap, Brandon McKinzie, Brandon Wang, Brendan Quinn, Brian Fioca, Brian Hsu, Brian Yang, Brian Yu, Brian Zhang, Brittany Brenner, Callie Riggins Zetino, Cameron Raymond, Camillo Lugaresi, Carolina Paz, Cary Hudson, Cedric Whitney, Chak Li, Charles Chen, Charlotte Cole, Chelsea Voss, Chen Ding, Chen Shen, Chengdu Huang, Chris Colby, Chris Hallacy, Chris Koch, Chris Lu, Christina Kim, CJ Minott-Henriques, Cliff Frey, Cody Yu, Coley Czarnecki, Colin Reid, Colin Wei, Cory Decareaux, Cristina Scheau, Cyril Zhang, Cyrus Forbes, Da Tang, Dakota Goldberg, Dan Roberts, Dana Palmie, Daniel Kappler, Daniel Levine, Daniel Wright, Dave Leo, David Lin, David Robinson, Declan Grabb, Derek Chen, Derek Lim, Dimitris Tsipras, Dinghua Li, Dingli Yu, DJ Strouse, Drew Williams, Dylan Hunn, Ed Bayes, Edwin Arbus, Ekin Akyurek, Elaine Ya Le, Elana Widmann, Eli Yani, Elizabeth Proehl, Enis Sert, Enoch Cheung, Eri Schwartz, Eric Han, Eric Jiang, Eric Mitchell, Eric Sigler, Eric Wallace, Erik Ritter, Erin Kavanaugh, Evan Mays, Evgenii Nikishin, Fangyuan Li, Felipe Petroski Such, Filipe de Avila Belbute Peres, Filippo Raso, Florent Bekerman, Foivos Tsimpourlas, Fotis Chantzis, Francis Song, Francis Zhang, Gaby Raila, Garrett McGrath, Gary Briggs, Gary Yang, Giambattista Parascandolo, Gildas Chabot, Grace Kim, Grace Zhao, Gregory Valiant, Guillaume Leclerc, Hadi Salman, Hanson Wang, Hao Sheng, Haoming Jiang, Haoyu Wang, Haozhun Jin, Harshit Sikchi, Heather Schmidt, Henry Aspegren, Honglin Chen, Huida Qiu, Hunter Lightman, Ian Covert, Ian Kivlichan, Ian Silber, Ian Sohl, Ibrahim Hammoud, Ignasi Clavera, Ikai Lan, Ilge Akkaya, Ilya Kostrikov, Irina Kofman, Isak Etinger, Ishaan Singal, Jackie Hehir, Jacob Huh, Jacqueline Pan, Jake Wilczynski, Jakub Pachocki, James Lee, James Quinn, Jamie Kiros, Janvi Kalra, Jasmyn Samaroo, Jason Wang, Jason Wolfe, Jay Chen, Jay Wang, Jean Harb, Jeffrey Han, Jeffrey Wang, Jennifer Zhao, Jeremy Chen, Jerene Yang, Jerry Tworek, Jesse Chand, Jessica Landon, Jessica Liang, Ji Lin, Jiancheng Liu, Jianfeng Wang, Jie Tang, Jihan Yin, Joanne Jang, Joel Morris, Joey Flynn, Johannes Ferstad, Johannes Heidecke, John Fishbein, John Hallman, Jonah Grant, Jonathan Chien, Jonathan Gordon, Jongsoo Park, Jordan Liss, Jos Kraaijeveld, Joseph Guay, Joseph Mo, Josh Lawson, Josh McGrath, Joshua Vendrow, Joy Jiao, Julian Lee, Julie Steele, Julie Wang, Junhua Mao, Kai Chen, Kai Hayashi, Kai Xiao, Kan Wu, Karan Sekhri, Karan Singhal, Karen Li, Kenny Nguyen, Keren Gu-Lemberg, Kevin King, Kevin Liu, Kevin Stone, Kevin Yu, Kristen Ying, Kristian Georgiev, Kristie Lim, Kushal Tirumala, Kyle Miller, Larry Lv, Laura Clare, Laurance Fauconnet, Lauren Itow, Lauren Yang, Laurentia Romaniuk, Leah Anise, Lee Byron, Leher Pathak, Leon Maksin, Leyan Lo, Leyton Ho, Li Jing, Liang Wu, Liang Xiong, Lien Mamitsuka, Lin Yang, Lindsay McCallum, Lindsey Held, Liz Bourgeois, Logan Engstrom, Lorenz Kuhn, Louis Feuvrier, Lu Zhang, Lucas Switzer, Lukas Kondraciuk, Lukasz Kaiser, Manas Joglekar, Mandeep Singh, Mandip Shah, Manuka Stratta, Marcus Williams, Mark Chen, Mark Sun, Marselus Cayton, Martin Li, Marwan Aljubeh, Matt Nichols, Matthew Haines, Max Schwarzer, Mayank Gupta, Meghan Shah, Melody Huang, Meng Dong, Mengqing Wang, Mia Glaese, Micah Carroll, Michael Lampe, Michael Malek, Michael Sharman, Michael Zhang, Michele Wang, Michelle Pokrass, Mihai Florian, Mikhail Pavlov, Miles Wang, Ming Chen, Mingxuan Wang, Minnia Feng, Mo Bavarian, Molly Lin, Moose Abdool, Mostafa Rohaninejad, Nacho Soto, Natalie Staudacher, Natan LaFontaine, Nathan Marwell, Nelson Liu, Nick Preston, Nick Turley, Nicklas Ansman, Nicole Blades, Nikil Pancha, Nikita Mikhaylin, Niko Felix, Nitish Keskar, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Oona Gleeson, Pamela Mishkin, Patryk Lesiewicz, Paul Baltescu, Pavel Belov, Peter Zhokhov, Philip Pronin, Phillip Guo, Phoebe Thacker, Qi Liu, Qiming Yuan, Qinghua Liu, Rachel Dias, Rachel Puckett, Rahul Arora, Ravi Teja Mullapudi, Raz Gaon, Reah Miyara, Rennie Song, Rishabh Aggarwal, RJ Marsan, Robel Yemiru, Robert Xiong, Rohan Kshirsagar, Rohan Nuttall, Roman Tsiupa, Ronen Eldan, Rose Wang, Roshan James, Roy Ziv, Rui Shu, Ruslan Nigmatullin, Saachi Jain, Saam Talaie, Sam Altman, Sam Arnesen, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Sarah Yoo, Savannah Heon, Scott Ethersmith, Sean Grove, Sean Taylor, Sebastien Bubeck, Sever Banesiu, Shaokyi Amdo, Shengjia Zhao, Shibani Santurkar, Shiyu Zhao, Shraman Ray Chaudhuri, Shreyas Krishnaswamy, Shuaiqi (Tony) Xia, Shuyang Cheng, Shyamal Anadkat, Simón Posada Fishman, Simon Tobin, Siyuan Fu, Somay Jain, Song Mei, Sonya Egoian, Spencer Kim, Spug Golden, SQ Mah, Steph Lin, Stephen Imm, Steve Sharpe, Steve Yadlowsky, Sungwon Eum, Sulman Choudhry, Suvansh Sanjeev, Tal Stramer, Tao Wang, Tao Xin, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Degry, Thomas Shadwell, Tianfu Fu, Tianshi Gao, Timur Garipov, Tina Sriskandarajah, Toki Sherbakov, Tomer Kaftan, Tomo Hiratsuka, Tongzhou Wang, Tony Song, Tony Zhao, Troy Peterson, Val Kharitonov, Victoria Chernova, Vineet Kosaraju, Vishal Kuo, Vitchyr Pong, Vivek Verma, Vlad Petrov, Wanning Jiang, Weixing Zhang, Wenda Zhou, Wenlei Xie, Wenting Zhan, Wes McCabe, Will DePue, Will Ellsworth, Wulfie Bain, Wyatt Thompson, Xiangning Chen, Xiangyu Qi, Xin Xiang, Xinwei Shi, Yann Dubois, Yaodong Yu, Yara Khakbaz, Yifan Wu, Yin Tat Lee, Yinbo Chen, Yizhen Zhang, Yizhong Xiong, Yonglong Tian, Young Cha, Yu Bai, Yu Yang, Yuan Yuan, Yuanzhi Li, Yufeng Zhang, Yujia Jin, Yun Jiang, Yunyun Wang, Yushi Wang, Yutian Liu, Zach Stubenvoll, Zehao Dou, Zheng Wu, Zhigang Wang",Introducing GPT-5,OpenAI,2025-08-07
