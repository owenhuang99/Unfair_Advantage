{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8259648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de2608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Link Ingestion MVP (Notebook-Ready)\n",
    "# =========================\n",
    "# deps: openai, trafilatura, bs4, lxml or html5lib, python-dotenv, pandas, requests\n",
    "# usage (quick start at bottom of this cell)\n",
    "import os, re, json, datetime, typing as T\n",
    "from dataclasses import dataclass\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import trafilatura\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf84379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Load environment (expects OPENAI_API_KEY) ----\n",
    "load_dotenv()\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "effff292",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =========================\n",
    "# Config (edit as you like)\n",
    "# =========================\n",
    "MODEL_WITH_WEB = os.getenv(\"MODEL_WITH_WEB\", \"gpt-4o-mini\")     # has web tool on eligible accounts\n",
    "MODEL_FALLBACK = os.getenv(\"MODEL_FALLBACK\", \"gpt-4.1-mini\")    # local LLM fallback\n",
    "TAXONOMY_PATH  = os.getenv(\"TAXONOMY_PATH\", \"taxonomy.json\")    # where allowed lists persist\n",
    "CSV_PATH       = os.getenv(\"CSV_PATH\", \"links_store.csv\")       # where your DataFrame persists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4adba6",
   "metadata": {},
   "source": [
    "# MVP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a7df4b",
   "metadata": {},
   "source": [
    "## Function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6251884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:128.0) Gecko/20100101 Firefox/128.0\"\n",
    "}\n",
    "\n",
    "STRICT_JSON_RULES = (\n",
    "    \"Return STRICT JSON with keys: \"\n",
    "    '[\"title\",\"author\",\"publish_date\",\"categories\",\"tags\",\"tldr\",\"language\",\"citations\",\"confidence_notes\"]. '\n",
    "    \"categories=1–3 short labels; tags=5–12 keywords; tldr=3–6 crisp bullets; \"\n",
    "    \"language=two-letter ISO code; citations=list of {title,url}; \"\n",
    "    \"confidence_notes=1–2 short sentences. No markdown—JSON only.\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Types\n",
    "# =========================\n",
    "@dataclass\n",
    "class PageContent:\n",
    "    url: str\n",
    "    domain: str\n",
    "    title: T.Optional[str]\n",
    "    author: T.Optional[str]\n",
    "    publish_date: T.Optional[str]\n",
    "    text: str\n",
    "    html_len: int\n",
    "    text_len: int\n",
    "\n",
    "# =========================\n",
    "# Helpers: parsing & fetching\n",
    "# =========================\n",
    "def _best_bs4_parser():\n",
    "    try:\n",
    "        import lxml  # noqa\n",
    "        return \"lxml\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            import html5lib  # noqa\n",
    "            return \"html5lib\"\n",
    "        except Exception:\n",
    "            return \"html.parser\"\n",
    "\n",
    "def _safe_json_loads(text: str) -> dict:\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}\", text, re.S)\n",
    "        return json.loads(m.group(0)) if m else {\"error\": \"Non-JSON output\", \"raw\": text[:1200]}\n",
    "\n",
    "def _fetch_html(url: str) -> str:\n",
    "    r = requests.get(url, headers=DEFAULT_HEADERS, timeout=25)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def _guess_meta(soup: BeautifulSoup) -> dict:\n",
    "    meta = {\"title\": None, \"author\": None, \"publish_date\": None}\n",
    "    # title\n",
    "    if soup.title and soup.title.string:\n",
    "        meta[\"title\"] = soup.title.string.strip()\n",
    "    ogt = soup.find(\"meta\", property=\"og:title\")\n",
    "    if ogt and ogt.get(\"content\"):\n",
    "        meta[\"title\"] = ogt[\"content\"].strip()\n",
    "    # author\n",
    "    for k in [\"author\", \"article:author\", \"og:article:author\"]:\n",
    "        m = soup.find(\"meta\", attrs={\"name\": k}) or soup.find(\"meta\", attrs={\"property\": k})\n",
    "        if m and m.get(\"content\"):\n",
    "            meta[\"author\"] = m[\"content\"].strip(); break\n",
    "    # date\n",
    "    for k in [\"article:published_time\", \"og:published_time\", \"pubdate\", \"publish-date\", \"date\"]:\n",
    "        m = soup.find(\"meta\", attrs={\"name\": k}) or soup.find(\"meta\", attrs={\"property\": k})\n",
    "        if m and m.get(\"content\"):\n",
    "            meta[\"publish_date\"] = m[\"content\"].strip(); break\n",
    "    if meta[\"publish_date\"] is None:\n",
    "        t = soup.find(\"time\")\n",
    "        if t and (t.get(\"datetime\") or t.text):\n",
    "            meta[\"publish_date\"] = (t.get(\"datetime\") or t.text).strip()\n",
    "    return meta\n",
    "\n",
    "def extract_readable_text(url: str) -> PageContent:\n",
    "    html = _fetch_html(url)\n",
    "    text = (trafilatura.extract(html, include_comments=False, include_tables=False, favor_precision=True) or \"\").strip()\n",
    "\n",
    "    if len(text) < 200:\n",
    "        PARSER = _best_bs4_parser()\n",
    "        soup = BeautifulSoup(html, PARSER)\n",
    "        for tag in soup([\"script\",\"style\",\"noscript\",\"header\",\"footer\",\"nav\",\"aside\"]):\n",
    "            tag.extract()\n",
    "        text = soup.get_text(\"\\n\", strip=True)\n",
    "\n",
    "    PARSER = _best_bs4_parser()\n",
    "    soup_full = BeautifulSoup(html, PARSER)\n",
    "    meta = _guess_meta(soup_full)\n",
    "\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return PageContent(\n",
    "        url=url,\n",
    "        domain=urlparse(url).netloc,\n",
    "        title=meta[\"title\"],\n",
    "        author=meta[\"author\"],\n",
    "        publish_date=meta[\"publish_date\"],\n",
    "        text=text,\n",
    "        html_len=len(html or \"\"),\n",
    "        text_len=len(text or \"\")\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# Taxonomy persistence\n",
    "# =========================\n",
    "def load_taxonomy(path: str = TAXONOMY_PATH) -> dict:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        return {\"categories\": [], \"tags\": []}\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    # ensure keys\n",
    "    data.setdefault(\"categories\", [])\n",
    "    data.setdefault(\"tags\", [])\n",
    "    return data\n",
    "\n",
    "def save_taxonomy(categories: T.List[str], tags: T.List[str], path: str = TAXONOMY_PATH) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"categories\": categories, \"tags\": tags}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# =========================\n",
    "# Matching & evolving lists\n",
    "# =========================\n",
    "def _normalize_token(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s.strip().lower())\n",
    "\n",
    "def update_matches(candidates: T.List[str], allowed: T.List[str], max_k: int) -> T.Tuple[T.List[str], T.List[str]]:\n",
    "    \"\"\"\n",
    "    Try to match candidates against allowed list.\n",
    "    If no match found for a candidate, add it as a new allowed term.\n",
    "    Returns (final_matches, updated_allowed_list).\n",
    "    \"\"\"\n",
    "    allowed_norm = { _normalize_token(a): a for a in allowed }\n",
    "    matches, updated_allowed = [], allowed.copy()\n",
    "    seen = set()\n",
    "\n",
    "    for c in candidates:\n",
    "        if not isinstance(c, str):\n",
    "            continue\n",
    "        c_clean = c.strip()\n",
    "        if not c_clean:\n",
    "            continue\n",
    "        c_norm = _normalize_token(c_clean)\n",
    "\n",
    "        hit = None\n",
    "        # exact\n",
    "        if c_norm in allowed_norm:\n",
    "            hit = allowed_norm[c_norm]\n",
    "        else:\n",
    "            # substring match either way\n",
    "            for an_norm, raw in allowed_norm.items():\n",
    "                if c_norm in an_norm or an_norm in c_norm:\n",
    "                    hit = raw; break\n",
    "        if hit:\n",
    "            if hit not in seen:\n",
    "                matches.append(hit); seen.add(hit)\n",
    "        else:\n",
    "            # new term → append to allowed\n",
    "            if c_clean not in updated_allowed:\n",
    "                updated_allowed.append(c_clean)\n",
    "            if c_clean not in seen:\n",
    "                matches.append(c_clean); seen.add(c_clean)\n",
    "\n",
    "        if len(matches) >= max_k:\n",
    "            break\n",
    "\n",
    "    return matches, updated_allowed\n",
    "\n",
    "# =========================\n",
    "# LLM calls (web tool → local fallback)\n",
    "# =========================\n",
    "def analyze_link_with_web_tool(url: str, allowed_categories: T.List[str], allowed_tags: T.List[str]) -> dict:\n",
    "    if not hasattr(client, \"responses\"):\n",
    "        raise RuntimeError(\"responses_api_unavailable\")\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"{STRICT_JSON_RULES}\\n\\n\"\n",
    "        f\"When labeling categories and tags, prefer from the lists below when semantically appropriate.\\n\"\n",
    "        f\"Allowed categories: {json.dumps(allowed_categories[:50])}\\n\"\n",
    "        f\"Allowed tags: {json.dumps(allowed_tags[:200])}\\n\\n\"\n",
    "        f\"Read this URL and summarize with citations: {url}\"\n",
    "    )\n",
    "\n",
    "    resp = client.responses.create(\n",
    "        model=MODEL_WITH_WEB,\n",
    "        tools=[{\"type\": \"web_search\"}],\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise analyst that reads the provided URL using the web tool.\"},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "    )\n",
    "    output_text = getattr(resp, \"output_text\", getattr(resp, \"output\", \"\"))\n",
    "    return _safe_json_loads(output_text)\n",
    "\n",
    "def summarize_local_content(page: PageContent, allowed_categories: T.List[str], allowed_tags: T.List[str]) -> dict:\n",
    "    payload = {\n",
    "        \"source_url\": page.url,\n",
    "        \"detected_title\": page.title,\n",
    "        \"detected_author\": page.author,\n",
    "        \"detected_publish_date\": page.publish_date,\n",
    "        \"article_text\": page.text[:22_000],\n",
    "        \"allowed_categories\": allowed_categories[:50],\n",
    "        \"allowed_tags\": allowed_tags[:200]\n",
    "    }\n",
    "    if hasattr(client, \"responses\"):\n",
    "        resp = client.responses.create(\n",
    "            model=MODEL_FALLBACK,\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": STRICT_JSON_RULES},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(payload)}\n",
    "            ],\n",
    "        )\n",
    "        output_text = getattr(resp, \"output_text\", getattr(resp, \"output\", \"\"))\n",
    "        return _safe_json_loads(output_text)\n",
    "    else:\n",
    "        # very old fallback\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": STRICT_JSON_RULES},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(payload)},\n",
    "        ]\n",
    "        cc = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "        return _safe_json_loads(cc.choices[0].message.content)\n",
    "\n",
    "# =========================\n",
    "# Public API: main function\n",
    "# =========================\n",
    "def analyze_link_plus(\n",
    "    url: str,\n",
    "    allowed_categories: T.List[str] = None,\n",
    "    allowed_tags: T.List[str] = None,\n",
    "    force_local: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Returns a normalized record (dict) and includes updated taxonomy under _taxonomy.\n",
    "    Keys:\n",
    "        fetched_at_utc, url, domain, headline, categories, tags, tldr (list),\n",
    "        content_text, source_title, author, publish_date, _source, _taxonomy\n",
    "    \"\"\"\n",
    "    assert url.startswith(\"http\"), \"Pass a valid http(s) URL.\"\n",
    "    allowed_categories = allowed_categories or []\n",
    "    allowed_tags = allowed_tags or []\n",
    "\n",
    "    # parse locally first so you can verify and fall back if needed\n",
    "    page = extract_readable_text(url)\n",
    "\n",
    "    # LLM metadata\n",
    "    try:\n",
    "        if force_local:\n",
    "            raise RuntimeError(\"forced_local\")\n",
    "        llm_data = analyze_link_with_web_tool(url, allowed_categories, allowed_tags)\n",
    "        mode = \"openai_web_tool\"\n",
    "        model_used = MODEL_WITH_WEB\n",
    "    except Exception:\n",
    "        if page.text_len < 200:\n",
    "            raise RuntimeError(\"Could not extract enough text; page may be paywalled or script-rendered.\")\n",
    "        llm_data = summarize_local_content(page, allowed_categories, allowed_tags)\n",
    "        mode = \"local_fallback\"\n",
    "        model_used = MODEL_FALLBACK\n",
    "\n",
    "    # normalize fields\n",
    "    fetched_at_utc = datetime.datetime.utcnow().isoformat() + \"Z\"\n",
    "    raw_categories = llm_data.get(\"categories\") or []\n",
    "    raw_tags = llm_data.get(\"tags\") or []\n",
    "    tldr = llm_data.get(\"tldr\") or []\n",
    "\n",
    "    # TLDR as list\n",
    "    if isinstance(tldr, str):\n",
    "        parts = [p.strip(\" -•\\t\") for p in re.split(r\"[\\n•\\-]+\", tldr) if p.strip()]\n",
    "        tldr = parts[:6] if parts else [llm_data.get(\"tldr\", \"\")]\n",
    "\n",
    "    # evolve taxonomy\n",
    "    categories, updated_categories = update_matches(raw_categories, allowed_categories, max_k=3)\n",
    "    tags, updated_tags = update_matches(raw_tags, allowed_tags, max_k=12)\n",
    "\n",
    "    record = {\n",
    "        \"fetched_at_utc\": fetched_at_utc,\n",
    "        \"url\": page.url,\n",
    "        \"domain\": page.domain,\n",
    "        \"headline\": (llm_data.get(\"title\") or page.title or \"\").strip()[:300],\n",
    "        \"categories\": categories,\n",
    "        \"tags\": tags,\n",
    "        \"tldr\": tldr,\n",
    "        \"content_text\": page.text,    # full parsed text so you can verify LLM output\n",
    "        \"source_title\": page.title,\n",
    "        \"author\": llm_data.get(\"author\") or page.author,\n",
    "        \"publish_date\": llm_data.get(\"publish_date\") or page.publish_date,\n",
    "        \"_source\": {\"mode\": mode, \"model\": model_used},\n",
    "        \"_taxonomy\": {\n",
    "            \"updated_categories\": updated_categories,\n",
    "            \"updated_tags\": updated_tags\n",
    "        }\n",
    "    }\n",
    "    return record\n",
    "\n",
    "# # =========================\n",
    "# # DataFrame Store (in-memory + CSV)\n",
    "# # =========================\n",
    "# COLUMNS = [\n",
    "#     \"fetched_at_utc\",\"url\",\"domain\",\"headline\",\"categories\",\"tags\",\"tldr\",\n",
    "#     \"content_text\",\"source_title\",\"author\",\"publish_date\"\n",
    "# ]\n",
    "\n",
    "# def init_store() -> pd.DataFrame:\n",
    "#     return pd.DataFrame(columns=COLUMNS)\n",
    "\n",
    "# def append_record(df: pd.DataFrame, record: dict) -> pd.DataFrame:\n",
    "#     # store lists as Python lists (dtype=object)\n",
    "#     row = {k: record.get(k, None) for k in COLUMNS}\n",
    "#     return pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# def save_csv(df: pd.DataFrame, path: str = CSV_PATH) -> None:\n",
    "#     df2 = df.copy()\n",
    "#     for col in [\"categories\",\"tags\",\"tldr\"]:\n",
    "#         if col in df2.columns:\n",
    "#             df2[col] = df2[col].apply(lambda x: x if isinstance(x, str) else json.dumps(x, ensure_ascii=False))\n",
    "#     df2.to_csv(path, index=False)\n",
    "\n",
    "# def load_csv(path: str = CSV_PATH) -> pd.DataFrame:\n",
    "#     p = Path(path)\n",
    "#     if not p.exists():\n",
    "#         return init_store()\n",
    "#     df = pd.read_csv(path)\n",
    "#     for col in [\"categories\",\"tags\",\"tldr\"]:\n",
    "#         if col in df.columns:\n",
    "#             df[col] = df[col].apply(lambda s: json.loads(s) if isinstance(s, str) and s.startswith(\"[\") else [])\n",
    "#     return df\n",
    "\n",
    "# def simple_search(df: pd.DataFrame, query: str, top_n: int = 50) -> pd.DataFrame:\n",
    "#     q = (query or \"\").lower().strip()\n",
    "#     if not q:\n",
    "#         return df.head(top_n)\n",
    "#     mask = (\n",
    "#         df[\"headline\"].fillna(\"\").str.lower().str.contains(q) |\n",
    "#         df[\"url\"].fillna(\"\").str.lower().str.contains(q) |\n",
    "#         df[\"domain\"].fillna(\"\").str.lower().str.contains(q) |\n",
    "#         df[\"content_text\"].fillna(\"\").str.lower().str.contains(q) |\n",
    "#         df[\"tags\"].astype(str).str.lower().str.contains(q) |\n",
    "#         df[\"categories\"].astype(str).str.lower().str.contains(q)\n",
    "#     )\n",
    "#     return df[mask].head(top_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5612d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store record incrementally in the CSV\n",
    "\n",
    "# =========================\n",
    "# URL canonicalization (for stable dedupe)\n",
    "# =========================\n",
    "from urllib.parse import urlparse, parse_qsl, urlencode, urlunparse\n",
    "\n",
    "_STRIP_PARAMS = {\n",
    "    \"utm_source\",\"utm_medium\",\"utm_campaign\",\"utm_term\",\"utm_content\",\n",
    "    \"utm_name\",\"utm_id\",\"gclid\",\"gclsrc\",\"fbclid\",\"mc_cid\",\"mc_eid\"\n",
    "}\n",
    "\n",
    "def canonicalize_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize URL for dedupe:\n",
    "    - lowercase scheme/host\n",
    "    - remove common tracking params\n",
    "    - sort remaining query params\n",
    "    - strip trailing slash (except root)\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    scheme = (parsed.scheme or \"https\").lower()\n",
    "    netloc = parsed.netloc.lower()\n",
    "    path = parsed.path or \"/\"\n",
    "    # normalize trailing slash\n",
    "    if path != \"/\" and path.endswith(\"/\"):\n",
    "        path = path[:-1]\n",
    "\n",
    "    # clean & sort query\n",
    "    q = []\n",
    "    for k, v in parse_qsl(parsed.query, keep_blank_values=True):\n",
    "        if k.lower() in _STRIP_PARAMS:\n",
    "            continue\n",
    "        q.append((k, v))\n",
    "    q.sort(key=lambda kv: kv[0].lower())\n",
    "    query = urlencode(q, doseq=True)\n",
    "\n",
    "    canon = urlunparse((scheme, netloc, path, \"\", query, \"\"))\n",
    "    return canon\n",
    "\n",
    "# =========================\n",
    "# DataFrame Store (CSV cache with dedupe by url_canonical)\n",
    "# =========================\n",
    "COLUMNS = [\n",
    "    \"fetched_at_utc\",\"url\",\"url_canonical\",\"domain\",\"headline\",\n",
    "    \"categories\",\"tags\",\"tldr\",\"content_text\",\"source_title\",\"author\",\"publish_date\"\n",
    "]\n",
    "\n",
    "def init_store() -> pd.DataFrame:\n",
    "    return pd.DataFrame(columns=COLUMNS)\n",
    "\n",
    "def _ensure_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # add any missing columns (for backward compatibility)\n",
    "    for col in COLUMNS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    # backfill url_canonical if missing/empty\n",
    "    if df[\"url_canonical\"].isna().any() or (df[\"url_canonical\"] == \"\").any():\n",
    "        df[\"url_canonical\"] = df.apply(\n",
    "            lambda r: canonicalize_url(r[\"url\"]) if pd.isna(r[\"url_canonical\"]) or r[\"url_canonical\"] == \"\" else r[\"url_canonical\"],\n",
    "            axis=1\n",
    "        )\n",
    "    return df[COLUMNS]\n",
    "\n",
    "def save_csv(df: pd.DataFrame, path: str = CSV_PATH) -> None:\n",
    "    df2 = df.copy()\n",
    "    # lists -> JSON strings for portability\n",
    "    for col in [\"categories\",\"tags\",\"tldr\"]:\n",
    "        if col in df2.columns:\n",
    "            df2[col] = df2[col].apply(lambda x: x if isinstance(x, str) else json.dumps(x, ensure_ascii=False))\n",
    "    df2.to_csv(path, index=False)\n",
    "\n",
    "def load_csv(path: str = CSV_PATH) -> pd.DataFrame:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        return init_store()\n",
    "    df = pd.read_csv(path)\n",
    "    # restore lists\n",
    "    for col in [\"categories\",\"tags\",\"tldr\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda s: json.loads(s) if isinstance(s, str) and s.startswith(\"[\") else ([] if pd.isna(s) else s))\n",
    "    df = _ensure_columns(df)\n",
    "    return df\n",
    "\n",
    "def append_record(df: pd.DataFrame, record: dict) -> pd.DataFrame:\n",
    "    row = {\n",
    "        \"fetched_at_utc\": record.get(\"fetched_at_utc\"),\n",
    "        \"url\": record.get(\"url\"),\n",
    "        \"url_canonical\": canonicalize_url(record.get(\"url\",\"\")),\n",
    "        \"domain\": record.get(\"domain\"),\n",
    "        \"headline\": record.get(\"headline\"),\n",
    "        \"categories\": record.get(\"categories\"),\n",
    "        \"tags\": record.get(\"tags\"),\n",
    "        \"tldr\": record.get(\"tldr\"),\n",
    "        \"content_text\": record.get(\"content_text\"),\n",
    "        \"source_title\": record.get(\"source_title\"),\n",
    "        \"author\": record.get(\"author\"),\n",
    "        \"publish_date\": record.get(\"publish_date\"),\n",
    "    }\n",
    "    return pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "def get_cached_row(df: pd.DataFrame, url: str) -> T.Optional[dict]:\n",
    "    canon = canonicalize_url(url)\n",
    "    hits = df[df[\"url_canonical\"] == canon]\n",
    "    if len(hits) == 0:\n",
    "        return None\n",
    "    rec = hits.iloc[0].to_dict()\n",
    "    # ensure list types for convenience\n",
    "    for col in [\"categories\",\"tags\",\"tldr\"]:\n",
    "        v = rec.get(col)\n",
    "        if isinstance(v, str):\n",
    "            try:\n",
    "                rec[col] = json.loads(v) if v.startswith(\"[\") else [v]\n",
    "            except Exception:\n",
    "                rec[col] = [v]\n",
    "    return rec\n",
    "\n",
    "# =========================\n",
    "# Orchestrator: ingest OR fetch from cache\n",
    "# =========================\n",
    "def ingest_or_fetch(\n",
    "    url: str,\n",
    "    taxonomy_path: str = TAXONOMY_PATH,\n",
    "    csv_path: str = CSV_PATH,\n",
    "    force_reingest: bool = False,\n",
    ") -> T.Tuple[dict, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    If canonical URL exists in CSV, return the cached row (and skip LLM).\n",
    "    Else run analyze_link_plus(), update taxonomy JSON, append 1 row to CSV, and return the new row.\n",
    "    Returns: (row_as_dict, updated_df)\n",
    "    \"\"\"\n",
    "    # 0) load CSV (cache) and taxonomy\n",
    "    df = load_csv(csv_path)\n",
    "    tax = load_taxonomy(taxonomy_path)\n",
    "    tax.setdefault(\"categories\", [])\n",
    "    tax.setdefault(\"tags\", [])\n",
    "\n",
    "    # 1) cached?\n",
    "    if not force_reingest:\n",
    "        cached = get_cached_row(df, url)\n",
    "        if cached is not None:\n",
    "            rec = {}\n",
    "            return rec, cached, df  # nothing else to do\n",
    "\n",
    "    # 2) not cached → analyze\n",
    "    rec = analyze_link_plus(\n",
    "        url,\n",
    "        allowed_categories=tax[\"categories\"],\n",
    "        allowed_tags=tax[\"tags\"]\n",
    "    )\n",
    "\n",
    "    # 3) update taxonomy (evolving lists)\n",
    "    tax[\"categories\"] = rec[\"_taxonomy\"][\"updated_categories\"]\n",
    "    tax[\"tags\"] = rec[\"_taxonomy\"][\"updated_tags\"]\n",
    "    save_taxonomy(tax[\"categories\"], tax[\"tags\"], taxonomy_path)\n",
    "\n",
    "    # 4) append to CSV cache\n",
    "    df = append_record(df, rec)\n",
    "    save_csv(df, csv_path)\n",
    "\n",
    "    # 5) return a compact dict consistent with CSV row formatting\n",
    "    row = {\n",
    "        \"fetched_at_utc\": rec[\"fetched_at_utc\"],\n",
    "        \"url\": rec[\"url\"],\n",
    "        \"url_canonical\": canonicalize_url(rec[\"url\"]),\n",
    "        \"domain\": rec[\"domain\"],\n",
    "        \"headline\": rec[\"headline\"],\n",
    "        \"categories\": rec[\"categories\"],\n",
    "        \"tags\": rec[\"tags\"],\n",
    "        \"tldr\": rec[\"tldr\"],\n",
    "        \"content_text\": rec[\"content_text\"],\n",
    "        \"source_title\": rec[\"source_title\"],\n",
    "        \"author\": rec[\"author\"],\n",
    "        \"publish_date\": rec[\"publish_date\"],\n",
    "    }\n",
    "    return rec, row, df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ae54f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fetched_at_utc</th>\n",
       "      <th>url</th>\n",
       "      <th>url_canonical</th>\n",
       "      <th>domain</th>\n",
       "      <th>headline</th>\n",
       "      <th>categories</th>\n",
       "      <th>tags</th>\n",
       "      <th>tldr</th>\n",
       "      <th>content_text</th>\n",
       "      <th>source_title</th>\n",
       "      <th>author</th>\n",
       "      <th>publish_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-13T15:05:07.059924Z</td>\n",
       "      <td>https://medium.com/data-science/from-data-scie...</td>\n",
       "      <td>https://medium.com/data-science/from-data-scie...</td>\n",
       "      <td>medium.com</td>\n",
       "      <td>From Data Scientist to ML / AI Product Manager</td>\n",
       "      <td>[AI/ML, Product, Career]</td>\n",
       "      <td>[ML Product Manager, AI Product Manager, Data ...</td>\n",
       "      <td>[Transitioning from Data Scientist to ML/AI Pr...</td>\n",
       "      <td>From Data Scientist to ML / AI Product Manager...</td>\n",
       "      <td>From Data Scientist to ML / AI Product Manager</td>\n",
       "      <td>Anna Via</td>\n",
       "      <td>2024-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-13T15:07:09.608280Z</td>\n",
       "      <td>https://www.confident-ai.com/blog/why-llm-as-a...</td>\n",
       "      <td>https://www.confident-ai.com/blog/why-llm-as-a...</td>\n",
       "      <td>www.confident-ai.com</td>\n",
       "      <td>LLM-as-a-Judge Simply Explained: The Complete ...</td>\n",
       "      <td>[AI/ML, Product, Startups]</td>\n",
       "      <td>[LLM, RAG, LangGraph, Agentic AI, Vector DB, P...</td>\n",
       "      <td>[LLM-as-a-Judge automates LLM evaluation, enha...</td>\n",
       "      <td>Your LLM app is streaming out tokens faster th...</td>\n",
       "      <td>LLM-as-a-Judge Simply Explained: The Complete ...</td>\n",
       "      <td>Jeffrey Ip</td>\n",
       "      <td>2025-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-13T16:13:01.025716Z</td>\n",
       "      <td>https://blog.langchain.com/the-rise-of-context...</td>\n",
       "      <td>https://blog.langchain.com/the-rise-of-context...</td>\n",
       "      <td>blog.langchain.com</td>\n",
       "      <td>The Rise of \"Context Engineering\"</td>\n",
       "      <td>[AI/ML, Product, Startups]</td>\n",
       "      <td>[LLM, Agentic AI, Prompting, Product Strategy,...</td>\n",
       "      <td>[Context engineering involves building dynamic...</td>\n",
       "      <td>Header image from Dex Horthy on Twitter.\\nCont...</td>\n",
       "      <td>The rise of \"context engineering\"</td>\n",
       "      <td>LangChain</td>\n",
       "      <td>2025-06-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-13T16:32:00.207193Z</td>\n",
       "      <td>https://openai.com/index/introducing-gpt-5/</td>\n",
       "      <td>https://openai.com/index/introducing-gpt-5</td>\n",
       "      <td>openai.com</td>\n",
       "      <td>Introducing GPT-5</td>\n",
       "      <td>[AI/ML, Product, Startups]</td>\n",
       "      <td>[LLM, Agentic AI, Vector DB, Prompting, ML Pro...</td>\n",
       "      <td>[GPT-5 is OpenAI's most advanced AI model, uni...</td>\n",
       "      <td>We are introducing GPT‑5, our best AI system y...</td>\n",
       "      <td>Introducing GPT-5</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>2025-08-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                fetched_at_utc  \\\n",
       "0  2025-09-13T15:05:07.059924Z   \n",
       "1  2025-09-13T15:07:09.608280Z   \n",
       "2  2025-09-13T16:13:01.025716Z   \n",
       "3  2025-09-13T16:32:00.207193Z   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://medium.com/data-science/from-data-scie...   \n",
       "1  https://www.confident-ai.com/blog/why-llm-as-a...   \n",
       "2  https://blog.langchain.com/the-rise-of-context...   \n",
       "3        https://openai.com/index/introducing-gpt-5/   \n",
       "\n",
       "                                       url_canonical                domain  \\\n",
       "0  https://medium.com/data-science/from-data-scie...            medium.com   \n",
       "1  https://www.confident-ai.com/blog/why-llm-as-a...  www.confident-ai.com   \n",
       "2  https://blog.langchain.com/the-rise-of-context...    blog.langchain.com   \n",
       "3         https://openai.com/index/introducing-gpt-5            openai.com   \n",
       "\n",
       "                                            headline  \\\n",
       "0     From Data Scientist to ML / AI Product Manager   \n",
       "1  LLM-as-a-Judge Simply Explained: The Complete ...   \n",
       "2                  The Rise of \"Context Engineering\"   \n",
       "3                                  Introducing GPT-5   \n",
       "\n",
       "                   categories  \\\n",
       "0    [AI/ML, Product, Career]   \n",
       "1  [AI/ML, Product, Startups]   \n",
       "2  [AI/ML, Product, Startups]   \n",
       "3  [AI/ML, Product, Startups]   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [ML Product Manager, AI Product Manager, Data ...   \n",
       "1  [LLM, RAG, LangGraph, Agentic AI, Vector DB, P...   \n",
       "2  [LLM, Agentic AI, Prompting, Product Strategy,...   \n",
       "3  [LLM, Agentic AI, Vector DB, Prompting, ML Pro...   \n",
       "\n",
       "                                                tldr  \\\n",
       "0  [Transitioning from Data Scientist to ML/AI Pr...   \n",
       "1  [LLM-as-a-Judge automates LLM evaluation, enha...   \n",
       "2  [Context engineering involves building dynamic...   \n",
       "3  [GPT-5 is OpenAI's most advanced AI model, uni...   \n",
       "\n",
       "                                        content_text  \\\n",
       "0  From Data Scientist to ML / AI Product Manager...   \n",
       "1  Your LLM app is streaming out tokens faster th...   \n",
       "2  Header image from Dex Horthy on Twitter.\\nCont...   \n",
       "3  We are introducing GPT‑5, our best AI system y...   \n",
       "\n",
       "                                        source_title      author publish_date  \n",
       "0     From Data Scientist to ML / AI Product Manager    Anna Via   2024-04-03  \n",
       "1  LLM-as-a-Judge Simply Explained: The Complete ...  Jeffrey Ip   2025-08-21  \n",
       "2                  The rise of \"context engineering\"   LangChain   2025-06-23  \n",
       "3                                  Introducing GPT-5      OpenAI   2025-08-07  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Quick demo — mimics your preferred usage\n",
    "# =========================\n",
    "# test_url = \"https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method\"\n",
    "test_url = \"https://blog.langchain.com/the-rise-of-context-engineering/\"\n",
    "\n",
    "rec, row, df = ingest_or_fetch(test_url)\n",
    "# `row` is the single record (dict); `df` is the full accumulated store\n",
    "# If you run again with the same URL, it will return from cache and skip LLM.\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d59a197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fetched_at_utc</th>\n",
       "      <th>url</th>\n",
       "      <th>url_canonical</th>\n",
       "      <th>domain</th>\n",
       "      <th>headline</th>\n",
       "      <th>categories</th>\n",
       "      <th>tags</th>\n",
       "      <th>tldr</th>\n",
       "      <th>content_text</th>\n",
       "      <th>source_title</th>\n",
       "      <th>author</th>\n",
       "      <th>publish_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-13T15:05:07.059924Z</td>\n",
       "      <td>https://medium.com/data-science/from-data-scie...</td>\n",
       "      <td>https://medium.com/data-science/from-data-scie...</td>\n",
       "      <td>medium.com</td>\n",
       "      <td>From Data Scientist to ML / AI Product Manager</td>\n",
       "      <td>[AI/ML, Product, Career]</td>\n",
       "      <td>[ML Product Manager, AI Product Manager, Data ...</td>\n",
       "      <td>[Transitioning from Data Scientist to ML/AI Pr...</td>\n",
       "      <td>From Data Scientist to ML / AI Product Manager...</td>\n",
       "      <td>From Data Scientist to ML / AI Product Manager</td>\n",
       "      <td>Anna Via</td>\n",
       "      <td>2024-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-13T15:07:09.608280Z</td>\n",
       "      <td>https://www.confident-ai.com/blog/why-llm-as-a...</td>\n",
       "      <td>https://www.confident-ai.com/blog/why-llm-as-a...</td>\n",
       "      <td>www.confident-ai.com</td>\n",
       "      <td>LLM-as-a-Judge Simply Explained: The Complete ...</td>\n",
       "      <td>[AI/ML, Product, Startups]</td>\n",
       "      <td>[LLM, RAG, LangGraph, Agentic AI, Vector DB, P...</td>\n",
       "      <td>[LLM-as-a-Judge automates LLM evaluation, enha...</td>\n",
       "      <td>Your LLM app is streaming out tokens faster th...</td>\n",
       "      <td>LLM-as-a-Judge Simply Explained: The Complete ...</td>\n",
       "      <td>Jeffrey Ip</td>\n",
       "      <td>2025-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-13T16:13:01.025716Z</td>\n",
       "      <td>https://blog.langchain.com/the-rise-of-context...</td>\n",
       "      <td>https://blog.langchain.com/the-rise-of-context...</td>\n",
       "      <td>blog.langchain.com</td>\n",
       "      <td>The Rise of \"Context Engineering\"</td>\n",
       "      <td>[AI/ML, Product, Startups]</td>\n",
       "      <td>[LLM, Agentic AI, Prompting, Product Strategy,...</td>\n",
       "      <td>[Context engineering involves building dynamic...</td>\n",
       "      <td>Header image from Dex Horthy on Twitter.\\nCont...</td>\n",
       "      <td>The rise of \"context engineering\"</td>\n",
       "      <td>LangChain</td>\n",
       "      <td>2025-06-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-13T16:32:00.207193Z</td>\n",
       "      <td>https://openai.com/index/introducing-gpt-5/</td>\n",
       "      <td>https://openai.com/index/introducing-gpt-5</td>\n",
       "      <td>openai.com</td>\n",
       "      <td>Introducing GPT-5</td>\n",
       "      <td>[AI/ML, Product, Startups]</td>\n",
       "      <td>[LLM, Agentic AI, Vector DB, Prompting, ML Pro...</td>\n",
       "      <td>[GPT-5 is OpenAI's most advanced AI model, uni...</td>\n",
       "      <td>We are introducing GPT‑5, our best AI system y...</td>\n",
       "      <td>Introducing GPT-5</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>2025-08-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                fetched_at_utc  \\\n",
       "0  2025-09-13T15:05:07.059924Z   \n",
       "1  2025-09-13T15:07:09.608280Z   \n",
       "2  2025-09-13T16:13:01.025716Z   \n",
       "3  2025-09-13T16:32:00.207193Z   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://medium.com/data-science/from-data-scie...   \n",
       "1  https://www.confident-ai.com/blog/why-llm-as-a...   \n",
       "2  https://blog.langchain.com/the-rise-of-context...   \n",
       "3        https://openai.com/index/introducing-gpt-5/   \n",
       "\n",
       "                                       url_canonical                domain  \\\n",
       "0  https://medium.com/data-science/from-data-scie...            medium.com   \n",
       "1  https://www.confident-ai.com/blog/why-llm-as-a...  www.confident-ai.com   \n",
       "2  https://blog.langchain.com/the-rise-of-context...    blog.langchain.com   \n",
       "3         https://openai.com/index/introducing-gpt-5            openai.com   \n",
       "\n",
       "                                            headline  \\\n",
       "0     From Data Scientist to ML / AI Product Manager   \n",
       "1  LLM-as-a-Judge Simply Explained: The Complete ...   \n",
       "2                  The Rise of \"Context Engineering\"   \n",
       "3                                  Introducing GPT-5   \n",
       "\n",
       "                   categories  \\\n",
       "0    [AI/ML, Product, Career]   \n",
       "1  [AI/ML, Product, Startups]   \n",
       "2  [AI/ML, Product, Startups]   \n",
       "3  [AI/ML, Product, Startups]   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [ML Product Manager, AI Product Manager, Data ...   \n",
       "1  [LLM, RAG, LangGraph, Agentic AI, Vector DB, P...   \n",
       "2  [LLM, Agentic AI, Prompting, Product Strategy,...   \n",
       "3  [LLM, Agentic AI, Vector DB, Prompting, ML Pro...   \n",
       "\n",
       "                                                tldr  \\\n",
       "0  [Transitioning from Data Scientist to ML/AI Pr...   \n",
       "1  [LLM-as-a-Judge automates LLM evaluation, enha...   \n",
       "2  [Context engineering involves building dynamic...   \n",
       "3  [GPT-5 is OpenAI's most advanced AI model, uni...   \n",
       "\n",
       "                                        content_text  \\\n",
       "0  From Data Scientist to ML / AI Product Manager...   \n",
       "1  Your LLM app is streaming out tokens faster th...   \n",
       "2  Header image from Dex Horthy on Twitter.\\nCont...   \n",
       "3  We are introducing GPT‑5, our best AI system y...   \n",
       "\n",
       "                                        source_title      author publish_date  \n",
       "0     From Data Scientist to ML / AI Product Manager    Anna Via   2024-04-03  \n",
       "1  LLM-as-a-Judge Simply Explained: The Complete ...  Jeffrey Ip   2025-08-21  \n",
       "2                  The rise of \"context engineering\"   LangChain   2025-06-23  \n",
       "3                                  Introducing GPT-5      OpenAI   2025-08-07  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c03e042",
   "metadata": {},
   "source": [
    "## Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "396340da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# MVP Flashcards (Single CSV + Swipe Queue)\n",
    "# =========================\n",
    "\n",
    "import os, re, json, hashlib, datetime, typing as T\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---- OpenAI client ----\n",
    "load_dotenv()\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "CARDS_CSV_PATH = os.getenv(\"CARDS_CSV_PATH\", \"cards_store.csv\")\n",
    "MODEL_FOR_CARDS = os.getenv(\"MODEL_FOR_CARDS\", \"gpt-4.1-mini\")  # lightweight, cheap\n",
    "MAX_TEXT_CHARS = int(os.getenv(\"MAX_TEXT_CHARS\", \"8000\"))       # cap article text sent to LLM\n",
    "\n",
    "# =========================\n",
    "# CSV schema\n",
    "# =========================\n",
    "CARD_COLUMNS = [\n",
    "    \"card_id\",         # pk (sha1 of url_canonical + \"\\n\" + normalized_question)\n",
    "    \"url_canonical\",   # article key\n",
    "    \"question\",\n",
    "    \"answer\",\n",
    "    \"learned\",         # bool\n",
    "    \"created_at_utc\"   # ISO str\n",
    "]\n",
    "\n",
    "def _ensure_cards_csv(path: str = CARDS_CSV_PATH) -> pd.DataFrame:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        df = pd.DataFrame(columns=CARD_COLUMNS)\n",
    "        df.to_csv(path, index=False)\n",
    "        return df\n",
    "    df = pd.read_csv(path)\n",
    "    # Backward/robust handling\n",
    "    for col in CARD_COLUMNS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    # Coerce learned -> bool\n",
    "    if \"learned\" in df.columns:\n",
    "        df[\"learned\"] = df[\"learned\"].apply(lambda x: bool(x) if isinstance(x, (bool,)) else (str(x).lower() == \"true\"))\n",
    "    return df[CARD_COLUMNS]\n",
    "\n",
    "def _save_cards_df(df: pd.DataFrame, path: str = CARDS_CSV_PATH) -> None:\n",
    "    df2 = df.copy()\n",
    "    df2.to_csv(path, index=False)\n",
    "\n",
    "# =========================\n",
    "# Utilities\n",
    "# =========================\n",
    "def _normalize_question(q: str) -> str:\n",
    "    q = (q or \"\").strip()\n",
    "    # collapse spaces, strip trailing ?\n",
    "    q = re.sub(r\"\\s+\", \" \", q)\n",
    "    return q\n",
    "\n",
    "def _card_id(url_canonical: str, question: str) -> str:\n",
    "    base = f\"{url_canonical}\\n{_normalize_question(question)}\"\n",
    "    return hashlib.sha1(base.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "def _utc_now_iso() -> str:\n",
    "    return datetime.datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "def _safe_json_loads(text: str) -> dict:\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}\", text, re.S)\n",
    "        return json.loads(m.group(0)) if m else {\"error\": \"Non-JSON\", \"raw\": text[:1200]}\n",
    "\n",
    "# =========================\n",
    "# LLM: generate Q&A from content_text\n",
    "# =========================\n",
    "_SYSTEM_PROMPT = (\n",
    "    \"You generate concise, factual Q&A flashcards from text. Avoid trivia and ambiguity. \"\n",
    "    \"Prefer concrete facts, key definitions, mechanisms, comparisons, and takeaways. \"\n",
    "    \"Keep questions ≤ 140 characters, answers ≤ 240 characters.\"\n",
    ")\n",
    "\n",
    "_USER_TEMPLATE = \"\"\"From the text below, produce 3–6 Q&A flashcards that capture the core facts/concepts.\n",
    "Return STRICT JSON only:\n",
    "{\"cards\":[{\"q\":\"...\",\"a\":\"...\"}]}\n",
    "\n",
    "Text (may be truncated):\n",
    "{TEXT}\n",
    "\"\"\"\n",
    "\n",
    "def _call_llm_for_cards(content_text: str, model: str = MODEL_FOR_CARDS) -> T.List[T.Dict[str, str]]:\n",
    "    # Use Responses API if available\n",
    "    payload = _USER_TEMPLATE.replace(\"{TEXT}\", (content_text or \"\")[:MAX_TEXT_CHARS])\n",
    "    if hasattr(client, \"responses\"):\n",
    "        resp = client.responses.create(\n",
    "            model=model,\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": _SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": payload}\n",
    "            ],\n",
    "        )\n",
    "        out = getattr(resp, \"output_text\", getattr(resp, \"output\", \"\"))\n",
    "        data = _safe_json_loads(out)\n",
    "    else:\n",
    "        # very old fallback\n",
    "        cc = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"system\", \"content\": _SYSTEM_PROMPT},\n",
    "                      {\"role\": \"user\", \"content\": payload}]\n",
    "        )\n",
    "        data = _safe_json_loads(cc.choices[0].message.content)\n",
    "\n",
    "    cards = data.get(\"cards\", [])\n",
    "    # lightweight validation & cleaning\n",
    "    clean = []\n",
    "    seen_q = set()\n",
    "    for c in cards:\n",
    "        q = _normalize_question(str(c.get(\"q\", \"\")).strip())\n",
    "        a = str(c.get(\"a\", \"\")).strip()\n",
    "        if not q or not a:\n",
    "            continue\n",
    "        if len(q) > 200 or len(a) > 500:\n",
    "            # discard egregiously long pairs (model might go verbose)\n",
    "            continue\n",
    "        if q.lower() in seen_q:\n",
    "            continue\n",
    "        seen_q.add(q.lower())\n",
    "        clean.append({\"q\": q, \"a\": a})\n",
    "    return clean[:6]  # cap to 6\n",
    "\n",
    "# =========================\n",
    "# Public API\n",
    "# =========================\n",
    "# def generate_cards_for_url(\n",
    "#     url_canonical: str,\n",
    "#     n: int = 5,\n",
    "#     *,\n",
    "#     regenerate: bool = False,\n",
    "#     return_scope: str = \"all\",  # \"all\" or \"url\"\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Idempotent by default.\n",
    "#     - If cards for `url_canonical` already exist and `regenerate=False`, skip LLM and just return existing.\n",
    "#     - If no cards exist yet (new URL), call LLM and upsert.\n",
    "#     - Never flips learned=True back to False.\n",
    "#     - Returns either the full cards DataFrame (\"all\") or only rows for this URL (\"url\").\n",
    "#     \"\"\"\n",
    "#     if not url_canonical or not isinstance(url_canonical, str):\n",
    "#         raise ValueError(\"url_canonical must be a non-empty string.\")\n",
    "\n",
    "#     df = _ensure_cards_csv(CARDS_CSV_PATH)\n",
    "\n",
    "#     # Fast path: URL already present -> skip generation unless forced\n",
    "#     if not regenerate and (df[\"url_canonical\"] == url_canonical).any():\n",
    "#         return df if return_scope == \"all\" else df[df[\"url_canonical\"] == url_canonical].copy()\n",
    "\n",
    "#     # Otherwise, try to generate\n",
    "#     rec, row, df_full = ingest_or_fetch(url_canonical)\n",
    "#     pairs = _call_llm_for_cards(row[\"content_text\"])\n",
    "#     if not pairs:\n",
    "#         # Nothing generated; just return whatever we have already\n",
    "#         return df if return_scope == \"all\" else df[df[\"url_canonical\"] == url_canonical].copy()\n",
    "\n",
    "#     existing_ids = set(df[\"card_id\"].astype(str).tolist())\n",
    "#     rows_to_add = []\n",
    "#     now_iso = _utc_now_iso()\n",
    "#     count_added = 0\n",
    "\n",
    "#     for pair in pairs:\n",
    "#         if count_added >= max(1, n):  # ensure at least 1 if any generated\n",
    "#             break\n",
    "#         q, a = pair[\"q\"], pair[\"a\"]\n",
    "#         cid = _card_id(url_canonical, q)\n",
    "#         if cid in existing_ids:\n",
    "#             continue\n",
    "#         rows_to_add.append({\n",
    "#             \"card_id\": cid,\n",
    "#             \"url_canonical\": url_canonical,\n",
    "#             \"question\": q,\n",
    "#             \"answer\": a,\n",
    "#             \"learned\": False,\n",
    "#             \"created_at_utc\": now_iso\n",
    "#         })\n",
    "#         count_added += 1\n",
    "\n",
    "#     if rows_to_add:\n",
    "#         df = pd.concat([df, pd.DataFrame(rows_to_add)], ignore_index=True)\n",
    "#         _save_cards_df(df, CARDS_CSV_PATH)\n",
    "\n",
    "#     return df if return_scope == \"all\" else df[df[\"url_canonical\"] == url_canonical].copy()\n",
    "\n",
    "def generate_cards_for_url(\n",
    "    url_canonical: str,\n",
    "    n: int = 5,\n",
    "    *,\n",
    "    regenerate: bool = False,\n",
    "    return_scope: str = \"all\",      # \"all\" or \"url\"\n",
    "    reset_learn: bool = False,\n",
    "    reset_learn_scope: str = \"all\", # \"url\" or \"all\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Idempotent by default.\n",
    "    - Optional resets before generation:\n",
    "        * reset_learn=True & reset_learn_scope=\"url\" -> set learned=False for this URL only\n",
    "        * reset_learn=True & reset_learn_scope=\"all\" -> set learned=False for ALL rows\n",
    "    - If cards for `url_canonical` already exist and `regenerate=False`, skip LLM and just return existing.\n",
    "    - If no cards exist yet (new URL), call LLM and upsert.\n",
    "    - Returns either the full cards DataFrame (\"all\") or only rows for this URL (\"url\").\n",
    "    \"\"\"\n",
    "    if not url_canonical or not isinstance(url_canonical, str):\n",
    "        raise ValueError(\"url_canonical must be a non-empty string.\")\n",
    "    if return_scope not in (\"all\", \"url\"):\n",
    "        raise ValueError(\"return_scope must be 'all' or 'url'.\")\n",
    "    if reset_learn_scope not in (\"url\", \"all\"):\n",
    "        raise ValueError(\"reset_learn_scope must be 'url' or 'all'.\")\n",
    "\n",
    "    # Optional reset(s)\n",
    "    if reset_learn:\n",
    "        reset_learned(url_canonical, reset_learn_scope)\n",
    "\n",
    "    df = _ensure_cards_csv(CARDS_CSV_PATH)\n",
    "\n",
    "    # Fast path: URL already present -> skip generation unless forced\n",
    "    if not regenerate and (df[\"url_canonical\"] == url_canonical).any():\n",
    "        return df if return_scope == \"all\" else df[df[\"url_canonical\"] == url_canonical].copy()\n",
    "\n",
    "    # Otherwise, try to generate\n",
    "    rec, row, df_full = ingest_or_fetch(url_canonical)\n",
    "    pairs = _call_llm_for_cards(row[\"content_text\"])\n",
    "    if not pairs:\n",
    "        # Nothing generated; just return whatever we have already\n",
    "        df = _ensure_cards_csv(CARDS_CSV_PATH)\n",
    "        return df if return_scope == \"all\" else df[df[\"url_canonical\"] == url_canonical].copy()\n",
    "\n",
    "    existing_ids = set(df[\"card_id\"].astype(str).tolist())\n",
    "    rows_to_add = []\n",
    "    now_iso = _utc_now_iso()\n",
    "    count_added = 0\n",
    "\n",
    "    for pair in pairs:\n",
    "        if count_added >= max(1, n):  # ensure at least 1 if any generated\n",
    "            break\n",
    "        q, a = pair[\"q\"], pair[\"a\"]\n",
    "        cid = _card_id(url_canonical, q)\n",
    "        if cid in existing_ids:\n",
    "            continue\n",
    "        rows_to_add.append({\n",
    "            \"card_id\": cid,\n",
    "            \"url_canonical\": url_canonical,\n",
    "            \"question\": q,\n",
    "            \"answer\": a,\n",
    "            \"learned\": False,\n",
    "            \"created_at_utc\": now_iso\n",
    "        })\n",
    "        count_added += 1\n",
    "\n",
    "    if rows_to_add:\n",
    "        df = pd.concat([df, pd.DataFrame(rows_to_add)], ignore_index=True)\n",
    "        _save_cards_df(df, CARDS_CSV_PATH)\n",
    "\n",
    "    df = _ensure_cards_csv(CARDS_CSV_PATH)\n",
    "    return df if return_scope == \"all\" else df[df[\"url_canonical\"] == url_canonical].copy()\n",
    "\n",
    "\n",
    "def load_unlearned_cards(url_canonical: str) -> pd.DataFrame:\n",
    "    df = _ensure_cards_csv(CARDS_CSV_PATH)\n",
    "    mask = (df[\"url_canonical\"] == url_canonical) & (~df[\"learned\"])\n",
    "    return df[mask].copy()\n",
    "\n",
    "def start_session(url_canonical: str, shuffle: bool = True) -> deque:\n",
    "    \"\"\"\n",
    "    Returns a deque of dicts: [{\"card_id\",\"question\",\"answer\"}, ...] for unlearned cards.\n",
    "    \"\"\"\n",
    "    df = load_unlearned_cards(url_canonical)\n",
    "    print(f\"Found {len(df)} unlearned cards for {url_canonical}\")\n",
    "    cards = df[[\"card_id\",\"question\",\"answer\"]].to_dict(orient=\"records\")\n",
    "    if shuffle and len(cards) > 1:\n",
    "        import random\n",
    "        random.shuffle(cards)\n",
    "    return deque(cards)\n",
    "\n",
    "def swipe_left(queue: deque) -> None:\n",
    "    \"\"\"\n",
    "    \"Review again\": move the current card to the end of the queue.\n",
    "    No persistence change.\n",
    "    \"\"\"\n",
    "    if not queue:\n",
    "        return\n",
    "    queue.append(queue.popleft())\n",
    "\n",
    "def swipe_right(queue: deque, card_id: str) -> None:\n",
    "    \"\"\"\n",
    "    \"I know it\": mark learned=True in CSV, then remove from the queue.\n",
    "    \"\"\"\n",
    "    if not queue:\n",
    "        return\n",
    "    current = queue[0]\n",
    "    if current[\"card_id\"] != card_id:\n",
    "        # guard: if caller passed mismatched card_id, align to current\n",
    "        card_id = current[\"card_id\"]\n",
    "\n",
    "    # persist learned=True\n",
    "    df = _ensure_cards_csv(CARDS_CSV_PATH)\n",
    "    idx = df.index[df[\"card_id\"] == card_id]\n",
    "    if len(idx) > 0:\n",
    "        df.loc[idx, \"learned\"] = True\n",
    "        _save_cards_df(df, CARDS_CSV_PATH)\n",
    "\n",
    "    # remove from queue\n",
    "    queue.popleft()\n",
    "\n",
    "def count_unlearned(url_canonical: str) -> int:\n",
    "    return len(load_unlearned_cards(url_canonical))\n",
    "\n",
    "# def reset_learned(url_canonical: str) -> int:\n",
    "#     \"\"\"\n",
    "#     Set learned=False for all cards of an article.\n",
    "#     Returns the number of cards reset.\n",
    "#     \"\"\"\n",
    "#     df = _ensure_cards_csv(CARDS_CSV_PATH)\n",
    "#     mask = (df[\"url_canonical\"] == url_canonical)\n",
    "#     n = int(mask.sum())\n",
    "#     if n > 0:\n",
    "#         df.loc[mask, \"learned\"] = False\n",
    "#         _save_cards_df(df, CARDS_CSV_PATH)\n",
    "#     return n\n",
    "\n",
    "def reset_learned(url_canonical: str, reset_learn_scope: str = \"url\") -> int:\n",
    "    \"\"\"\n",
    "    Set learned=False based on scope.\n",
    "      - reset_learn_scope == \"url\": only rows for the given url_canonical\n",
    "      - reset_learn_scope == \"all\": all rows in the CSV\n",
    "\n",
    "    Returns the number of rows reset.\n",
    "    \"\"\"\n",
    "    if reset_learn_scope not in (\"url\", \"all\"):\n",
    "        raise ValueError(\"reset_learn_scope must be 'url' or 'all'.\")\n",
    "\n",
    "    df = _ensure_cards_csv(CARDS_CSV_PATH)\n",
    "    if df.empty:\n",
    "        return 0\n",
    "\n",
    "    if reset_learn_scope == \"all\":\n",
    "        n = len(df)\n",
    "        if n > 0:\n",
    "            df[\"learned\"] = False\n",
    "            _save_cards_df(df, CARDS_CSV_PATH)\n",
    "        return n\n",
    "\n",
    "    # scope == \"url\"\n",
    "    mask = (df[\"url_canonical\"] == url_canonical)\n",
    "    n = int(mask.sum())\n",
    "    if n > 0:\n",
    "        df.loc[mask, \"learned\"] = False\n",
    "        _save_cards_df(df, CARDS_CSV_PATH)\n",
    "    return n\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Convenience helpers\n",
    "# =========================\n",
    "def url_exists_in_cards(url_canonical: str, path: str = CARDS_CSV_PATH) -> bool:\n",
    "    df = _ensure_cards_csv(path)\n",
    "    return bool((df[\"url_canonical\"] == url_canonical).any())\n",
    "\n",
    "def get_cards_for_url(url_canonical: str, path: str = CARDS_CSV_PATH) -> pd.DataFrame:\n",
    "    df = _ensure_cards_csv(path)\n",
    "    return df[df[\"url_canonical\"] == url_canonical].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "05012c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>url_canonical</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>learned</th>\n",
       "      <th>created_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422780d48fc32402</td>\n",
       "      <td>https://www.confident-ai.com/blog/why-llm-as-a...</td>\n",
       "      <td>What is 'LLM-as-a-Judge' in evaluating LLM out...</td>\n",
       "      <td>LLM-as-a-Judge uses language models to automat...</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-09-13T16:33:34.033902Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7a64dabeca7e0612</td>\n",
       "      <td>https://www.confident-ai.com/blog/why-llm-as-a...</td>\n",
       "      <td>What are the two main types of LLM-as-a-Judge?</td>\n",
       "      <td>Single-output judges score individual model ou...</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-09-13T16:33:34.033902Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ebd4511c91826032</td>\n",
       "      <td>https://blog.langchain.com/the-rise-of-context...</td>\n",
       "      <td>What is context engineering in LLM applications?</td>\n",
       "      <td>Context engineering is building dynamic system...</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-09-13T16:33:38.807195Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3486f2905c78aee0</td>\n",
       "      <td>https://blog.langchain.com/the-rise-of-context...</td>\n",
       "      <td>Why do many LLM agentic systems fail?</td>\n",
       "      <td>Failures often arise because the model is not ...</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-09-13T16:33:38.807195Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            card_id                                      url_canonical  \\\n",
       "0  422780d48fc32402  https://www.confident-ai.com/blog/why-llm-as-a...   \n",
       "1  7a64dabeca7e0612  https://www.confident-ai.com/blog/why-llm-as-a...   \n",
       "2  ebd4511c91826032  https://blog.langchain.com/the-rise-of-context...   \n",
       "3  3486f2905c78aee0  https://blog.langchain.com/the-rise-of-context...   \n",
       "\n",
       "                                            question  \\\n",
       "0  What is 'LLM-as-a-Judge' in evaluating LLM out...   \n",
       "1     What are the two main types of LLM-as-a-Judge?   \n",
       "2   What is context engineering in LLM applications?   \n",
       "3              Why do many LLM agentic systems fail?   \n",
       "\n",
       "                                              answer  learned  \\\n",
       "0  LLM-as-a-Judge uses language models to automat...    False   \n",
       "1  Single-output judges score individual model ou...     True   \n",
       "2  Context engineering is building dynamic system...    False   \n",
       "3  Failures often arise because the model is not ...     True   \n",
       "\n",
       "                created_at_utc  \n",
       "0  2025-09-13T16:33:34.033902Z  \n",
       "1  2025-09-13T16:33:34.033902Z  \n",
       "2  2025-09-13T16:33:38.807195Z  \n",
       "3  2025-09-13T16:33:38.807195Z  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_canonical = \"https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method\"\n",
    "df_cards = generate_cards_for_url(url_canonical, n=2)\n",
    "df_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fa73652c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>url_canonical</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>learned</th>\n",
       "      <th>created_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422780d48fc32402</td>\n",
       "      <td>https://www.confident-ai.com/blog/why-llm-as-a...</td>\n",
       "      <td>What is 'LLM-as-a-Judge' in evaluating LLM out...</td>\n",
       "      <td>LLM-as-a-Judge uses language models to automat...</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-09-13T16:33:34.033902Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7a64dabeca7e0612</td>\n",
       "      <td>https://www.confident-ai.com/blog/why-llm-as-a...</td>\n",
       "      <td>What are the two main types of LLM-as-a-Judge?</td>\n",
       "      <td>Single-output judges score individual model ou...</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-09-13T16:33:34.033902Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ebd4511c91826032</td>\n",
       "      <td>https://blog.langchain.com/the-rise-of-context...</td>\n",
       "      <td>What is context engineering in LLM applications?</td>\n",
       "      <td>Context engineering is building dynamic system...</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-09-13T16:33:38.807195Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3486f2905c78aee0</td>\n",
       "      <td>https://blog.langchain.com/the-rise-of-context...</td>\n",
       "      <td>Why do many LLM agentic systems fail?</td>\n",
       "      <td>Failures often arise because the model is not ...</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-09-13T16:33:38.807195Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            card_id                                      url_canonical  \\\n",
       "0  422780d48fc32402  https://www.confident-ai.com/blog/why-llm-as-a...   \n",
       "1  7a64dabeca7e0612  https://www.confident-ai.com/blog/why-llm-as-a...   \n",
       "2  ebd4511c91826032  https://blog.langchain.com/the-rise-of-context...   \n",
       "3  3486f2905c78aee0  https://blog.langchain.com/the-rise-of-context...   \n",
       "\n",
       "                                            question  \\\n",
       "0  What is 'LLM-as-a-Judge' in evaluating LLM out...   \n",
       "1     What are the two main types of LLM-as-a-Judge?   \n",
       "2   What is context engineering in LLM applications?   \n",
       "3              Why do many LLM agentic systems fail?   \n",
       "\n",
       "                                              answer  learned  \\\n",
       "0  LLM-as-a-Judge uses language models to automat...    False   \n",
       "1  Single-output judges score individual model ou...    False   \n",
       "2  Context engineering is building dynamic system...    False   \n",
       "3  Failures often arise because the model is not ...    False   \n",
       "\n",
       "                created_at_utc  \n",
       "0  2025-09-13T16:33:34.033902Z  \n",
       "1  2025-09-13T16:33:34.033902Z  \n",
       "2  2025-09-13T16:33:38.807195Z  \n",
       "3  2025-09-13T16:33:38.807195Z  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_canonical = \"https://blog.langchain.com/the-rise-of-context-engineering/\"\n",
    "df_cards = generate_cards_for_url(url_canonical, n=2, reset_learn=True)\n",
    "df_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d4765da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_unlearned_cards() -> pd.DataFrame:\n",
    "    df = _ensure_cards_csv(CARDS_CSV_PATH)\n",
    "    if df.empty:\n",
    "        return df.copy()\n",
    "    return df[~df[\"learned\"]].copy()\n",
    "\n",
    "from ipywidgets import Button, HBox, VBox, HTML\n",
    "from IPython.display import display\n",
    "from collections import deque\n",
    "\n",
    "def run_widget_session(url_canonical, shuffle=True, learn_all: bool = False):\n",
    "    \"\"\"\n",
    "    Round-based review:\n",
    "      - 'Review again' defers the card to the next round (again_q).\n",
    "      - Deferred cards won't reappear until the next round.\n",
    "      - If learn_all=True, review unlearned cards across ALL URLs; else only for url_canonical.\n",
    "    \"\"\"\n",
    "    # Build initial queue from unlearned cards\n",
    "    if learn_all:\n",
    "        df = load_all_unlearned_cards()\n",
    "        scope_label = \"all URLs\"\n",
    "    else:\n",
    "        df = load_unlearned_cards(url_canonical)\n",
    "        scope_label = f\"{url_canonical}\"\n",
    "\n",
    "    print(f\"Found {len(df)} unlearned cards for {scope_label}\")\n",
    "    cards = df[[\"card_id\", \"question\", \"answer\"]].to_dict(orient=\"records\")\n",
    "    if shuffle and len(cards) > 1:\n",
    "        import random\n",
    "        random.shuffle(cards)\n",
    "\n",
    "    # Two-queue scheduler\n",
    "    main_q = deque(cards)   # current round\n",
    "    again_q = deque()       # next round\n",
    "    round_idx = 1\n",
    "    total_start = len(cards)\n",
    "\n",
    "    # UI elements\n",
    "    title = HTML(\"<h3>Flashcard Session</h3>\")\n",
    "    subtitle = HTML(\"\")\n",
    "    q_html = HTML(\"\")\n",
    "    a_html = HTML(\"\")\n",
    "    btn_reveal = Button(description=\"Reveal answer\")\n",
    "    btn_know   = Button(description=\"I know it 👍\", button_style=\"success\")\n",
    "    btn_review = Button(description=\"Review again 🔁\", button_style=\"warning\")\n",
    "\n",
    "    container = VBox([title, subtitle, q_html, a_html, HBox([btn_reveal, btn_know, btn_review])])\n",
    "\n",
    "    def update_subtitle():\n",
    "        subtitle.value = (\n",
    "            f\"<div style='color:#666'>Scope: {scope_label} • Round {round_idx} • \"\n",
    "            f\"{len(main_q)} left in this round\"\n",
    "            f\"{' • ' + str(len(again_q)) + ' deferred' if len(again_q) else ''}\"\n",
    "            f\"{f' • {total_start} total' if total_start else ''}\"\n",
    "            f\"</div>\"\n",
    "        )\n",
    "\n",
    "    def rollover_if_needed():\n",
    "        nonlocal main_q, again_q, round_idx\n",
    "        if not main_q and again_q:\n",
    "            main_q = again_q\n",
    "            again_q = deque()\n",
    "            round_idx += 1\n",
    "\n",
    "    def disable_all():\n",
    "        btn_reveal.disabled = True\n",
    "        btn_know.disabled = True\n",
    "        btn_review.disabled = True\n",
    "\n",
    "    def render():\n",
    "        rollover_if_needed()\n",
    "        update_subtitle()\n",
    "\n",
    "        if not main_q:\n",
    "            q_html.value = \"<b>All cards learned or deferred round(s) completed. 🎉</b>\"\n",
    "            a_html.value = \"\"\n",
    "            disable_all()\n",
    "            return\n",
    "\n",
    "        card = main_q[0]\n",
    "        q_html.value = f\"<b>Question</b><div style='margin-top:4px'>{card['question']}</div>\"\n",
    "        a_html.value = \"<i>(click 'Reveal answer')</i>\"\n",
    "\n",
    "    def on_reveal(_):\n",
    "        if not main_q: return\n",
    "        card = main_q[0]\n",
    "        a_html.value = f\"<b>Answer</b><div style='margin-top:6px'>{card['answer']}</div>\"\n",
    "\n",
    "    def on_know(_):\n",
    "        # Persist learned=True and remove from current round\n",
    "        if not main_q: return\n",
    "        card = main_q[0]\n",
    "        swipe_right(main_q, card[\"card_id\"])  # persists and pops left from main_q\n",
    "        render()\n",
    "\n",
    "    def on_review(_):\n",
    "        if not main_q: return\n",
    "        # Defer current card to next round: move from main_q -> again_q\n",
    "        card = main_q.popleft()\n",
    "        again_q.append(card)\n",
    "        render()\n",
    "\n",
    "    btn_reveal.on_click(on_reveal)\n",
    "    btn_know.on_click(on_know)\n",
    "    btn_review.on_click(on_review)\n",
    "\n",
    "    display(container)\n",
    "    render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4d3b02f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_widget_session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_widget_session\u001b[49m(url_canonical, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, learn_all=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'run_widget_session' is not defined"
     ]
    }
   ],
   "source": [
    "run_widget_session(url_canonical, shuffle=True, learn_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60172026",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # === Button-based swipe UI (requires ipywidgets) ===\n",
    "# # If needed: pip install ipywidgets && jupyter nbextension enable --py widgetsnbextension\n",
    "# from ipywidgets import Button, HBox, VBox, HTML, Output\n",
    "# from IPython.display import display, clear_output\n",
    "\n",
    "# def run_widget_session(url_canonical, shuffle=True):\n",
    "#     queue = start_session(url_canonical, shuffle=shuffle)\n",
    "#     out = Output()\n",
    "\n",
    "#     if not queue:\n",
    "#         display(HTML(\"<b>No unlearned cards for this article. 🎉</b>\"))\n",
    "#         return\n",
    "\n",
    "#     # widgets\n",
    "#     q_html = HTML(\"<h4>Question</h4>\")\n",
    "#     a_html = HTML(\"<i>(click 'Reveal answer')</i>\")\n",
    "#     a_shown = {\"val\": False}\n",
    "\n",
    "#     btn_reveal = Button(description=\"Reveal answer\", button_style=\"\")\n",
    "#     btn_know   = Button(description=\"I know it 👍\", button_style=\"success\")\n",
    "#     btn_review = Button(description=\"Review again 🔁\", button_style=\"warning\")\n",
    "\n",
    "#     # helpers\n",
    "#     def render_current():\n",
    "#         if not queue:\n",
    "#             with out:\n",
    "#                 clear_output()\n",
    "#                 display(HTML(\"<b>All cards learned for this article. 🎉</b>\"))\n",
    "#             # disable buttons\n",
    "#             btn_reveal.disabled = True\n",
    "#             btn_know.disabled   = True\n",
    "#             btn_review.disabled = True\n",
    "#             return\n",
    "\n",
    "#         card = queue[0]\n",
    "#         q_html.value = f\"<h4>Question</h4><div>{card['question']}</div>\"\n",
    "#         a_html.value = \"<i>(click 'Reveal answer')</i>\"\n",
    "#         a_shown[\"val\"] = False\n",
    "\n",
    "#     def on_reveal(_):\n",
    "#         if not queue:\n",
    "#             return\n",
    "#         card = queue[0]\n",
    "#         a_html.value = f\"<b>Answer</b><div style='margin-top:6px;'>{card['answer']}</div>\"\n",
    "#         a_shown[\"val\"] = True\n",
    "\n",
    "#     def on_know(_):\n",
    "#         if not queue:\n",
    "#             return\n",
    "#         card = queue[0]\n",
    "#         swipe_right(queue, card[\"card_id\"])   # persist learned=True and pop from queue\n",
    "#         render_current()\n",
    "\n",
    "#     def on_review(_):\n",
    "#         if not queue:\n",
    "#             return\n",
    "#         swipe_left(queue)                     # move current to end\n",
    "#         render_current()\n",
    "\n",
    "#     btn_reveal.on_click(on_reveal)\n",
    "#     btn_know.on_click(on_know)\n",
    "#     btn_review.on_click(on_review)\n",
    "\n",
    "#     with out:\n",
    "#         clear_output()\n",
    "#         render_current()\n",
    "#         display(VBox([\n",
    "#             q_html,\n",
    "#             a_html,\n",
    "#             HBox([btn_reveal, btn_know, btn_review])\n",
    "#         ]))\n",
    "\n",
    "#     display(out)\n",
    "\n",
    "# # usage:\n",
    "# # run_widget_session(url_canonical, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d56abe2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 unlearned cards for https://blog.langchain.com/the-rise-of-context-engineering/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f111ded65dc14376856a7ff1fca43e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_widget_session(url_canonical, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed3b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f371bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CLI-style swipe loop (works in any notebook) ===\n",
    "def run_text_session(url_canonical):\n",
    "    q = start_session(url_canonical, shuffle=True)\n",
    "    if not q:\n",
    "        print(\"No unlearned cards for this article. 🎉\")\n",
    "        return\n",
    "\n",
    "    print(f\"Session loaded with {len(q)} cards. Type:\\n\"\n",
    "          \"  s = show answer, r = I know it (remove), l = review again (send to end), q = quit\\n\")\n",
    "\n",
    "    while q:\n",
    "        card = q[0]\n",
    "        print(\"\\nQ:\", card[\"question\"])\n",
    "        cmd = input(\"Command [s/r/l/q]: \").strip().lower()\n",
    "\n",
    "        if cmd == \"q\":\n",
    "            print(\"Exited.\")\n",
    "            break\n",
    "        elif cmd == \"s\":\n",
    "            print(\"A:\", card[\"answer\"])\n",
    "        elif cmd == \"r\":\n",
    "            swipe_right(q, card[\"card_id\"])\n",
    "            print(\"✅ Marked learned. Remaining:\", len(q))\n",
    "        elif cmd == \"l\":\n",
    "            swipe_left(q)\n",
    "            print(\"🔁 Moved to end. Queue size:\", len(q))\n",
    "        else:\n",
    "            print(\"Unknown command. Use s/r/l/q.\")\n",
    "\n",
    "    if not q:\n",
    "        print(\"\\nAll cards learned for this article. 🎉\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b446bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage:\n",
    "run_text_session(url_canonical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b8c4e2",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5e74c",
   "metadata": {},
   "source": [
    "## Function1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb50b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === deps (assumes you've already installed: openai, trafilatura, bs4, lxml or html5lib, python-dotenv) ===\n",
    "# import os, re, json, datetime, typing as T\n",
    "# from dataclasses import dataclass, asdict\n",
    "# from urllib.parse import urlparse\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import trafilatura\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# # ------------ Parser selection (robust) ------------\n",
    "# def _best_bs4_parser():\n",
    "#     try:\n",
    "#         import lxml  # noqa: F401\n",
    "#         return \"lxml\"\n",
    "#     except Exception:\n",
    "#         try:\n",
    "#             import html5lib  # noqa: F401\n",
    "#             return \"html5lib\"\n",
    "#         except Exception:\n",
    "#             return \"html.parser\"\n",
    "\n",
    "# # ------------ Models & settings ------------\n",
    "# MODEL_WITH_WEB = \"gpt-4o-mini\"   # has web tool on supported accounts\n",
    "# MODEL_FALLBACK  = \"gpt-4.1-mini\" # summarization if web tool unavailable\n",
    "\n",
    "# STRICT_JSON_RULES = (\n",
    "#     \"Return STRICT JSON with keys: \"\n",
    "#     '[\"title\",\"author\",\"publish_date\",\"categories\",\"tags\",\"tldr\",\"language\",\"citations\",\"confidence_notes\"]. '\n",
    "#     \"categories=1–3 short labels; tags=5–12 keywords; tldr=3–6 crisp bullets; \"\n",
    "#     \"language=two-letter ISO code; citations=list of {title,url}; \"\n",
    "#     \"confidence_notes=1–2 short sentences. No markdown—JSON only.\"\n",
    "# )\n",
    "\n",
    "# DEFAULT_HEADERS = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:128.0) Gecko/20100101 Firefox/128.0\"\n",
    "# }\n",
    "\n",
    "# @dataclass\n",
    "# class PageContent:\n",
    "#     url: str\n",
    "#     domain: str\n",
    "#     title: T.Optional[str]\n",
    "#     author: T.Optional[str]\n",
    "#     publish_date: T.Optional[str]\n",
    "#     text: str\n",
    "#     html_len: int\n",
    "#     text_len: int\n",
    "#     _html: T.Optional[str] = None\n",
    "\n",
    "# # ------------ Helpers ------------\n",
    "# def _safe_json_loads(text: str) -> dict:\n",
    "#     try:\n",
    "#         return json.loads(text)\n",
    "#     except Exception:\n",
    "#         m = re.search(r\"\\{.*\\}\", text, re.S)\n",
    "#         return json.loads(m.group(0)) if m else {\"error\": \"Non-JSON output\", \"raw\": text[:1200]}\n",
    "\n",
    "# def _fetch_html(url: str) -> str:\n",
    "#     r = requests.get(url, headers=DEFAULT_HEADERS, timeout=25)\n",
    "#     r.raise_for_status()\n",
    "#     return r.text\n",
    "\n",
    "# def _guess_meta(soup: BeautifulSoup) -> dict:\n",
    "#     meta = {\"title\": None, \"author\": None, \"publish_date\": None}\n",
    "#     # title\n",
    "#     if soup.title and soup.title.string:\n",
    "#         meta[\"title\"] = soup.title.string.strip()\n",
    "#     ogt = soup.find(\"meta\", property=\"og:title\")\n",
    "#     if ogt and ogt.get(\"content\"):\n",
    "#         meta[\"title\"] = ogt[\"content\"].strip()\n",
    "#     # author\n",
    "#     for k in [\"author\", \"article:author\", \"og:article:author\"]:\n",
    "#         m = soup.find(\"meta\", attrs={\"name\": k}) or soup.find(\"meta\", attrs={\"property\": k})\n",
    "#         if m and m.get(\"content\"):\n",
    "#             meta[\"author\"] = m[\"content\"].strip(); break\n",
    "#     # date\n",
    "#     for k in [\"article:published_time\", \"og:published_time\", \"pubdate\", \"publish-date\", \"date\"]:\n",
    "#         m = soup.find(\"meta\", attrs={\"name\": k}) or soup.find(\"meta\", attrs={\"property\": k})\n",
    "#         if m and m.get(\"content\"):\n",
    "#             meta[\"publish_date\"] = m[\"content\"].strip(); break\n",
    "#     if meta[\"publish_date\"] is None:\n",
    "#         t = soup.find(\"time\")\n",
    "#         if t and (t.get(\"datetime\") or t.text):\n",
    "#             meta[\"publish_date\"] = (t.get(\"datetime\") or t.text).strip()\n",
    "#     return meta\n",
    "\n",
    "# def extract_readable_text(url: str, include_html: bool=False) -> PageContent:\n",
    "#     html = _fetch_html(url)\n",
    "\n",
    "#     # 1) trafilatura (best effort)\n",
    "#     text = (trafilatura.extract(\n",
    "#         html, include_comments=False, include_tables=False, favor_precision=True\n",
    "#     ) or \"\").strip()\n",
    "\n",
    "#     # 2) fallback: BeautifulSoup\n",
    "#     if len(text) < 200:\n",
    "#         PARSER = _best_bs4_parser()\n",
    "#         soup = BeautifulSoup(html, PARSER)\n",
    "#         for tag in soup([\"script\",\"style\",\"noscript\",\"header\",\"footer\",\"nav\",\"aside\"]):\n",
    "#             tag.extract()\n",
    "#         text = soup.get_text(\"\\n\", strip=True)\n",
    "\n",
    "#     PARSER = _best_bs4_parser()\n",
    "#     soup_full = BeautifulSoup(html, PARSER)\n",
    "#     meta = _guess_meta(soup_full)\n",
    "\n",
    "#     text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "#     return PageContent(\n",
    "#         url=url,\n",
    "#         domain=urlparse(url).netloc,\n",
    "#         title=meta[\"title\"],\n",
    "#         author=meta[\"author\"],\n",
    "#         publish_date=meta[\"publish_date\"],\n",
    "#         text=text,\n",
    "#         html_len=len(html or \"\"),\n",
    "#         text_len=len(text or \"\"),\n",
    "#         _html=html if include_html else None\n",
    "#     )\n",
    "\n",
    "# # ------------ Web tool path (no response_format to be version-agnostic) ------------\n",
    "# def analyze_link_with_web_tool(url: str) -> dict:\n",
    "#     if not hasattr(client, \"responses\"):\n",
    "#         raise RuntimeError(\"responses_api_unavailable\")\n",
    "\n",
    "#     base_kwargs = dict(\n",
    "#         model=MODEL_WITH_WEB,\n",
    "#         tools=[{\"type\": \"web_search\"}],\n",
    "#         input=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a precise analyst that reads the provided URL using the web tool.\"},\n",
    "#             {\"role\": \"user\", \"content\": f\"{STRICT_JSON_RULES}\\n\\nRead this URL and summarize it with citations: {url}\"}\n",
    "#         ],\n",
    "#     )\n",
    "#     resp = client.responses.create(**base_kwargs)\n",
    "#     # Some SDKs expose .output_text, others .output; handle both:\n",
    "#     output_text = getattr(resp, \"output_text\", getattr(resp, \"output\", \"\"))\n",
    "#     return _safe_json_loads(output_text)\n",
    "\n",
    "# # ------------ Fallback summarization (local content + LLM) ------------\n",
    "# def summarize_local_content(page: PageContent) -> dict:\n",
    "#     payload = {\n",
    "#         \"source_url\": page.url,\n",
    "#         \"detected_title\": page.title,\n",
    "#         \"detected_author\": page.author,\n",
    "#         \"detected_publish_date\": page.publish_date,\n",
    "#         \"article_text\": page.text[:22_000]\n",
    "#     }\n",
    "#     if hasattr(client, \"responses\"):\n",
    "#         resp = client.responses.create(\n",
    "#             model=MODEL_FALLBACK,\n",
    "#             input=[\n",
    "#                 {\"role\": \"system\", \"content\": STRICT_JSON_RULES},\n",
    "#                 {\"role\": \"user\", \"content\": json.dumps(payload)}\n",
    "#             ],\n",
    "#         )\n",
    "#         output_text = getattr(resp, \"output_text\", getattr(resp, \"output\", \"\"))\n",
    "#         return _safe_json_loads(output_text)\n",
    "#     else:\n",
    "#         # Very old SDK fallback via Chat Completions\n",
    "#         messages = [\n",
    "#             {\"role\": \"system\", \"content\": STRICT_JSON_RULES},\n",
    "#             {\"role\": \"user\", \"content\": json.dumps(payload)},\n",
    "#         ]\n",
    "#         cc = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "#         return _safe_json_loads(cc.choices[0].message.content)\n",
    "\n",
    "# # ------------ Public API returning BOTH LLM + content ------------\n",
    "# def analyze_link_plus(url: str, include_html: bool=False, text_cap: int=50_000) -> dict:\n",
    "#     \"\"\"\n",
    "#     Returns:\n",
    "#     {\n",
    "#       \"llm\": {...},                # categories/tags/tldr/etc.\n",
    "#       \"content\": {\n",
    "#           \"url\",\"domain\",\"meta\":{\"title\",\"author\",\"publish_date\"},\n",
    "#           \"text\",\"text_len\",\"html_len\", [\"html\" if include_html=True]\n",
    "#       },\n",
    "#       \"_source\": {...}             # mode + bookkeeping\n",
    "#     }\n",
    "#     \"\"\"\n",
    "#     assert url.startswith(\"http\"), \"Pass a valid http(s) URL.\"\n",
    "\n",
    "#     # Always build content locally so you can verify tags/summary\n",
    "#     page = extract_readable_text(url, include_html=include_html)\n",
    "#     content_block = {\n",
    "#         \"url\": page.url,\n",
    "#         \"domain\": page.domain,\n",
    "#         \"meta\": {\"title\": page.title, \"author\": page.author, \"publish_date\": page.publish_date},\n",
    "#         \"text\": page.text[:text_cap],                   # cap returned text to keep memory manageable\n",
    "#         \"text_len\": page.text_len,\n",
    "#         \"html_len\": page.html_len,\n",
    "#     }\n",
    "#     if include_html and page._html is not None:\n",
    "#         content_block[\"html\"] = page._html  # careful: can be huge\n",
    "\n",
    "#     # Try web tool first for the LLM output\n",
    "#     try:\n",
    "#         llm_data = analyze_link_with_web_tool(url)\n",
    "#         mode = \"openai_web_tool\"\n",
    "#         model_used = MODEL_WITH_WEB\n",
    "#     except Exception:\n",
    "#         if page.text_len < 200:\n",
    "#             raise RuntimeError(\"Could not extract enough text; page may be paywalled or script-rendered.\")\n",
    "#         llm_data = summarize_local_content(page)\n",
    "#         # fill missing fields\n",
    "#         llm_data.setdefault(\"title\", page.title)\n",
    "#         llm_data.setdefault(\"author\", page.author)\n",
    "#         llm_data.setdefault(\"publish_date\", page.publish_date)\n",
    "#         llm_data.setdefault(\"citations\", [{\"title\": page.title or \"\", \"url\": page.url}])\n",
    "#         mode = \"local_fallback\"\n",
    "#         model_used = MODEL_FALLBACK\n",
    "\n",
    "#     result = {\n",
    "#         \"llm\": llm_data,\n",
    "#         \"content\": content_block,\n",
    "#         \"_source\": {\n",
    "#             \"mode\": mode,\n",
    "#             \"model\": model_used,\n",
    "#             \"url\": page.url,\n",
    "#             \"domain\": page.domain,\n",
    "#             \"fetched_at\": datetime.datetime.utcnow().isoformat() + \"Z\"\n",
    "#         }\n",
    "#     }\n",
    "#     return result\n",
    "\n",
    "\n",
    "# test_url = \"https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method\"\n",
    "# out = analyze_link_plus(test_url, include_html=False, text_cap=40000)\n",
    "# print(json.dumps(out[\"llm\"], indent=2, ensure_ascii=False))\n",
    "# print(\"---\\nCONTENT META:\", json.dumps(out[\"content\"][\"meta\"], indent=2, ensure_ascii=False))\n",
    "# print(\"TEXT PREVIEW:\\n\", out[\"content\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc2112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wl/_gt22h7503dgw67xjtv9ccn00000gn/T/ipykernel_6815/3911997558.py:291: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  fetched_at_utc = datetime.datetime.utcnow().isoformat() + \"Z\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fetched_at_utc</th>\n",
       "      <th>headline</th>\n",
       "      <th>categories</th>\n",
       "      <th>tags</th>\n",
       "      <th>tldr</th>\n",
       "      <th>content_text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-13T15:06:49.176125Z</td>\n",
       "      <td>LLM-as-a-Judge Simply Explained: The Complete ...</td>\n",
       "      <td>[AI/ML, Product, Startups]</td>\n",
       "      <td>[LLM, RAG, LangGraph, Agentic AI, Vector DB, P...</td>\n",
       "      <td>[LLM-as-a-Judge automates LLM evaluation, enha...</td>\n",
       "      <td>Your LLM app is streaming out tokens faster th...</td>\n",
       "      <td>https://www.confident-ai.com/blog/why-llm-as-a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                fetched_at_utc  \\\n",
       "0  2025-09-13T15:06:49.176125Z   \n",
       "\n",
       "                                            headline  \\\n",
       "0  LLM-as-a-Judge Simply Explained: The Complete ...   \n",
       "\n",
       "                   categories  \\\n",
       "0  [AI/ML, Product, Startups]   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [LLM, RAG, LangGraph, Agentic AI, Vector DB, P...   \n",
       "\n",
       "                                                tldr  \\\n",
       "0  [LLM-as-a-Judge automates LLM evaluation, enha...   \n",
       "\n",
       "                                        content_text  \\\n",
       "0  Your LLM app is streaming out tokens faster th...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.confident-ai.com/blog/why-llm-as-a...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # =========================\n",
    "# # Quick smoke test / demo (Notebook-friendly)\n",
    "# # =========================\n",
    "\n",
    "# # 1) load taxonomy (or init if empty)\n",
    "# tax = load_taxonomy(TAXONOMY_PATH)\n",
    "# if not tax[\"categories\"]:\n",
    "#     tax[\"categories\"] = [\"AI/ML\",\"Product\",\"Startups\",\"Career\",\"Health\",\"Finance\"]\n",
    "# if not tax[\"tags\"]:\n",
    "#     tax[\"tags\"] = [\"LLM\",\"RAG\",\"LangGraph\",\"Agentic AI\",\"Vector DB\",\"Prompting\",\"A/B Testing\",\"Retention\"]\n",
    "# save_taxonomy(tax[\"categories\"], tax[\"tags\"], TAXONOMY_PATH)\n",
    "\n",
    "# # 2) test a URL\n",
    "# test_url = \"https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method\"\n",
    "\n",
    "# rec = analyze_link_plus(\n",
    "#     test_url,\n",
    "#     allowed_categories=tax[\"categories\"],\n",
    "#     allowed_tags=tax[\"tags\"]\n",
    "# )\n",
    "\n",
    "# # 3) update taxonomy (so new categories/tags are remembered)\n",
    "# tax[\"categories\"] = rec[\"_taxonomy\"][\"updated_categories\"]\n",
    "# tax[\"tags\"] = rec[\"_taxonomy\"][\"updated_tags\"]\n",
    "# save_taxonomy(tax[\"categories\"], tax[\"tags\"], TAXONOMY_PATH)\n",
    "\n",
    "# # 4) convert to DataFrame directly\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame([{\n",
    "#     \"fetched_at_utc\": rec[\"fetched_at_utc\"],\n",
    "#     \"headline\": rec[\"headline\"],\n",
    "#     \"categories\": rec[\"categories\"],\n",
    "#     \"tags\": rec[\"tags\"],\n",
    "#     \"tldr\": rec[\"tldr\"],\n",
    "#     \"content_text\": rec[\"content_text\"],\n",
    "#     \"url\": rec[\"url\"]\n",
    "# }])\n",
    "\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128f45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6aeab6e",
   "metadata": {},
   "source": [
    "## Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aacbe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Implement cycle ordering logic:\n",
    "# from ipywidgets import Button, HBox, VBox, HTML\n",
    "# from IPython.display import display, clear_output\n",
    "# from collections import deque\n",
    "\n",
    "# def run_widget_session(url_canonical, shuffle=True):\n",
    "#     \"\"\"\n",
    "#     Round-based review:\n",
    "#       - 'Review again' defers the card to the next round (again_q).\n",
    "#       - You won't see deferred cards again until you've finished the current round.\n",
    "#     \"\"\"\n",
    "#     # Build initial queue from unlearned cards\n",
    "#     df = load_unlearned_cards(url_canonical)\n",
    "#     print(f\"Found {len(df)} unlearned cards for {url_canonical}\")\n",
    "#     cards = df[[\"card_id\",\"question\",\"answer\"]].to_dict(orient=\"records\")\n",
    "#     if shuffle and len(cards) > 1:\n",
    "#         import random\n",
    "#         random.shuffle(cards)\n",
    "\n",
    "#     # Two-queue scheduler\n",
    "#     main_q = deque(cards)      # current round\n",
    "#     again_q = deque()          # next round\n",
    "#     round_idx = 1\n",
    "#     total_start = len(cards)\n",
    "\n",
    "#     # UI elements\n",
    "#     title = HTML(f\"<h3>Flashcard Session</h3>\")\n",
    "#     subtitle = HTML(\"\")\n",
    "#     q_html = HTML(\"\")\n",
    "#     a_html = HTML(\"\")\n",
    "#     btn_reveal = Button(description=\"Reveal answer\")\n",
    "#     btn_know   = Button(description=\"I know it 👍\", button_style=\"success\")\n",
    "#     btn_review = Button(description=\"Review again 🔁\", button_style=\"warning\")\n",
    "\n",
    "#     container = VBox([title, subtitle, q_html, a_html, HBox([btn_reveal, btn_know, btn_review])])\n",
    "\n",
    "#     def update_subtitle():\n",
    "#         # Progress: remaining in current round / total (and round number)\n",
    "#         subtitle.value = (\n",
    "#             f\"<div style='color:#666'>Round {round_idx} • \"\n",
    "#             f\"{len(main_q)} left in this round\"\n",
    "#             f\"{' • ' + str(len(again_q)) + ' deferred' if len(again_q) else ''}\"\n",
    "#             f\"{f' • {total_start} total' if total_start else ''}\"\n",
    "#             f\"</div>\"\n",
    "#         )\n",
    "\n",
    "#     def rollover_if_needed():\n",
    "#         nonlocal main_q, again_q, round_idx\n",
    "#         if not main_q and again_q:\n",
    "#             main_q = again_q\n",
    "#             again_q = deque()\n",
    "#             round_idx += 1\n",
    "\n",
    "#     def disable_all():\n",
    "#         btn_reveal.disabled = True\n",
    "#         btn_know.disabled = True\n",
    "#         btn_review.disabled = True\n",
    "\n",
    "#     def render():\n",
    "#         rollover_if_needed()\n",
    "#         update_subtitle()\n",
    "\n",
    "#         if not main_q:\n",
    "#             q_html.value = \"<b>All cards learned or deferred round(s) completed. 🎉</b>\"\n",
    "#             a_html.value = \"\"\n",
    "#             disable_all()\n",
    "#             return\n",
    "\n",
    "#         card = main_q[0]\n",
    "#         q_html.value = f\"<b>Question</b><div style='margin-top:4px'>{card['question']}</div>\"\n",
    "#         a_html.value = \"<i>(click 'Reveal answer')</i>\"\n",
    "\n",
    "#     def on_reveal(_):\n",
    "#         if not main_q: return\n",
    "#         card = main_q[0]\n",
    "#         a_html.value = f\"<b>Answer</b><div style='margin-top:6px'>{card['answer']}</div>\"\n",
    "\n",
    "#     def on_know(_):\n",
    "#         # Persist learned=True and remove from current round\n",
    "#         if not main_q: return\n",
    "#         card = main_q[0]\n",
    "#         swipe_right(main_q, card[\"card_id\"])  # this pops from the queue inside swipe_right\n",
    "#         # BUT our swipe_right expects the deque of dicts; we passed main_q.\n",
    "#         # It will popleft() the current item; keep again_q untouched.\n",
    "#         render()\n",
    "\n",
    "#     def on_review(_):\n",
    "#         if not main_q: return\n",
    "#         # Defer current card to next round: move from main_q -> again_q\n",
    "#         card = main_q.popleft()\n",
    "#         again_q.append(card)\n",
    "#         render()\n",
    "\n",
    "#     btn_reveal.on_click(on_reveal)\n",
    "#     btn_know.on_click(on_know)\n",
    "#     btn_review.on_click(on_review)\n",
    "\n",
    "#     display(container)\n",
    "#     render()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "what_to_eat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
